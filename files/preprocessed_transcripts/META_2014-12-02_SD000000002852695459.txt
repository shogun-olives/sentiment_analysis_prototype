right , think 're going go ahead get started . stephen ju credit suisse internet equity research team . joined jason taylor , head infrastructure development effort facebook . without ado , take away . great ! name jason taylor run group called infrastructure foundation facebook . responsible server design , server supply chain , overall capacity management . capacity engineering , performance review , thing like . also long-term infrastructure plan . today going walk little bit infrastructure talk eﬃciency program excited look toward future eﬃciency large-scale computing . facebook large . 82 % monthly active user outside united state . global deployment . international data center , one lulea several united state . 1.35 billion connect u monthly , 1.2 billion mobile stunning 930 million photo uploaded site every day . that's lot medium , lot content distributed facebook . 6 billion like , 12 billion message per day . 's active site , dynamic . built infrastructure accommodate . , last five year really eﬃciency top priority company . initially would say really necessity . facing huge uptick adoption facebook usage . eﬃciency always core able scale . reached large-scale became necessary long-term financial viability also ability build platform scale well . , cost perspective really eﬃciency break three area . data center , heat management really one important thing term core eﬃciency . poorly designed facility , facility concentrate heat much , could easily pay 50 % 90 % additional electricity bill every watt deliver server . facebook , 've designed server -- server data center , heat tax 7 % , mean using cold air outside . chilling air ; passing across server , mixing hot aisle evacuating side building . term raw thermal eﬃciency , data center second none . , server pride vanity-free design . really focus supply chain optimization . 2011 released first data center first set server . also started open compute project , 'm sure many familiar , give away design server open design , approach . think eﬃciency server level . main eﬃciency win really come software . horizontal win like hhvm hphp , win cash , database . web absolutely critical continuing deliver really eﬃcient infrastructure . peak time one front-end cluster really pretty piping hot , run 10 hour day 90 % 93 % server utilization . really work tremendous amount making sure individual server software optimized also whole data center optimized provide content . like blue . server blue led . see -- front server . enclosed space hot aisle containment . cold air come ceiling , 's sucked server . inside hot aisle containment temperature reach 100 degree . hot air evacuated building potentially mixed winter time . 's thermally eﬃcient system . thing notice server look . that's really work hard homogeneous footprint get good win term serviceability maintenance , driver , everything else . , also open eﬃciency win . talked publicly , released data center design , released server design also released core software power facebook . hhvm core php web server . , ballpark , five six time eﬃcient traditional apache stack . flashcache service use database trade ﬂash caching access slower hard drive . presto one data processing/data warehouse piece software . rock stevie [ ph ] , proxy , thrift folly general library use . case really try , able support open sourcing project , try keep . reason really believe entire industry benefit eﬃciency work benefit industry feeding back contributing new idea design . fundamentally company going win lose based product , cost infrastructure . cost eﬃciency win infrastructure something would like entire industry benefit . term architecture keep pretty simple . front-end cluster . front-end cluster synonymous network cluster . large . 12,000 server per cluster . stamp capacity push order serve hot request user . service cluster contain many dedicated service -- search , photo , message . others . back-end cluster built optimized database storage . think lot power redundancy cluster . take one service , 's useful think little bit one large service work . facebook viewing main feed . desktop , center ; mobile , main experience . reason activity friend facebook . data kept index called news feed rack . reason activity last day people using facebook kept one rack . design leaf aggregator . leaf contain data , storage recent activity -- 's ram . aggregator thing ranking algorithm consolidates information respond request . web hit come . say , need story , go news feed rack , pick one aggregator say , give story . aggregator blast query parallel 40 server rack , gather subset data , rank based interest . since displayed . , engineer facebook server like long 's one five server . n't allow variation . one main team facebook infra capacity engineering . wear black t-shirt say front . good saying kind engineering request fundamentally software far ﬂexible hardware . pay hardware . hardware becomes ineﬃcient lot variation . homogeneity keep infrastructure , easier optimize , better supply chain , better eﬃciencies stack term operating understanding server . facebook year -- five type server . 's web , main compute workhorse . database , year evolved purely disk thing entirely ﬂash . hadoop main data warehouse data processing . 's lot compute lot disk . photo -- 's lowest dollar gig . certainly store lot medium facebook . optimizing dollar per gig really important . last feed rack , 's news feed service talked . engineer like lot memory lot compute . 's rack give . advantage five server type really constraining pretty classic . get volume pricing . putting order , large order , really work deeply supplier work way 's beneficial supply chain , 's predictable . pas saving , important . 's also important repurposing . five service large projection term well something could launch something could launch using type server , ﬂexible reallocating server one service another . mean n't server infrastructure lay fallow waiting product launch . 's -- well , guy mutual fund . 's like mutual fund . 's dollar . really manage well . key advantage easier operation . typical data center facility might data center tech server ratio one 400 450. facility somewhere 1 15,000 1 20,000. server . optimized serviceability work hard make easy . also translates operation software benefit , eﬃciency consumer device . drawback . drawback intrinsic hardware -- soon allocate hardware , hardware land life three four year . however , software need change time . point time , hardware software n't fit well . , facebook rack computer , really thinking piece software fit individual server , allocate rack , question software live whole rack . want talk point really idea mentioned . want talk fun result . disaggregated rack -- shooting better component service fit time also looking extend useful life server . , think rack news feed server ignore fact 's bunch server think , well , really ? 've got bunch compute , 've got bunch ram . 've got ﬂash . , exactly computer ram n't matter 's within rack got nice healthy network . break disaggregated rack idea major component , got processor compute server . 've got ram kind ram server . might storage server might flash . rather put one server , time going hit weakest link , going enough computer enough ram , let's break , put high-bandwidth backplane switch resource service need . time , wasting service -- wasting resource . , server service fit better across service time . also accommodate longer hardware refresh . type 6 server . news feed server -- cpu left , right left ram . news feed server fit cpu ram well . , designed server news feed . however , go another service , maybe search one index service , might need ram cpu . mean terminally using cpu resource . thing happen beginning service 's life , maybe year one , perfect fit . along year two need ram need ﬂash . able allocate time allocate hardware along need service provides huge benefit otherwise buying server really needed ram . , ca n't open case 10,000 server upgrade ram . n't work . able add sled ram huge benefit . third benefit really keep hardware long physically last . , many time computer scale , deprecating based critical resource 's longer good enough . mean throwing away resource perfectly fine . compute really n't get old . ram -- 's solid-state device ; pure ram device operate forever . disk wear time . ﬂash , depending write volume , might wear time . actually live . think disaggregated rack , say , graph search , rather computer three resource , let 's compute server , thing compute -- ram mainly focused compute -- would type 1 server u -- ﬂash sled , would anywhere 30 256 terabyte ﬂash single sled . ram sled might 256 512 gig ram storage . , year one ratio pick might perfect . year two might discover -- might eﬃciency win . index might grow time . best thing would give service ﬂash . rather allocate entire separate rack , thereby doubling cost , allocate one resource slam another flash sled . kind ﬂexibility really lead pretty nice eﬃciency win . still maintain core strength -- volume pricing , custom configuration , sort thing . really allows u much smarter technology refreshes hardware , resource -- thinking rack level evolve hardware service . , last year talked approximate tco win . three ; six-year period , looking 12 % 20 % opex saving , conservative side , aggressive , 14 % 30 % . using different approach -- keep mind nothing fundamentally changed computer allocating . allocating different way . helping software team little bit ﬂexible bring resource . work pretty much anybody scale . , -- time talked last year working project . 've landed one service . , 've got 20 , 30 service , maybe 40 major one . one started actually able realize 40 % saving total cost operating equipment . nothing bringing resource necessary online right time customizing rack scalable , ﬂexible way maintains supply chain win able realize 40 % reduction cost one service . something -- approach technique think pretty much anyone use . , think computer resource think last 20 year , -- one idea disaggregated rack able adopt different type resource . look last 20 year , type thing server scale pretty much . 's -- server -- amir michael holding one first facebook-built server . server , architecture perspective , almost identical 386 tower case 20 year ago . new technology -- math coprocessors , two processor server , multicore . good . game changer last 20 year really gpus , great vector math . ﬂash memory , phenomenal win last four five year . looking forward really major advancement network . 100- gig nics , 400 gig switch -- perfectly reasonable , given state technology . flash going flash provider really pushing higher higher iop heavier-duty ﬂash . actually feel at-scale environment want look direction , going lower iop really n't need many . also much careful use ﬂash . mean get much denser ﬂash sled realize nice benefit . think ﬂash 's always going nice market high- performance ﬂash . think data center world lot ﬂash interest going shift towards lower lower ﬂash . last year talk asked industry -- please make worst ﬂash possible . really , work q - stephen ju q - unidentified participantvery -- necessarily low-quality ﬂash lower-endurance ﬂash , tlc even beyond term bit-density . , 's also number ram alternative think coming interesting . phase-change memory , think , going -- work 's going pretty good . resistive memory par term technology . also cold ﬂash worm solid-state storage . storage need spinning medium . 's perfectly reasonable immutable data put solid- state device . look eye chart technology next year , eye chart . 's lot detail . simplify bit -- give one second take photo . sorry . pretty basic evolution . last -- , say , 2009 , say , 2020 , think computer data center going much focused one processor , system-on-a-chip processor . whole eﬃciency ecosystem 's developing really strong . yes . still ram . think phase-change memory resistive memory also strong player . worm -- solid-state storage . technology might evolve next , say , three five year . permanent , immutable storage solid-state completely reasonable . think bit density get point superior tco sense thin hard drive compared hard drive . optical data storage great future term ability densify medium time . 've seen number announcement term archival disk taking look like blu-ray disc taking 100 gig 500 gig even terabyte . 100-gig fiber server -- think couple year away . technology interested , technology excited . , 'd like open question . question answer yes . probably time couple question . poll question audience . 's lot talk mesos implication computing software infrastructure stack . perspective mesos get adopted implication change ? - jason taylor q - unidentified participant - jason taylor q - unidentified participant q - stephen ju - jason taylor mesos virtualization thing ? yes . n't . infrastructure based bare metal scaling . virtualization cloud excellent managing heavy idle workload . 20 idle server need compact two idle server , 's obviously win . n't buy 18 idle server . 're building scale building throughput , best eﬃciency win come , one , really balancing utilization gear , lot disag work . also really looking deeply software hardware work together look big performance win . first , would say , 3x hphp 's win really came going interpreted language compiled language . solid win . win since really come optimization code work . also work specific hardware run . soon start virtualizing , soon start putting layer abstraction , decouple engineer , fantastic , -- essentially n't allow make kind optimization anymore . facebook , large workload , really focus using piece hardware much . extent eﬃciencies discussed morning going allow facebook capital-eﬃcient growth company continues grow revenue next three five year ? think fundamentally eﬃciency top priority u long time . think important aspect eﬃciency u coherence brings software engineer term unifying thinking software hardware work together . want talk actual capital spend would talk deborah . term eﬃciency core way work . talked lot 100-gig [ ph ] fiber . talk ( inaudible ) activity ? expansion 100-gig fiber within footprint . sure . say talked 100-gig fiber 400-gig fiber , within data center . happened telecommunication industry delivered fantastic technology last bunch q - stephen ju - jason taylorof year talked , dark fiber , major facility two major facility , running hundred mile . well look core technology , ready adopted data center space . kind bandwidth within data center possible . 's number great technology developing right term silicon photonics . 's several company working essentially fully integrated chip optic package deliver 100-gig performance really low cost . 's largest thing happened . biggest thing happened two three year ago ﬂash data center . thing 's happening right amount network buy reasonable price climbing dramatically . joined facebook 1-gig nic 's everywhere . 1 gig standard . 2011 shifted 10 gig . pretty soon , next couple year 25 gig server . 'd say within three year 100 gig service . around six-year period going 100x amount bandwidth that's available -- 's transformative data center operates also write service . network -- think work improvement networking going largest driver towards change large-scale internet company work . think actually time . thank much . thank .