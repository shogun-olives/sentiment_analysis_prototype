okay . thanks much , hi , everyone . , 's brett simpson . 's pleasure welcome ian buck , know , run data center division nvidia , one key growth engine business . think particularly interesting time connect ian simona heading new product cycle hopper . looking back nvidia launched ampere , think back early 2020 , business $ 1 billion quarterly revenue back . looking today , 's almost $ 4 billion quarterly revenue . 's really strong period business well done . many know ian 's inventor cuda , 're going try touch sort software opportunity nvidia see ahead presentation . ian , thanks much joining u today . thanks . thank . also simona jankowski line , know vp ir nvidia . 'm going pas simona read safe harbor . simona , . q - brett simpson - ian buck q - brett simpson - ian buckthank , brett , hosting u . quick reminder , comment today may contain forward-looking statement , investor advised read report filed sec information related risk uncertainty facing business . back . question answer thanks , simona . maybe start , ian , maybe recap 2022 period ? 's to-date looking division market opportunity see ahead , maybe touch sort customer reaction new product think happening bleeding edge ai see evolving 2023 ? yeah . mean 's whirlwind . probably hit u back , obviously , 2020 , 's launched ampere , feel like lifetime ago , back 2020. -- throughout covid , launched ampere . following year , announced grace cpu work . year , course , 've announced hopper . nvidia new clip investment importance computing across industry , innovation allow u continue invent new gpus , new architecture , new algorithm serve growing market ai computing data center general biannual clip point . 're making new gpu architecture every two year . 've committed new cpu architecture also every two year . exciting launch hopper year . hopper gpu specifically designed advance ai well hpc . -- specialization transformer-based model , foundational model -- used today large language model , generative ai , applied pretty much every domain computer want see , listen , learn generate back . 're excited hopper 's introduction . 's production . pc ai [ ph ] system coming available oems cloud hyperscalers , hopper bringing market . great . maybe given environment run moment , think lot investor trying understand whether environment 're right , made think different opportunity see ahead ? look next sort six 12 month 've got lot product rollouts , et cetera , enterprise engagement still ? demand signal still strong thinking six 12 month ago ? yeah . foundation engage grown last two year 've remained consistent , increasing activity motion . start certainly big hyperscalers work , best work , microsoft partnering openai , meta google even others , taking advantage gpus software solution 're layering top order develop next-generation ai technology capability , moonshot foundational model . lot work software , hardware . fact , nvidia software developer hardware developer , example . lot work framework like pytorch tensorflow . also develop large language model supercomputer share rest world help move ball forward give u experience continuously improve -- platform , along taking input every major hyperscalers well experience . build supercomputer try test advance state art . 's certainly big hyperscale side . activation cloud continued grow . certainly , every enterprise cloud-first strategy kind 're bringing technology , workﬂows , way business cloud . result , -- public cloud activation platform certainly grown . saw a100 , 'll continue see grow h100 . enterprise side , -- certainly upward trend . challenge enterprise side meeting enterprise developer . n't talent pool necessary google amazon meta may . -- see opportunity ai could changing advancing business . role help move forward provide enterprise-supported platform , 's big part nvidia ai enterprise platform help provide stable supported platform enterprise rely getting best -- best performance , support need directly nvidia , working together ( inaudible ) cloud partner . also helping activate right kind software take advantage , natively ai framework targeted service support come working , 's lot great activity happening , course , start-up community , providing cloud-based service , enterprise share data get result back without build software scratch . -- 're seeing also , course , cloud provider providing service , lot developed deployed nvidia gpus along -- growing start-up broader isv category well . even traditional isvs may area numerical simulation trying deploy ai surrogate model first principle physic simulation product development sort thing . finally , -- nvidia , trying move ball forward well . area see opportunity move market forward , 'll invest , we've done , particularly healthcare space clara platform , applying ai thing like medical imaging proteomics . 're also speech ai . we've q - brett simpson - ian bucksort provided foundational support riva platform help enterprise take advantage end-to-end speech ai , offer really cool ability customization train new model , new voice additional thing like . recently , last gdc , announced nemo large language model service build service enterprise customize large model provide open . gpt-like model , variety different gpt size . 've done 530 billion parameter model called megatron , make available service . enterprise take existing train model , model super smart tune -- fine-tune basically technical prompting . couple example , hundred example 500 model learns answer question way customer looking [ ph ] answered . 're asking question broader internet customer support , 've already given hundred example customer support call answer look like , ( inaudible ) answer appropriately context business . enterprise side , 're beginning -- 're still beginning ai adoption , 're seeing lot interest . nvidia standpoint , company engage different level , certainly consuming sort first-party offering decided invest , also isvs start-ups well , course , platform holistically . 's big part strategy , open move ball forward helping company customer developer user get benefit ai . want come back service model around megatron gpt broadly little bit later , wanted -- mean , guess go back gpt-3 model , coming a100 around time . much nlp-centric upgrade cycle could see playing nvidia last couple year . talk little bit today ? 's bleeding edge , 's worked lab see ? mean , conference year ago , ian , talked model size growing like 10x year . still happening ? mean must well trillion parameter -- model size trillion , guess , parameter . go next couple year hopper cycle ? type workload hopper ideally positioned deliver ? yeah . certainly , natural -- nlp large language model , llm community n't slowed one bit , see . many -- major player , well start-ups , -- taking advantage model . large . tend -- well , previous model may focus thing like computer vision , understanding picture picture stop sign . 's capability -- think first principle sort q - brett simpson - ian buckunderstanding perspective , 's fairly straightforward . fact , nature , see animal bug thing , basic vision capability like , identify object , call . language different . language unique . encompasses -- understand language , need know word 'm saying right , mean order make context anything useful . mean sort -- model encompass understand corpus human understanding . result , 're trained entire internet , literally , data set basically big scrape internet cleaned tuned trained . tend obviously much bigger . -- 's 530 billion parameter model , 've talked already . certainly , new work -- new model coming already trillion parameter . 're still short like brain scale , 's question -- 150 trillion parameter , 're ( inaudible ) trillion right . 2025 , 2026. yeah . 'll get , moore 's law . model -- capability growing , model . basically , large language model today , obviously , provide convincing chat dialogue back forth . used sentiment analysis , sort thing understand human knowledge . next step , obviously , 're seeing right . remains true . model large , certainly , n't train single gpu even single server , tend train across pod collection . trained megatron model 4,000 gpus final training run took little month . course , 's lot r & get point develop model . , way , certainly -- 's trained , tuned different use case . tuning step actually much approachable requires le infrastructure going train entire model . point , people developing new foundational model used different use case . 's lot interesting research development biggest player infrastructure . one goal hopper bring cost large language model training make applicable , 've done . hopper run trick [ ph ] train 6 time 9 time faster ampere , requiring 6 time 9 time le infrastructure depending model . 's really designed transformer layer talked , take advantage reduced precision mixed precision stack still maintain accuracy . 's easy offer bit [ ph ] ﬂoating point , 's hard make work work well . fact , use selene supercomputer train different heuristic q - brett simpson [ ph ] baked software/hardware hopper make successful . part hopper made nlp applicable dramatically improved inference performance . performance take run model production , train deploy . hopper 30 time faster ampere . allows people take largest possible model still run reasonable amount infrastructure . single dgx-like system deliver reasonable real-time performance running model . going dramatically broaden applicability nlp model . lot people run use case probably saw previously , bespoke offering large model . 's goal , 're definitely seeing lot interest deploying hopper . , course , next step else generate . else large language model produce question-and-answer kind response , chat box response customer service response sentiment response ? 're starting see hit mixed modality space generative ai . work done stable diffusion folk like stability ai midjourney , runway others showing take large language model connect image generation output , picture rather -- text . 's super exciting example another place large language model underlying generative portion ai , understanding corpus image mean , ( multiple speaker ) another great example . project farther future , imagine ai generating sort thing , generating potential chemical compound next-generation therapeutic , material property next- generation manufacturing material science . came back supercomputing conference dallas , texas , talk town we're building foundational -- using supercomputer build foundational model science hpc next-generation use case across science community . sort opportunity . -- 's going get u access platform technology researcher , user , company take advantage deploy different use case . that's make part job fun , help get . maybe focus little bit training guess 've seen last 12 month , mean , meta quite public , 've built massive cluster 16,000 a100s , think earnings report , talked microsoft rolling ten thousand gpus . think oracle also pretty active building large training cluster , large exaﬂop supercomputer . heading phase amount people keep building massive training cluster going consolidate handful player ? see hundred potential training cluster like getting developed ? 'm trying get sense -- see training evolving . - ian buckand guess part also , want come back service opportunity license pre-trained model . going lot enterprise get involved ai rather train everything ground , license something 's pre-trained 'll pay service fee rather necessarily buying big hardware cluster ? well , one thing sort bet ai still n't -- tapped different application ai . fundamentally , ai statistical trick , , algorithm take data write code . case microsoft , 's literally 's . 's helping write code co-pilot program . application ai seem balanced -- everything 're computing enterprise revolves around data collect customer , data business generates , operates teach u communication customer ﬂow business also generates massive amount data interpreted understood taking advantage ai improve make better . -- continued grow . result , access infrastructure developing ai , either -- first principle foundational level , taking existing foundational model applying growing . -- 's definitely interest start-ups others , major isvs big company explore , explore building new foundational capability . , need infrastructure . 's interesting cloud starting provide infrastructure building ai supercomputer bespoke supercomputer . certainly , microsoft led way announcement providing independent interconnected gpu cluster cloud people rent kind infrastructure . 're seeing oracle many others company like meta , building infrastructure . 're one many either going build on-prem able consume rented cloud . 's 're seeing growth ai infrastructure particularly scale-out infrastructure available cloud . 's important note n't rent . 're designed fractionalized way one note . everything single , focused around single node capability many gpus fit single server , usually maxing eight 16. infrastructure cloud scalable thinking data -- renting entire data center row [ ph ] pod collection half rack various work developing foundational model going serve different industry . 's broad . 's largest largest model , one tend get excitement press certain capability . designing model , new kind model different kind workload foundation level requires level infrastructure scale . q - brett simpson - ian buckthen second part , course , -- applying -- taking foundation model applying different use case business , certainly see thing like speech ai foundational model 's trained english , even [ ph ] want certain accident certain voice . thing n't need retrained scratch . take foundational model deploy . 's business figuring take advantage service -- sort different provider go consume . 's causing growth gpus across cloud still on-prem . think 's ebb ﬂow term on-cloud versus enterprise , n't -- 're platform choice . try activate channel . 's 's coming . n't think 's -- ai squirreling away corner people afford . -- 's certainly interest exploring outer limit ai large infrastructure , continue . believe 're seeing diversification different use case developing foundational model , sort different scale different kind player . ability consume get cloud well on-prem . finally , breadth different service like mentioned , take advantage -- 's trained , 's learned [ ph ] service . yeah . interesting . interesting . thinking -- mean , mentioned 're still early phase building workload compute requirement go . guess look hyperscalers bleeding edge today making big investment public cloud serving instance thousand enterprise , guess break today , see guy maybe spending multiple billion year nvidia . break overall revenue , see 're sort stage . large investment made hyperscalers . see scaling ? looking -- inevitable hyperscalers spending $ 10 billion plus point soon ? think fortune 500 , sizing investment opportunity ? see fortune 500 company spending $ 1 billion infrastructure really differentiate ai ? think year , development opportunity nvidia specifically ? yeah . mean think 're still early stage . think -- 's diﬃcult project dollar . obviously , 's exciting continue exciting , definitely see growth moving forward . one way look 's logical case every server inside hyperscale data center accelerated level 're operating data either ﬂowing data center east-west within data center . -- every bit data logically interpreted , understood ai make insight improve function operation data center 's trying impact outcome result . today , probably le - - around le 10 % hyperscale data center accelerated . what's preventing -- cause growth people identifying capability ai could impact improve part workﬂow , part user story , part capability part data center . particularly time data center space precious , 've certainly seen growth data center n't pop overnight . take year plan year build . -- accelerated computing offer great way optimizing data center space moving stuff cpu-based service , consume lot using ai reduce amount infrastructure preserve space norm [ ph ] grow , data center space important metric , foundation grow accelerating computing allows le data center space . likewise , power energy eﬃciency . operation scale much lower total energy cost cpu infrastructure , may able . there's lot interest identifying workload shifting accelerated portion data center order thing , optimize data center usage , improve throughput workﬂow consuming le data center space eﬃcient power order naturally grow . think -- application . , course , may -- happen inside data center , 's diﬃcult folk outside see . first people hyperscalers 're seeing service 're building , also make service publicly visible projection get translated inside operational line inside business . enterprise side , 's activation different enterprise isvs major enterprise service company , 're seeing . 're seeing -- go look success story talked jensen's talked gdc keynote , look -- oracle world joined stage safra talked enterprise adopting . 's question -- 's still fairly small percentage total enterprise software stack ( inaudible ) ai . 's lot valid reason , something 's going changing fairly quickly . 's becoming point order competitive enterprise software world , need deploying , offering capability giving cio cfo consumer isv software , fortune 500 , need [ ph ] strategy . imagine , 're asking isvs , leveraging ai make business run better service incorporated service already ? 're already stage lot baseline functionality capability , obviously , operation enterprise served major enterprise isvs . 're trying hard right , others , activating ai foundational enterprise software stack . q - brett simpson - ian buckand , 'll grow -- 'll get faster , eﬃcient 'll see adoption gpus across cloud data center well on-prem offering company run workload eﬃciently , low possible latency order deliver need service . think 's interesting thing track . think 's looking isvs 're deploying ai major fortune 500 , 're deploying . place , think , look sector well , interesting thing happening retail . 're talking -- think mcdonald 's publicly talked pilot -- speech ai , sort talking -- looking different way providing different kind service . obviously ai-driven exciting see retail community take advantage ai . certainly look segment -- vertical vertical segment . one two player go , see opportunity , 'll turn around excited take advantage . 's lot investment activity happening across enterprise . role obviously , working isvs provide foundational layer company -- fortune 500 owner need get direct support nvidia back that's kind nvidia ai enterprise offering market . yeah . yeah . make sense . maybe wanted talk little bit -- mean , guess , look 2023 , big architecture change compute generally , dpu going start become thing . think , obviously , 're removed grace cpu side hopper big architecture change interconnect sits around . would describe transition we're seeing -- much cpu-centric approach ? -- compute complex becoming platform sale nvidia rather selling sort discrete card ? content ? guess looking content today within server , see dramatic increase -- ramp bluefield grace switching infrastructure developing ? yeah . think couple thing . one , obviously , -- existing business , people interested getting access great infrastructure computing , 'll continue -- today x86 gpus , either sort standard server accelerator , look like pcie card . 's quite large one , similar may seen geforce product , go standard server eight 16 . n't graphic connector , 're optimized computing . also come smaller size inference use case , tend much smaller edge like accelerator . go -- go way embedded space , measured single-digit watt , 's deployed edge kind use case even conference room telco kind application robotics . go way server explicitly designed computing scale , see hgx dgx solution . moving forward , think -- 's interesting trend toward data center unit compute . people looking server component chip order advancement computing ai think looking entire data center optimize entire data center compute . many year ago , data center hyperscalers , basically map-reduced [ ph ] , kind sql-like data center driven i/o compute . would deliberately choose smaller cpu focus interconnect scale along storage . compute king , 're looking entire data center , improve compute utility entire data center optimize unit -- total unit computing . look way , want look capability data center . accelerator , obviously , cpu , they're connected , network integrated together far scale across data center . see nvidia investing three . 've done -- obviously , continued path accelerator . 're building dpus . -- bluefield 2 , 've announced bluefield 3. we've got strong road map well going way bluefield 4 connecting interconnect data center together intelligently , allow thing like in-network computing , operation done line speed network provide security isolation also -- software-defined networking people demonstrated eﬃcient providing kind service , particularly cloud-based use case , important , well needing performant . 're seeing cloud provider provide infiniband infrastructure alongside high-speed ethernet , good market depending different use case , 're serve . certainly , demand high-speed high-performance ethernet infiniband growing quite bit , want ( inaudible ) intelligently applying dpu . 's rolling across seeing traction cpu side . one interesting thing cpu traditionally , they're connected gpu via pcie express standard bus . continue support . 'll support long shall live , basically . opportunity . -- another capability provide , perhaps bespoke capability putting grace next hopper gpu tightly integrate two . fact , 've done grace hopper product , announced year , put two chip next built high- speed interconnect , nvlink chip-to-chip interconnect , offer 900 gigabyte second bandwidth wo , 's fully coherent . 's big uplift get pcie , maybe 100 gigabyte second , 50 100 , 're getting 900 grace hopper 's fully coherent . gpu operate entire data 's contained entire server , cpu ( inaudible ) gpu memory . really make allow u work largest model like talked , basically operating different scale , 're seeing interest q - brett simpson q - jim fontanelli - ian buckgrace hopper large-scale recommenders deploying large-scale nlp . rest market going -- obviously , well optimized run everything gpu , continue exist quite time . 're seeing interesting opportunity grace hopper sort push limit go training inference , able deploy large model minimal infrastructure provides sort real-time latency performance may spread model across multiple gpus , 'll able execute single gpu combined grace memory deliver large language model real time . excited , 's another interest hpc supercomputing community well . really like coherence cpu gpu , really love arm architecture ecosystem come long way term arm . we're -- see everyone investing arm well across different vertical . excited see take shape 'll bringing grace market next year . great . interesting . well , think 're -- 'm conscious time . think good segue perhaps open q & . 've got colleague , jim line . jim , relay question 've picked investor far ? yeah , sure . first question , think quite straightforward , quickly take hopper majority data center shipment ? yeah . every transition little -- 're like little different see gaming side , tends bit switchover . data center , we're operating enterprise use case . people qualified deployed scale , ampere a100 gpu obviously quite successful still quite diﬃcult get cloud try ask instance , still show sold lot place . expect continue trend . usually , transition happen period quarter different -- hopper becomes available different market . first , 'll [ ph ] come market -- coming market pcie product , get major oems . hgx nvlink connected baseboard product coming market q1 next year cloud well . h1 , 're going transition differently also different region . 's nature work work -- hard work take qualify server hyperscale entire hyperscale data center deploy scale , 's fire forget , data center massive . way execute scale make sure best possible quality test work scale . happen throughout 2023 expecting a100 continue 2024 well , slowly blend . saw a100 , 'll see . q - brett simpson - ian buck q - brett simpson q - jim fontanelli - ian buckcourse , everything inﬂuenced economics market condition trend ai . certainly , lot largest customer large ( inaudible ) interested getting access hopper quickly . 'm looking forward seeing announcement . reference , ian , long take a100 cross 100 ? sense benchmark ? yeah . saw -- believe six eight quarter worth -- continued -- voltaire [ ph ] point largely tailed , least , think , six eight quarter transition . lot go , obviously , 's kind saw last time . jim , maybe got time one . ... yeah . maybe longer-term question , might good place wrap . question industry often look transition decade cycle . think estimate around 3 % 4 % server accelerated globally '22 . think market tam adoption curve regard server get accelerated go decade ? 's certainly picking . mean -- 's one reason 're investing accelerated roadmap new gpus every two year , new cpu every two year dpus every two year . simply demand interest -- n't like cpu wait process technology transition memory technology transition . 've dialed tuned manufacturing process ship often a01 silicon production product like ampere allows people get access technology even sooner . , opportunity acceleration become richer sooner unlike perhaps era moore 's law , waited next technology transition get faster . 's people want get faster , faster . -- main limiter , think one main limiter number company , isvs fortune 500 -- get building talent pool figure adopt technology use case . nvidia , -- provide platform , case , vertical platform like clara merlin recommender system maxine teleconference communication . product solution way . provide foundational layer allows enterprise meet ( inaudible ) gap . q - brett simpson - ian buck q - brett simpson - simona jankowski - ian buckthat -- player enter space get access technology affordably , especially hopper reducing cost access perform ai infrastructure even perform ai infrastructure , say , ramp driver broader adoption platform across enterprise . 'm expecting , like said , 're still beginning little curve 3 % 4 % . expect isvs adopt , company invest , service get launched everyone , number start -- continue curve even . certainly , era compute paramount access infrastructure key yet , 're bit data center crunch perhaps importance running thing eﬃciently greener capability , better usage -- power access , 's compounding factor driving growth . think 's going exciting . exciting . hopper transition yet another turn crank , 2023 bring even , '24 . 'm happy keep coming back guy telling next thing roll [ ph ] . yeah . 'll definitely take , ian . hopper anything good result saw ampere , 's going interesting couple year . good luck new product introduction next year . thank . simona , thanks much time . great discussion could gone couple page question , guess 'll read another time . right . well , thanks much joining u , guy . really appreciate end session . thanks much . thank . thank .