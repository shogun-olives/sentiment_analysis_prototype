hi everyone , welcome gtc . 's simona jankowski , head investor relation nvidia . hope chance view jensen 's news pack keynote morning . also published several press release vlogs detailing today's announcement . next hour opportunity unpack discus today 's news ceo , jensen huang ; cfo , colette kress open q & session financial analyst . begin , let quickly cover safe harbor statement . today's discussion , may make forward-looking statement based current expectation . subject number significant risk uncertainty actual result may differ materially . discussion factor could affect future financial result business , please refer recent form 10-k 10-q report may file form 8-k security exchange commission . statement made today , based information currently available u , except required law , assume obligation update statement . jensen huangwe 'll start today brief comment jensen , followed q & session jensen colette . let turn jensen . hi , everybody , welcome gtc . gtc conference developer , inspire world deposit -- possibility accelerated computing celebrate work researcher scientist use . , please sure check-in conference session . cover really amazing topic . gtc keynote highlighted several thing . let -- go slide , 'm going , colette , cover basically first slide , rest slide provided reference . -- let make couple comment first . core computing today , fundamental dynamic work course inï¬‚uenced one important technology driver history industry , moore 's law . fundamentally come significant slowdown , could argue 's moore's law ended . first time history , longer possible using general-purpose computing , cpu , gain necessary throughput without also corresponding amount increase cost power . lack decreasing power effectively decreasing cost going make really hard world continue sustain increased workload , maintaining sustainability computing . one important factor dynamic computing today sustainability . accelerate workload , reclaim power use whatever reclaim invest back growth . first thing waste power , -- accelerate everything possibly really focused sustainability . gave several example workload used , highlight many case accelerate application 50 time 40 , 50 , 60 , 70 time 100 time . process , decreasing power order magnitude , decrease cost factor 20. approach easy . accelerated computing , full stack challenge , nvidia accelerated computing , 's full stack . 've talked many session past , start architecture system , system software , acceleration library application ( inaudible ) . 're data center scale computing architecture reason , refactor application accelerated , algorithm highly paralyzed . , also scale . one benefit accelerated computing work , scale , also scale . combination allowed u bring million x acceleration factor many application domain , course , one important one artificial intelligence . nvidia 's accelerated computing platform also multi-domain . really important data center , computer single-use device . make computer incredible instrument ability process multiple type application . nvidia 's accelerated computing multi-domain . particle physic , ï¬‚uid dynamic , way robotics , artificial intelligence , forth , computer graphic , image processing , video processing , type domain consume enormous amount cpu core today , enormous amount power . opportunity accelerate reduce power , reduce cost . course nvidia 's accelerated computing platform cloud edge . architecture available every cloud , 's available on-prem every computer maker world 's available edge inferencing system autonomous system , robotic self-driving car , forth . lastly , one important characteristic nvidia's accelerated computing platform although full stack , design architect data center scale , 's available cloud edge . completely open , meaning access literally computing platform , computing maker , anywhere world . one important characteristic computing platform 's openness , reach , acceleration capability positive -- virtuous cycle -- positive virtuous cycle accelerated computing achieved . accelerated computing artificial intelligence arrived . talked three dynamic . one sustainability , mentioned . second generative ai . foundational work done last 10 year , beginning really big breakthrough computer vision perception led industrial revolution autonomous vehicle , robotics . tipping -- tip iceberg . generative ai , gone beyond perception generation information . longer understanding world , also make recommendation generic content great value . generative ai triggered inï¬‚ection point artificial intelligence 's driven step-function increase adoption ai world importantly step-function increase amount inference , deployed world 's cloud data center . third thing mentioned -- discussed keynote digitalization . really taking artificial intelligence next phase , next wave ai , ai operating digital information , generating text generating image ai operating factory physical plant autonomous system robotics . particular case , digitalization real opportunity automate world 's largest industry . spoke digitalization one particular industry , gave example omniverse digital , physical operating system industrial digitalization . demonstrated omniverse used beginning product conception , architecture , styling product design , way collaboration design , simulation product , engineering electronics setting virtual plant , way digital marketing retail . every aspect physical product company , digitalization opportunity automate , help collaborate bring world physical world digital know exactly happens . get world digital , ability accelerate workï¬‚ows , ability discover new product idea ability invent new business model tremendously increase . , spoke digitalization . five takeaway spoke keynote . 'll talk today , question area , love entertain . first course generative ai driving accelerating demand nvidia platform . came year full enthusiasm hopper launch , hopper designed transformer engine designed large language model people call foundation model . transformer model -- transformer engine proven incredibly successful . hopper adopted every cloud service provider know available oems . really signaling increase demand hopper versus previous generation accelerating demand , really signal inï¬‚ection ai , used ai research , generative ai moving deployment ai , world 's industry , importantly significant step-function inference ai model . generative ai driving accelerating demand . second thing , talked new chip coming marketplace . care deeply accelerating every possible workload . one important workload , course , artificial intelligence . another important workload accelerate operating system entire data center , imagine giant data center computer , ï¬‚eets computer orchestrated operated one giant system . operating system data center , includes containerization [ ph ] , virtualization , networking , storage , importantly , security , isolation future confidential computing application . 's operating software defined layer , software layer run across entire data center fabric . software layer consumes lot cpu core . frankly , would n't surprised many , depending type data center operated , would n't surprised 20 % , 30 % data center power dedicated networking fabric virtualization software-defined stack , basically operating system stack . want oï¬ „ oad , accelerate operating system modern software-defined data center , processor called bluefield . announced whole bunch new partner cloud data center adopted bluefield , 'm excited product . really believe going one important contribution make modern data center . company design , company wo n't resource design something complexity , cloud data center everywhere . announced grace hopper , going used one major inference workload , vector database , data processing , recommender system . recommender system 've spoken past , 's probably one valuable important application world today . lot digital commerce lot digital content made possible sophisticated recommender system moving deep learning . important opportunity u . grace hopper designed specifically give u opportunity get 10x speed-up recommender system large database . spoke grace . grace production , grace also sampling . grace designed rest workload cloud data center , possible accelerate . accelerate everything , left software , really want strong single threaded performance . single threaded performance grace designed . also designed grace cpu fast computer cpu , energy-eï¬ƒcient cloud data center . think entire data center one computer , data center computer , way designed cpu context accelerated data center ai first , cloud first data center , cpu design radically different . designed grace cpu -- excuse -- slightly reach . grace cpu designed , entire computer module , n't cpu , entire computer module grace superchip . go possibly called system could rack whole bunch grace computer cloud data center , energy-eï¬ƒcient yet performance single- threaded operation . 're really excited grace sampling . let 's see . spoke lot generative ai 's step-function increase amount inference workload 're going see . one thing 's really important inference coming world 's data center , really want accelerated one hand . hand , multi- modal , meaning , many different type workload want inference . sometimes want inference , want bring inference ai video 're augmented generative ai . sometimes 's image , producing beautiful image helping co-creator . sometimes 're generating text , long text , prompt could quite long , long context could generating long text , writing long program . application , one video , image , text , course also vector database , different characteristic . another challenge course cloud data center . one hand , would like specialized accelerator one modality one diverse generative ai workload . hand , would like data center fungible , workload moving , 're dynamic , new service coming , new tenant coming . people use different service different time day yet would like entire data center utilized much possible . power architecture , one architecture , one architecture four different configuration , run software stack , mean depending time day , one under-provisioned under-utilized , could always provision class configuration accelerator workload . fungibility data center give ability , architecture -- one architecture , inference configuration , inference platform give ability accelerate various workload best ability perfectly precisely predict amount workload entire data center ï¬‚exible fungible . one architecture , four configuration . one biggest area collaboration collaboration partnership google cloud gcp . 're working gcp , across large area accelerated workload data processing data product , spark rapid accelerate data product represents data processing , probably represents 10 % , 20 % , 25 % cloud data center workload . 's probably one intensive cpu core workload . opportunity , accelerating , bring 20x speed-up , bring lot cost-reduction customer enjoy . importantly , lot power reduction , 's associated . 're also accelerating inference triton server . 're also accelerating generative ai model . google world-class pioneering large language model 're accelerating putting onto inference platform l4 . , course , streaming graphic streaming video , opportunity accelerate . two team collaborating take large amount workload could accelerated generative ai accelerated computing workload accelerating l4 platform , gone public gcp . 're really excited collaboration , much tell soon . third thing talked acceleration library . mentioned , accelerated computing full stack challenge . unlike cpu , software written 's compiled using compiler general purpose code run , 's one wonderful advantage breakthrough cpu , general-purpose . acceleration aspect , want accelerate workload , redesign application , refactor algorithm altogether codify algorithm acceleration library . acceleration library way linear algebra fft , data processing , use ï¬‚uid dynamic particle physic computer graphic forth . quantum chemistry , inverse physic image reconstruction , forth . one domain require acceleration library . every acceleration library requires u understand domain , work ecosystem , create acceleration library connect application ecosystem power accelerate domain use . -- every single 're constantly improving acceleration library , installed-base benefit increased optimization investment capital already , infrastructure already . buy nvidia's system benefit acceleration year come . 's unusual u , platform increase performance anywhere four 10 time 've installed life . 're delighted continue improve library bring new feature optimization . year , optimized released 100 library 100 model -- 100 library model , better performance better capability . also announced several important new library . one new library i'll highlight culitho . computational lithography inverse physic problem calculates max -- process calculates maximize equation go optic interacts photoresist mask . ability basically inverse physic image processing make possible u , use wavelength light , much , much larger final pattern want create wafer . 's miracle fact , look modern microchip manufacturing , 're latest generation , 're using 13.5 nanometer light near x-ray , 's extreme ultraviolet yet using 13.5 nanometer light . could pattern nanometer , three nanometer , five-nanometer pattern wafer . mean , 's basically like using fuzzy light , fuzzy pen , create really fine pattern piece paper . ability requires magical instrument like asml 's magical instrument , computational library synopsys , miracle work tsmc . field imaging called computational lithography . 've worked last several year accelerate entire pipeline . single largest workload eda today , computationally intense . million million cpu core running all-the-time , order make possible u create different mask . step manufacturing process going get lot complicated coming year , magic 're going bring future lithography going get increasingly high machine learning artificial intelligence surely involved . first step u take entire stack accelerate . course last four year , 've accelerated computational lithography 50 time . , course , reduces cycle time pipeline throughput time chip world manufactured , really quite fantastic $ 40 billion , $ 50 billion investment factory , could reduce cycle time even 10 % , value world really quite extraordinary . thing 's really fantastic , also save enormous amount power . case tsmc work 've done far , opportunity take megawatt , ten megawatt reduce factor 5 10. reduction power , course , make manufacturing sustainable 's important initiative u . culitho , 'm excited . lastly , 'll talk single largest expansion business model history . know world becoming heavily cloud-first . cloud give opportunity engage computing platform quickly , instantly web browser . last -- last 10 year , capability cloud continued advance . point started cpu running hadoop mapreduce inquiry beginning , know , 're high performance computing , scientific computing system , ai supercomputer cloud . going partner world 's cloud service provider . starting oci , 've also announced cloud partnership azure gcp . 're going partner world 's leading cloud service provider implement install host nvidia ai , nvidia omniverse nvidia dgx cloud , cloud . incredible capability , one hand get fully optimized multi-cloud stack nvidia ai nvidia omniverse . opportunity enjoyed world 's cloud optimized configuration . get benefit nvidia software stack optimal form . benefit working directly nvidia computer scientist expert . company large workload would like benefit acceleration , benefit advanced ai , direct service engage world 's industry . 's wonderful way u combine best nvidia brings best csps . incredible service security , cloud -- security , storage , api service offer . well could -- likely already cloud 've selected . first time ability combine best world bring nvidia 's best combine csps best make capability available world 's industry . one service announced , 's -- 's platform service , nvidia ai , nvidia omniverse infrastructure service nvidia dgx cloud . also offered -- announced new layer . many customer work , many industry partner work , build foundational model . -- customer enterprise , industry would like access foundation model , obvious accessible thing work world leading service provider like open ai microsoft google . example ai model designed highly available , highly ï¬‚exible useful many industry . company want build custom model , based specifically data nvidia capability . customer would like build custom model based proprietary data , trained developed inference specific way , whether 's guard rail would like put , implement type instruction tuning would like perform type proprietary datasets would like retrieved . whatever specific requirement language model , generative image model 2d , 3d video biology , service allows u directly work help create model , fine-tune model deploy model nvidia dgx cloud . mentioned , dgx cloud run world 's major csps . already csp choice , 'm pretty certain 'll able hosted , okay . nvidia cloud service going expand business model offer infrastructure service , dgx cloud , platform service , - simona jankowski q - toshiya hari - jensen huang q - toshiya hari - jensen huangnvidia ai nvidia omniverse . new ai service designed custom , essentially foundry ai model available world's industry . world -- partnership world 's leading csps . 's , announcement made . lot go . thanks joining gtc . , colette answer question . question answer thank , jensen . let welcome financial analyst q & session . we're going taking question zoom , please use raise hand feature zoom would like ask question unmute called upon . 'll pause moment review queue take first question . first question toshiya hari goldman sachs . hi , jensen colette , hear okay ? perfectly . nice see . nice ( multiple speaker ) yeah . thank much hosting follow-up . jensen , guess , one question inference opportunity . obviously , dominate training space 've done many , many year . think inference side , competitive landscape little bit mixed , given incumbency around cpu , obviously encouraging see introduce new inference platform . guess , criticality recommender system spoke growth llm work google , seems like market moving direction . think opportunity inference , call three five year versus stand today think grace playing role next couple year ? thank . toshiya , thank . first , 'll work backwards . three five year , ai supercomputer building today , unquestionably advanced computer , world make today . course gigantic scale , includes computing fabric like nvlink , computing -- large computing -- large-scale computing fabric like infiniband sophisticated networking stitch altogether . software stack , operating system distributed computing u , software , 's computer science limit . 's really going quite - simona jankowskiexciting ai supercomputer going go beyond research extending essentially ai factory ai model people develop going fine-tuned improved basically forever . believe every company intelligence manufacturer . core company , produce intelligence . valuable data proprietary , 're inside wall company . capability create build ai system help curate data , package data together could used help train proprietary model -- custom model , accelerate business . system , ai training system continuous . second inference , inference largely cpu oriented workload . reason inference world today fairly lightweight . might recommending something related shopping ( inaudible ) -- forth . kind recommendation largely done cpu . future several reason even video processed cpu today . future likely happen , two fundamental dynamic inescapable point 's -- inevitable quite long-time , inescapable . one sustainability . ca n't continue take video workload process cpu . ca n't take deep learning model , even quality service little bit lesser good using cpu , burn -- 's burn much power . first reason accelerate everything sustainability , accelerate everything moore 's law ended . sensibility permeated , every single cloud service provider , amount workload requires acceleration increased much . -- attention acceleration , alertness acceleration increased . secondarily , everybody power limit . order grow future , really reclaim power acceleration put back growth . second reason generative ai arrived . 're going see every single industry benefiting augmenting co-creators , co-pilot , accelerates everything text , text create , chatbots interact , spreadsheet use , powerpoint photoshop , forth . 're going going augmented , 're going accelerated , inspired co-creator co-pilot . think net ai training , ai supercomputer become ai factory , every company -- either on-prem cloud . secondarily , every interaction computer future , 'll generative ai connected therefore amount inference workload quite large . sense , inference balance larger inference , larger training , training going right . thank . next question come c.j . muse evercore . q - c.j . muse - jensen huang q - c.j . muse - jensen huanggood morning . good afternoon . hear ? yes , c.j . nice talk . perfect . well , thank today . want put question , 'd like focus grace . past 've mostly discussed benefit grace hopper combined . today , 're also focusing bit grace standalone basis kind expecting . speak whether 've changed view expected service ( technical diï¬ƒculty ) ? think potential revenue contribution time ? particularly think grace standalone , grace superchip obviously grace hopper combined ? 'll start punch line , work backwards . think grace big business u , -- nowhere near scale accelerated computing . reason genuinely feel every workload accelerated must accelerated . everything data processing , course , computer graphic video processing generative ai . every workload accelerated must accelerated , basically leaf workload ca n't accelerated , meaning converse , another way saying , single- threaded code . 's single-threaded code amdahl 's law still prevails , everything left becomes bottleneck . single-threaded code largely related point data processing , fetching lot -- moving lot data . design cpu , really good two thing . well , mean , say -- let , two thing , plus design point . two characteristic really want cpu one extremely good single- threaded performance . 's many core , 's good single threated core . number two , amount data move extraordinary . one module move one terabyte per second data , 's extraordinary amount data move . want move , want process data extremely low power , reason innovated new way using cellphone dram , enhanced data center resilience used server . 's cost effective obviously cellphone volume high , power one-eighth power moving data going much workload vital u reduce . lastly , designed whole system instead building super-fast cpu core -- cpu . designed superfast cpu node . , enhance ability data center powered limited able use many cpu possible . think net accelerated computing dominant form computing future moore 's law come end . - simona jankowski q - joseph moore - jensen huangbut going remain going heavy data processing , heavy data movement single-threaded code . cpu remain , important . 's design point would different past . next question come joe moore morgan stanley . please go ahead . great . thank much . 'm told follow-up inference 's question . cost per query becoming major focus generative ai customer talking pretty significant reduction quarter year ahead . talk mean nvidia ? going h100 workload longer-term guy work customer get cost ? yeah . 's couple dynamic moving time . one- hand , model going get larger . reason 're going get larger wanted perform task better better better . 's every evidence capability , quality versatility model correlated size model amount data train model . one-hand , want larger larger , versatile . hand , many different type work workload . remember n't need largest model inference every single workload 's reason 530 [ ph ] billion parameter model , 40 billion parameter model , 20 billion parameter model even 8 billion parameter model . different model created way largely -- always need large model , reason need large model minimum , large model used help improve quality smaller model . case , kind like need professor improve quality student improve quality student forth . -- many different use case , 're going different size model . optimize across , use rightsized model rightsized application . inference platform extends way l4 l40 . one one announced week , incredible thing , hopper h100 nvlink , call h100 nvl . basically two hopper connected nvlink . result , 180 gigabyte -- 190 gigabyte , almost 190 gigabyte hbm3 memory . 190-gigabyte memory give ability inference modern large- size inference language model , way , would like use small configuration , dual h100 system solution let partition 18 , 18 -- 16 different -- correct 'm wrong later , 16 18 called multiple instance gpus mix . miniature gpus -- fraction gpus could inferencing different language model whole thing could connected four could put pci express server -- commodity server , used distribute large model across . already reduced , performance incredible . already reduced cost language - simona jankowski q - timothy arcuri - jensen huang - simona jankowski q - vivek aryainferencing factor 10 , a100 . 're going continue improve every single dimension , making language model better , making small model effective , well making inference cost effective new inference platform like nvl . importantly , software stack , 're constantly improving software stack course last couple two , three year , 've improved much . mean , order magnitude couple year 're expecting continue . next question come tim arcuri ubs . please go ahead . thanks lot . jensen , think , thought heard say , google 's inferencing large language model system . wanted confirm 's saying , guess , mean 're using new l4 platform ? -- brand new ? word , using tpu [ ph ] using new l4 platform ? curious detail . thanks . partnership gcp , big event inï¬‚ection point ai , 's also inï¬‚ection point partnership . lot engineer working together bring state-of-the art model google cloud . l4 versatile inference platform , could use video inferencing , image generation generative model , text generation large language model . mentioned keynote , model 're working together google bring l4 platform . , l4 going -- 's phenomenal inference platform , energy-eï¬ƒcient , 75 watt . performance chart 's incredibly easy deploy . -- l4 one end , 'll show , l4 , l4 -- l4 h100 . okay ? l4 , l4 . two processor , 700 watt 75 watt . power architecture , one , software stack , run well . depending model size , depending quality service , would like deploy , could infrastructure , fungible . 'm really excited partnership gcp model going bring inference platform gcp basically across board . next question come vivek arya bank america . please go ahead . thanks taking question thank jensen colette , informative event . near-term longer-term question . near-term , curious availability hopper , term supply ? - colette kress - jensen huang - simona jankowskilong-term , jensen , heard range software service innovation . track progress , right ? last number think heard term software sale 100 million , 1 % sale . would consider success next year ? percentage sale think could come software subscription overtime ? thank . let first start , vivek statement regarding supply h100 . yes , continue building h100 , demand 've seen quarter . keep mind , 're also seen stronger demand hyperscale customer data center platform focus generative ai . even last month since 've talked earnings , 're seeing more-and-more demand . feel confident able serve market , continue build supply , feel 're good space time . think software service substantial part business . however , know , serve market every layer , 're full stack company , 're open platform , meaning company would like -- customer would like work u infrastructure level , hardware level , we're delighted . would like work u hardware plus library level , 're delighted . platform level 're delighted . customer would like work u way service level level , inclusive , 're delighted . opportunity grow three layer , hardware layer course already large business , colette mentioned , part business , generative ai driving acceleration business . platform layer -- two layer stood cloud service . company 'd like on-prem , 're going based subscription . however , know today world multi-cloud , really need software cloud , well on-prem . ability u multi- cloud , hybrid cloud real advantage real benefit two software platform beginning . lastly ai foundation service analyze beginning . would say , model presented last time includes sensibility 're talking today . 've talking , laying foundation path towards today . big day u launch -- probably biggest business model expansion initiative history company . think $ 300 million platform platform software ai software service , today pulled . -- still think 's -- size consistent 've described . next question come raji gill needham . q - rajvindra gill - jensen huang thank , jensen , thank presentation . question technological perspective , regarding relationship memory compute . mentioned generative ai model creating huge amount compute , think memory model ? view memory potential bottleneck , solve memory disaggregation problem ? 'll help understand . thank . yeah . , well , turn computing everything bottleneck . push limit computing , living , n't build normal computer , know , build extreme computer . build type computer build processing bottleneck . actual computation bottleneck . memory bandwidth bottleneck . memory capacity bottleneck . networking -- computing fabric bottleneck . networking bottleneck . utilization bottleneck , everything 's bottleneck . live world bottleneck , 're surrounded bottle . , thing , true , mentioning , amount memory use , memory capacity use increasing tremendously . reason , course , generative ai work training model require lot memory . inferencing requires lot memory . native -- actual inferencing language model n't necessarily require lot memory . however , -- want connect retrieval model , augments language model , augments chatbot , proprietary well curated data , custom , proprietary , important . maybe 's healthcare record , maybe 's particular type domain biology , maybe something chip design . maybe 's ai , 's database domain knowledge nvidia make nvidia click proprietary data embedded inside wall company . using large language model , could create datasets augment language model . , increasingly need large amount data , need large fast data . large amount data , many idea , course , work 's done ssds , work 's people cxl basically affordable attached disaggregate memory . fantastic , none fast memory . 's affordable memory , 's large amount accessible hot memory , none 's fast memory . need something like grace hopper . need 1 terabyte per second access 0.5 terabyte data . 1 terabyte per second 0.5 terabyte data , wanted petabyte data distributed computing system , imagine much bandwidth we're bringing bear . approach high speed , high capacity data processing exactly grace hopper designed . - simona jankowski q - stacy rasgon - jensen huang - simona jankowski q - aaron rakers next question come stacy rasgon bernstein research . please go ahead . hey guy , thanks taking question . appreciate . wondering could go little bit economics dgx cloud business . like actually pay infrastructure cloud vendor pay debt ( inaudible ) lease back , 're running ? guess work customer paid get upside economics customer , pricing ? anything give u actually work impact model would super helpful . yes . stacy , thank . first , process go like , presented nvidia dgx cloud partnership csp partner , super excited . reason 's -- onboarding , well important customer large partner would consume storage , security , whole bunch application apis . presented idea would like rent nvidia dgx cloud would take instance -- reserved instance , called reserved instance market engage customer , super , super happy . obviously , nvidia deep relationship many large vertical ecosystem world , highlighted slide deck sense guy healthcare drug discovery , deep relationship many company . deep relationship every car company planet . two industry , particularly great deal urgency take advantage latest generation ai -- generative ai omniverse digitalization . first thing , present idea , proposal proposal partnership . 're interested far 've incredibly enthusiastic , would purchase system -- include people 's gear also includes gear standup dgx cloud . cloud service provider , procure whatever infrastructure , power , networking , storage , forth , order standup infrastructure hosted manage . okay , 's step two . step three take dgx cloud service market . combination value would deliver would set price engage customer directly engage customer business . next question come aaron rakers well fargo . please go ahead . - jensen huang - simona jankowskiyeah . thanks taking question . want go back , think maybe c.j . 's question earlier , kind breadth grace , maybe grace superchip , grace cpu strategy . think kind evolution , maybe help u appreciate , much data center cloud workload single threaded performance ? context , foresee situation actually -- see server partner deploying , grace cpu without necessarily deploying h100 subsequent version gpus . see actual single cpu deployment market opportunity ? 'll look backward , appreciate question , answer yes . however , grace really , really targeted niche market let clear . x86 -- use x86 company . use x86 , obviously pc , workstation , exciting go-to-markets intel sapphire rapid new workstation line . 're using sapphire rapid dgx , 're using sapphire rapid ovx server . 're single-threaded performance sapphire rapid really quite good , 's excellent fact . mentioned , take application accelerate workload -- work -- part curve accelerate ( inaudible ) . really 's left , single-threaded code . single- threaded code either control oftentimes 's moving memory around -- giant amount memory managing memory . amount data managing growing quite tremendously . mentioned , grace really designed type application data center largely accelerated well moving lot data . said , customer need x86 , obviously , represents large part world remains large part world . expect continue , x86 predominant platform n't make sense move enterprise computing arm , necessary grace necessarily . think 're focused application mentioned . however , csps already going move arm [ ph ] , would like , would like build cpu bespoke need requirement . grace really great companion reason , design point designed grace different design point almost cpu 've known designed . think cloud data center moving direction arm , really wonderful way either accelerate , benefit entire software expertise system ecosystem peripheral ecosystem 've brought grace design point special . 's really designed energy eï¬ƒcient , extreme energy eï¬ƒcient cloud data center . , anybody interested particular area everyone world , 's also important segment world . think grace going successful even independent standalone cpu . q - matthew ramsay - jensen huangour next question matt ramsay cowen . thank , simona . thanks , jensen colette . guess , two question jensen one kind follow-up something 'd asked , prior call . , transition happening , think - - data center business , selling accelerate card selling system . 'm really interested mean economics data center business , term margin long-term ? guess second question little bit related extends bit dgx cloud opportunity . wanted -- one question 've getting lot last month , month half since announced maybe take microsoft acute example , like guy partnering ? -- really partnership ? friction point might want customer relationship , evolve time ? could kind walk u , seems like would want ai customer , guy -- going go directly rented space cloud . , evolved overtime relationship largest csp customer bring business market ? thanks . really appreciate question . first question , ca n't build software , can't develop software . ca n't generally develop software , 're system company . reason , ca n't build software chip , chip n't sit computer . system company . especially , type software develop , 're trying replicate somebody's software , 're building bespoke brand new software . none software created existed created . even computer graphic rtx full path tracing ai generation used modern computer graphic n't possible created software . order create software , system . system company , make nvidia unique , build entire system data center . literally start data center , chip . start data center , build entire computer . future data center computer , entire data center computer . 's -- 's something 've spoken coming decade , one reason combination mellanox strategic , important , think people realize today . work together architect data center really quite foundational . way think world , see world -- entire data center , frankly , even planetary scale computer . think world starting . includes computing element , includes system , includes networking storage compute fabric cpu forth , way system software stack importantly algorithm , library . design data center way design , design discipline , disaggregate , fractionalize . customer would like buy , aged dgx gpu , right , gpu look like today . lot people think gpu look like , course , nvidia 's gpu run software stack . 's kind miracle . one run software , went slower take long-time . ability u design entire data center , disaggregate let customer decide 's best form factor , best configuration , best deployment methodology , people use mpis , people use kubernetes , people use vmware people use container bare metal list go . yet distributed computing stack affected . work work industry across layer software disaggregate system , component , system software . aggregate library . run anywhere like workstation , pc , way cloud supercomputer . disaggregate networking , disaggregate switch , disaggregate literally everything . 're delighted put together , would like u stand-up supercomputer like 30 day , 's possible , 've productized entire thing , disaggregated integrated world 's industry standard , wherever could . result computing platform literally everywhere 's binary compatible , 's magic . think 's one reason able -- one-hand system company develop software hand , computing platform company 's available everywhere . respect go-to-market , lose customer , csp would like direct customer relationship , 're delighted . reason whole bunch nvidia gpus cloud , 've nvidia computing cloud software platform cloud anyways . customer would like use way , download nvidia ai enterprise , run stack , forth . everything work exactly today . however , many customer 'd like need work u , refactor entire stack . expertise understand entire stack , take problem , otherwise barely possible . 's barely possible multi-cloud configuration , meaning would like run cloud well would like run azure oci gcp , well on- prem . expertise help . case need direct access engineer computer scientist also reason 're busy . 're working industry leader , would like build something quite special quite proprietary , based platform need computing expertise make possible either deploy - simona jankowski q - blayne curtis - jensen huangit scale want , reach multi-cloud want level cost power would like reduce . case contact u . notice , become direct customer interface , would still invite csp partner n't offer storage , n't offer rest apis , offer security . many industrial safety privacy data management regulation standard complied world 's leading csps expertise . lot collaboration 's going happen . come u , terrific . go csps , fabulous . 're happy either way . next question come blayne curtis barclays . please go ahead . hey , thanks let ask question . wanted ask inference 's kind two part , 's two ( inaudible ) , small model large . , guess , 'm curious , t4 card back ago , n't think anything ampere . l4 kind new version . 'm trying understand back t4 , think 're talking inference similar said kind large market , maybe even big training . think became cpu . 's changed , guess , 're feeling like smaller model need move accelerator ? trying understand large side , nvl like 700 watt , seems like lot power add every server . 's customer thinking deploying ? 's huge model , need lot horsepower , 's one-for- one every cpu . kind two part equation inference guy monetize ? yeah . thanks . t4 one successful product history . million t4s cloud . however ten million cpu cloud . 's still lot workload cloud , 's done cpu . two reason really need accelerated . one , course , sustainability . accelerate every workload . world ca n't continue consume power cpu throughput . 's number one . number two , generative ai inï¬‚ection point . 's question . capability ai , usefulness different industry , generative ai connected . think happened last couple month . generative ai connected popular application planet , oï¬ƒce , team , google doc . popular productivity application history humanity . 's term ai connected . inference somewhere . - simona jankowski q - w illiam stein - jensen huangand think nvidia platform really ideal platform inference , handle video , handle text , handle image , handle 3d , handle video , handle well ( inaudible ) , handle everything throw u . think really inï¬‚ection point . respect , reason ( inaudible ) nothing today 's cloud data center . thing really spectacular u , get replace hundred cpu server . 's reason accelerate . reason accelerate spend 700 watt , save 10 time , 700 watt 7 kilowatt . 's amount , want accelerate everything , sudden reclaim 6.9 kilowatt reinvest future workload . okay ? 's -- motion , conservation energy motion world csp going . accelerate workload , reclaim power , invested new growth . one , two , three step , way put gpus world csps easy pc [ ph ] today . thank . time one last question come stein truist . please go ahead . great . thanks much squeezing . jensen , several year ago really introduced world accelerate , also oï¬ „ oad parallel processing compute , maybe reintroduced , 's something used exist long-time ago . certainly modern time , like big computing , revolution way . product 're talking particular grace cpu bluefield dpu . talk , vision modern data center ? envision typical architecture looking like maybe three year , five year ? dpu relevant grace cpu relevant traditional x86 ? see x86 server , continuing perpetuate enterprise traditional enterprise software see going away ? i'd love sort long-term view . thank . really appreciate . believe data center next five 10 year , start 10 year work way back . even five year work way back , basically look like . ai factory insight ai factory working 24x7 . ai factory take data input , refine data transform data intelligence . ai factory data center , 's factory . reason 's factory one job , one job either refining improving enhancing large language model foundation model recommender system . factory job every single day , engineer constantly improving enhancing , giving new model , new data create new intelligence . every data center number one ai factory . inference ï¬‚eet , inference ï¬‚eet support diverse set workload . reason know video represents 80 % world 's internet today . video processed , generate text , generate image , generate 3d graphic . image 3d graphic populate virtual world . virtual world -- run omniverse type computer . omniverse computer , we'll course simulate physic insight , simulate autonomous agent inside . enable connect different application different tool would able essentially virtual integration plant , digital twin ï¬‚eets computer , self-driving car , forth . 'll type virtual world simulation computer . type inferencing system , whether 's 3d inferencing , case omniverse physic inferencing . case omniverse different domain generative ai . one configuration optimal domain . fungible , meaning , one architecture able receive oï¬ „ oad work something 's over-provisioned -- oversubscribed -- excuse , oversubscribed take -- pickup workload . okay , second part inference workload . every single one node smart nics , like dpu , data center operating system processing unit . going oï¬ „ oad -- oï¬ „ oad isolate , 's really important isolate , n't want tenant computer , basically inside . think world future zero trust . application communication isolated . 're either isolated encoding , 're isolated virtualization . operating system separated -- control plane [ ph ] separator compute plane . control plane , operating system data center run , oï¬ „ oaded , accelerated dpu , bluefield . 's another characteristic . lastly , whatever left , 's possible accelerate 're -- code ultimately single threaded . whatever left , need run cpu , energy-eï¬ƒcient possibly , cpu level , entire compute node really . reason people n't operate cpu , operate computer . 's nice cpu energy-eï¬ƒcient core , rest data processing i/o [ ph ] memory consumes lot power 's point . entire compute node energy-eï¬ƒcient . many cpu , -- lot x86 lot arm . think two cpu architecture continue grow world's data center ideally 've reclaimed power acceleration , give world lot power grow . acceleration reclaim grow three-step process , really vital future data center . think represents canonical data center , course , different size scale . know , see , question kind reveals mental image data center also explains 's vital -- one thing forgot say , really vital , connected two type network . one type network , 's computing fabric , nvlink infiniband computing fabric . 're really intended distributed computing , moving lot data around , orchestrating computation different computer . another layer networking , ethernet , example , control multi-tenancy orchestration , workload management , forth , deployment service user 's done ethernet . switch , nics , super sophisticated , copper , direct drive , long-reach fiber . layer fabric vitally important . see , invest . think data center scale start computation , acceleration . continue advance point , everything becomes bottleneck . whenever something becomes bottleneck specific viewpoint future nobody else building way nobody else could build way . would take -- tackle endeavor go remove bottleneck computing industry . one important bottleneck course nvlink , another one infiniband , another dpu , bluefield . talked grace remove bottleneck single threaded code large data processing code . entire momento [ ph ] model computing , think degree implemented , quickly world csps . reason , 's , clear . two fundamental driver computing near future . one sustainability , acceleration vital . second generative ai , ai computing vital . want thank joining gtc . lot news consume appreciate excellent question . importantly want thank researcher scientist took risk faith platform building , last 2.5 decade continue advance accelerated computing . used use technology used computing platform groundbreaking work . 's amazing work really inspired rest world jump accelerated computing . also want thank amazing employee nvidia incredible company felt build-in ecosystem 've built . thank , everybody . great night .