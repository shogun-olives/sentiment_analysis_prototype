good morning , good afternoon , everyone . thank -- huge thank everybody signed webinar . understand realize time extremely important appreciate spending valuable time u today . also special warm welcome nvidia team , colette , cfo , lot know already , gilad , svp networking , also special thanks simona stewart , made possible . kind get networking piece , one two question colette based sort speculation 's percolating medium u.s. china . colette , maybe light last night 's press article regarding potential new export control ai chip shipment china , tell u potential impact business nvidia ? thank much . thanks question . let see provide little bit understanding . aware report u.s. department commerce considering control may restrict export a800 [ ph ] h800 product china . however , given strength demand product worldwide , anticipate additional restriction adopted would immediate material impact financial result . anticipate immediate material impact financial result . long-term restriction prohibiting sale data center gpus china , implemented , result permanent loss opportunity u.s. industry compete lead one world 's largest market impact future business financial result . well , 's clear , colette . thank . could maybe help u -- one question 're getting lot morning colette kress harsh kumar colette kress harsh kumar gilad shainercontext around percentage data center revenue driven sale china . yeah , historically , little bit range term 've seen historically . believe contribution sale china range approximately 20 % 25 % data center revenue . keep mind , includes compute product system , also networking . okay , great . , one last one 've getting lot , switch -- guy able pivot quickly a800 matter week . people almost believed software change . wanted clarify , guy made switch a800 , software change hardware change ? software change . movement a800 absolutely hardware change made create a800 well create h800 . thank , colette . supposed largely networking session . , think , simona , want turnover aaron [ ph ] want turn presentation gilad . go get networking piece thing . thank , colette . yeah , thank much . nice . 'm gilad , svp networking , came nvidia acquisition mellanox mellanox almost beginning , 20 year plus nvidia -- mellanox think three year nvidia le . started network designer . early infiniband device , part -- part design . actually start looking entire platform software capability forth . excited think move next slide . , think going slide number word make sure much le word coming slide . presentation -- need say statement , presentation may contain forward-looking statement please refer sec filing risk uncertainty facing business . , move next slide . talk network ai , obviously , talking ai , 's bit network , right , 's design component . need look entire system . designing network essentially , want build need build design full accelerated compute network balance system . 's started nic . n't start switch . start gpu level . memory io gpus , way network , essentially started application framework , application level . look everything 's great advantage nvidia . look nvidia networking bar , 's networking entity actually build design full ai platform use ai platform . looking software perspective , platform , sdks , library , huge , ton amount software , part system . case connects network gpus example , full hardware capability compute server , compute gpus , switch , nics , forth . able build essentially ability build entire system , given opportunity -- unique opportunity place right data algorithm right place . date algorithm n't want run gpu . actually want run network 's call network computing . element traditionally may network , , probably want gpu , 's going much effective . able move . 're able build effective system delivers highest level performance , large scale . , understanding n't build network , actually built entire data center , let 's talk data center dive network . know data center computer today past cpu computer server become computer , datacenters computer . 're putting data center run workﬂows , run application . look data center , big collection gpus , actually , way connect gpus , define data center . way connect gpus define gpus define kind workload run gpus , essentially define data center . look different example . first example 's open [ ph ] cloud , traditional cloud . traditional cloud , 's data center built support many , many user support variety workload small . even single node workload , lot single node workload . traditional cloud connected today traditional ethernet network traditional ethernet network good enough kind platform . lot user , lot application , almost , rational . , 're actually facing creation new kind cloud . new kind cloud -- new class cloud . cloud support gen ai workload . cloud support ai workload . ai workload different traditional workload run traditional cloud . ai workload running single node , ai workload need run multiple gpus one across node . even important , talk ai ai workload , actually start talking distributed computing . distributed computing completely different disaggregated computing , 's completely different hyperscale , 's something new . 's something new cloud actually requires element support distributed computing . started talking latency need day latency effective bandwidth , completely different kind requirement . traditional internet still fine north-south traﬃc gen ai cloud . need use access service , control cloud , need new class ethernet support new class foreclosing new class cloud , 's exactly spectrum-x . 's first ethernet ground-up design ai . started interface people enjoy utilize ethernet ecosystem 's actually combining . , data center mission actually run massive large-scale workﬂows -- massive large-scale application , large language -- large llm , large language model , complex training deal complex training , new kind data . different kind data center , right ? 're talking many , many , many user variety workload , 're talking much le number user workload going consume entire gpus system single application . 's matter many gpus connect , 's matter many gpus workﬂow consume network , 's completely different thing . want support workload , 's going consume ten thousand gpus hundred thousand gpus , option -- option 's 's become gold standard , 's combination nvlink infiniband . nvlink infiniband think internet , 's completely different kind architecture , designed -- specifically designed distributed computing , optimized year 's network support running workﬂows large-scale gpus , okay . different kind network . 's one network fit , way show internet [ ph ] infiniband coexist continue coexist , every system built , need network user access , deal network run control north- south traﬃc always ethernet great , ai infrastructure need network east-west . need network compute -- distributed computing , nvlink infiniband actually go stronger . go next one , coming slide , 'm going refer couple term . want make sure everyone understand term , 's going make life easier , particular , nccl sharp . nccl -- 's nccl , nccl short nvidia collective communication library . 's software sdk . 's software sdk ai communication -- ai communication , multiple gpus . essentially , software framework support mainly two multi-gpu arrhythmics communication . one reduction reduced operation one all-to-all communication . , nccl essentially enables connection gpu side network side support two operation , reduction operation all-to-all operation . , nccl , want measure ai networking performance -- ai networking performance networking performance ai , nccl great -- great option test performance . look 's performance nccl reduction operation , 's performance nccl all-to-all operation example , demonstrate impact network . 's good way actually test network measure network . sharp , 's technology part e-network computing , nvidia e-network computing . 's technology implemented infiniband switch asics . 's something run cpu , 's embedded within switch asic . enable switch network perform data reduction operation data data transferred within data center . , previously data reduction operation part nccl done host , running host big toll host , going part nvidia advantage , ability move algorithm one side another side run right place . moving data reduction operation run switch network , reduces amount data need send network half . 's huge impact . mean 400 gig end- to-end infiniband network sharp better 800 gigabit per second end- to-end network without sharp . 's amazing capability infiniband 's one element enables infiniband making infiniband go standard ai factory . look nccl , impact nccl sharp , see right . 're gaining 1.7x higher-performance sharp , running nccl , running reduction switch network compare best network actually beat . would -- compare best theoretical performance ethernet , 's 1.7x . one key thing actually make infiniband gold standard large-scale ai ai factory . go back talk different network start cloud go spectrum-x . cloud -- cloud , two kind ethernet network , essentially two wall ethernet . network north-south connectivity . controlled access 's user access . cloud service . cloud service user access , loosely coupled application . typically use tcp traﬃc , jitter fine . user access , jitter jitter okay . latency actually critical . predictive constant performance bandwidth , 's important well . what's important deal heterogeneous traﬃc . need deal multiple loosely coupled process process enable run network . traditional ethernet used , right ? ethernet designed . kind cloud network know . cloud , second network , compute network , call east-west . traditional cloud , much east-west traﬃc workload , user running single node , therefore traditional cloud , take north-south network use east- west network 's fine , 's okay , work . , want host ai workload , want cloud generative ai application , east-west network need completely something else , east-west network need deal disaggregated computing . distributed computing sensitive latency , even sensitive tail latency . distributed computing , run application across multiple gpus , many , many gpus sense . one gpu communication going late , one , let 's say 'm running 500 gpus , one gpu communication going late 400 , one , entire workﬂows delayed -- entire workﬂow delayed , tail latency critical element ai performance . 's completely relevant north-south traﬃc , 's critical east-west . effective bandwidth important want provide constant performance , change performance level need deal burstiness [ ph ] , requirement distributed computing completely different , would say opposite need north-south . use traditional ethernet east-west , need something else . need different class network support new need ai application cloud , 's reason spectrum-x . 's reason designed spectrum-x needed new class ethernet kind infrastructure . next slide , please . let 's -- let 's look spectrum-x . spectrum-x , left side see start , 51.2 tera , number port essentially forth . right side , see snapshot software 's developed spectrum- x. ton software sdks . -- doca case run gpu bluefield provide network utilization [ ph ] , isolation application infrastructure -- application infrastructure . -- spectrum key switching . magnum io sdk includes nccl framework mentioned . operating system run spectrum switch , sonic cumulus aspect like . ton software . , spectrum-x would essentially design ground-up ai build new capability , actually designed new capability ethernet capability including , first lossless ethernet . 's interesting essentially combination element , 'm going go . first lossless ethernet , n't want drop packet . dropping packet mean 're creating jitter creating jitter 're reducing ai performance , n't want drop packet . top lossless ethernet , want support adaptive routing . ﬂow-by- ﬂow adaptive routing . see ﬂow late adaptive routing ethernet switch traditional ethernet , -- ﬂow ﬂow mean need -- run stream data n't change path stream data stream end , 's good ai . ai , want fine grain adaptive routing . want packet-by-packet adaptive routing . 's element -- 's enabled actually lossless , even -- even . want packet-by-packet adaptive routing . lossless network shallow buffer , buffer -- buffer . ethernet option , example , 're sometimes referred fabric , ethernet sometimes n't run actually ethernet , depend buffer . big buffer switch shock observer . , congestion kind hold data stuff like , buffer mean long-day latency . long-day latency something nice ai workload . n't want buffer . , idea combining lossless ethernet , find good [ ph ] adaptive routing shallow buffer , 's combination . combination exist traditional ethernet , completely exist . one part spectrum-x advantage . second part congestion control . need eliminate hotspot . designed spectrum-x congestion control based first telemetry information , also unique capability network order identify latency change , react hotspot impact performance application . important key provide traﬃc resolution . key eliminate noise . make sure noise impact ai performance . cloud , run many , many workload . want make sure workload , especially small-scale workload impact large- scale workload . running network , want make sure isolate noise small workload impact ai workload 's exactly 're congestion control , inventory- based congestion control capability different latency change identify hotspot actually negative impact . give u ? give u 1.6x higher ai fabric performance traditional internet . 're talking 95 % effective bandwidth skill load , keeping performance constant , predictive performance , keeping performance constant even lot workload running environment cloud . think security , virtualized network , everything part . spectrum-x actually bring speed feed need ai , ethernet interface . people leverage ethernet ecosystem service built ethernet , cloud service , thing sort , actually ethernet designed ai . next slide , please . , look support larger-scale ai workload , go infiniband . infiniband , starting see left side , latest generation , one thing need understand , infiniband designed based different kind architecture versus ethernet . ethernet built wide-area network time within data center algorithm designed ethernet -- algorithm designed ethernet . pfc example bgp moderate designed ethernet . ethernet complicated protocol . 's complicated protocol . build ethernet network , need actually choose feature performance -- need choose feature performance , that's ethernet , one switch fit . see variety switch coming different kind entity reason one switch fit . switch shallow buffer port , much good performance distributed computing supporting kind cloud interface . switch buffer order support sometimes doca service application come issue day latency reduced number port forth , need choose feature , performance , stuff . spectrum-x -- spectrum-x actually designed right element ( inaudible ) actually created thing n't exist traditional ethernet . infiniband , look infiniband , different kind architecture . 're using architecture . infiniband designed beginning support distributed computing . reason infiniband protocol simple . 's lightweight . 's , simple . 's simple , meaning infiniband leaf spine , kind freaking term ethernet , leaf spine , ethernet try build two-level network , two-level switch , n't go beyond two-level switch stuff like . exist infiniband . thing infiniband . infiniband , use many switch want . even - - large-scale system using three level switch infiniband . system even use four . want use five , use five . performance penalty . issue around , build size system want . 's like , 're designing formula race car . designing formula race car , many seat going put car , care . 's different kind design . look three-level switch infiniband , system use today , go way 65,000 gpus . go four level , several four level multiple four level already , go two million gpus infiniband network . want go five , go five . limit many gpus connect together 's even . n't see limit many gpus use single workﬂow , 's important point . limit 's infiniband gold standard large-scale ai . , infiniband pioneered rdma obviously , 's lot element rdma infiniband pioneered rdma within [ ph ] full computing . saw impact sharp . sharp give 1.7x nccl , compare best ethernet network build . 's pure software-defined -- pure software-defined network . designed sdn people knew sdn mean sdn , mean control entire routing single place . optimize routing workﬂows , build different kind network topology . treat change network quickly . reconfigure network course [ ph ] , port , configure , quickly , 's huge amount benefit pure software-defined network . give u ? look total performance , small 2x , gracious , many gpus want , building network loss latency , -- large scale load . short latency , extremely short latency . know impact sharp nccl operation , nearly 100 % effective bandwidth scale . 's amazing network . 's really amazing network , 's developed -- 20 year , right ? 's every generation bring new capability . upcoming quantum-3 , thing planning amazing , completely amazing . take infiniband completely next level compared anything else . , infiniband also there's tonne software , right ? sdk element . magnum io nccl two , obviously , management network , able simulate everything . there's ton software well . 's 's important end-to-end , 's we're end-to-end design . next slide , please . look impact network . network essentially small part data center -- small part data center expense . huge impact . huge impact ai performance , essentially , network pay -- network pay . infiniband offer highest scalability . , build size system want , 3 q - harsh kumarlevels , 4 level , 5 level , unlimited number gpus connect together . 're looking performance , took nccl nccl good indication network performance ai . first , see spectrum-x . 's completely different design ethernet enabled ethernet ecosystem , right , want joint ecosystem need performance ai 's spectrum-x . want build system that's going go scale , want get highest level performance , also bring infiniband cloud . reason . look infiniband , 's kind amazing top . look impact total ai performance , network essentially free , completely pay . , even someone going offer traditional ethernet free , completely free , 's going good enough , right , they're actually paying good infiniband 'm going get much better -- much better , right ? , essentially building ai infrastructure network essentially free . next slide , think last one , yeah . look -- looking networking revenue -- nvidia networking revenue , revenue doubled since mellanox acquisition . within see breakdown infiniband ethernet . infiniband tripled , 's growing , growing fast , continue grow . continue grow . spectrum-x , spectrum-x new class ethernet -- new class ethernet needed new class cloud . therefore , spectrum- x boost cloud ai network market increase market increase ethernet revenue 're moving forward . overall , believe -- see essentially , believe every data center become accelerated data center future . data center accelerated . used situation got 2x performance every two year , nothing . n't work anymore , n't work anymore . know want able increase capability , 's accelerated computing therefore , every data center become accelerated datacenter . every server gpu processing unit , every datacenter element . , talking $ 60 billion market opportunity nvidia networking side . , first thank listening . took time . 're happy answer question . question answer yeah , hey , gilad . thank much . extremely informative , actually answered whole bunch question . one one get investor concern around fact already come nvidia compute . come nvidia networking based merit example talked . get lot question . basically tied nvidia lot think , people know semiconductor - gilad shainer q - harsh kumar - gilad shainerbusiness , , always want option . could maybe talk -- work around one make infiniband , farmed place . yeah , well infiniband , infiniband 's standard technology . 's proprietary technology standard . 's like ethernet . ethernet also standard sense . company definitely create infiniband device actually , company build infiniband device different kind application . company building device long-haul connectivity . infiniband element fpga thing forth . , course , infiniband open , everyone use . , always guarantee networking , right , n't want use infiniband , use ethernet , n't want use ethernet , use infiniband . always choose . question -- question understanding 's . essentially -- especially look ai , look ai , ai requires datacenter skill . look actually want right element inside . 've said used get 2x performance every two year . 's case . therefore , 're going see more-and-more specialized technology actually us accelerated computing use technology enable achieve goal -- achieve goal . optimizing ai workload performance performed discrete compute networking device level . want look full stack approach . 's important , essentially would say 's time market , time solution . customer considers total cost ownership , performance , availability [ ph ] . time build deploy large- scale architecture . nvidia delivers . deliver full [ ph ] platform . 're huge amount optimization customer take whole , customer want take piece , whether wanted take piece mix-and-match thing happen market -- exist market . great . gilad one . guy sort gold standard company accelerated datacenters . come infiniband network adoption , noticed big difference metric training versus inferencing example infiniband network , either term port term metric think talk ? investor generally feel like inferencing common , 's going huge opportunity , wanted address . , yeah 's definitely good question , training -- training requires large-scale cluster right tightly coupled optimized massive , massive data q - harsh kumar - gilad shainer q - harsh kumarcompute . inferencing typically required much smaller -- smaller scale cluster . , 's happening generative ai becoming mainstream . therefore , number separate job running inferencing dramatically increase . therefore , inferencing require larger number accelerated server ﬂexibility essentially . probably going see people going deploy system want use system training inferencing -- training inferencing . therefore , case , obviously , infiniband great option . , someone going inferencing n't need go large one , course , use spectrum-x . 're going see probably system going use , make sense build system used , infiniband good option . great one , gilad . perception investment community even people know generative ai well infiniband work nvidia 's gpus , accurate listening talk , seems like 's case , wanted ask , since expert topic . yeah , , infiniband open used accelerated non- accelerated compute platform . nvidia develop full stack platform . customer choose take whole , want take design copy design whole . actually take piece . take gpus use network , take network use compute element , 's free use platform . definitely tied [ ph ] obviously end-to-end lot benefit right ? invested lot effort , lot effort investing customer much faster time computing , much faster time solution , much faster time build system . build supercomputer , ai supercomputer , n't want spend nine month build . 's nine month lifetime expensive system . 're , build system week , month . take full performance . people choose -- take component , use network within compute element forth . wonderful , know , 're road , want mindful time . 've got two minute , 'll ask one final question . typical setup , let 's say , guy go deploy accelerated ai datacenter . typically find entire datacenter either infiniband , ethernet possibility mix-and-match offering , depending line supposed ? - gilad shainer q - harsh kumar - gilad shainer yeah . first obviously entire ethernet system , know . system full ethernet . system , different kind ethernet . created spectrum-x , order bring right class ethernet ai compute fabric . definitely ethernet system , want build gen-ai cloud system want leverage ethernet ecosystem element . n't need develop software cloud . spectrum-x good answer . give speed [ ph ] needed ai give ecosystem friendliness ethernet . system definitely going exist . side , took large-scale need system essentially combining infiniband ethernet , 's one versus , completely -- completely going coexist . large ai factory , large system run large language model training , ethernet north- south access . infiniband built user access 's 's meant . 's purpose . -- interface , ethernet . compute fabric , want connect large amount gpus thousand ten thousand hundred thousand gpu single workﬂow , infiniband actually give combination nvlink infiniband give connectivity -- connectivity . look system design system example recommend look coping enable leverage everything designed , system includes infiniband ethernet completely coexit . 's one replaces . think , network , exist one purpose . , come end presentation . gilad , thank enough time , particularly . know 're road . colette , thank time appreciate comment thought early . simona , stewart , thank help pulling together . , next time . thank . thank much .