okay . thanks much . hi , everyone . 's brett simpson arete . 's pleasure welcome ian buck , many know inventor cuda , nvidia 's cuda software headed data center division many year -- think longer 've covered nvidia actually . ian , excited today . thanks joining u . thank . happy help . think 's particularly interesting time connect ian . 're keen get ian 's perspective ai market going trend next couple year . 's obviously phenomenal year . think go back last year spoke ian , think annualizing $ 16 billion revenue data center . think 're pretty much term quarterly sale . phenomenal achievement . great see . start , 'm going hand ian maybe talk little bit disclosure forward-looking statement . ian , get started . yes . really quick . mean reminder , presentation , 'll talk , may contain forward statement . brett simpson - brett simpson - ian buckinvestors advised always read report filed sec information related risk uncertainty facing business . great . start , 1-hour call . 're going cover prepared question 45 minute . 're going open investor q & . ( operator instruction ) question answer maybe jumping right , ian , start , maybe thought reviewing year 's gone ? obviously 's phenomenal year commercially , 's looking 2023 ? setting big priority next year two data center division ? yes . mean obviously -- oﬃcial chatgpt moment actually happened '22 , end '22 , really , stride impact felt 2023. really -- created -- showed world opportunity ai -- generative ai could interact computer , interact cloud , , like , use software . n't think 's ever , least recent history , technology introduction -- unlike maybe pc phone change modality , create whole new ecosystem , new opportunity , new way working . 's clearly happening gen ai . new service , new kind experience , new consumer product , new business opportunity , 're also making old stuff way interesting . word got sexy . always liked powerpoint , like actually like -- we're working angle . seeing commercialization ai happened really make useful tool literally everyone given -- killer app -- 's working ai , gosh , almost 10 year since first time nvidia gpus used . mean found u , n't find . back alexnet imagenet day , really ramped . year , -- there's couple great advance result . nvidia really , 've talked data center-scale company data center new unit compute . language model -- generative ai data center-scale problem . really brought different technology together , whether gpus , dpus , cpu , provide infrastructure addition , software go . nvidia company software engineer hardware engineer opportunity capability deliver . certainly , llm , generative ai ramp . nvidia product perspective , 've extremely thrilled - brett simpson - ian buckabout progress 've made grace grace hopper . 've seen -- 've brought market . 've seen opportunity provide single-node inferencing platform . 've seen supercomputing hpc business , often leading indicator 're going . obviously gpus scale 20 year . 're next year slated deploy order 200 exaﬂops ai across supercomputing , grace hopper . finally , course grace hopper multi-node going ai year , saw first glimpse announcement made : invent , taking grace hopper putting together 32-way nvlink ( inaudible ) rack build ai supercomputer , aws 16,000 gpus . 's double benefit also dgx cloud partner . four dgx cloud partner , azure , oci , gcp aws , working together . imagine , life pretty busy front . think 'll naturally carry accelerate next year . 've got enterprise adopting ai , building service capability partnering nvidia ask question data , whether like servicenow dropbox -- that's happening dgx cloud engagement . 's happening ai factory 're standing software , either framework service , 're putting together partnership enterprise . yes . well , let take point laid , ian . maybe first ai factory strategy . maybe lay -- mean obviously public cloud 's -- mainly conventional compute installed base , building significant gpus 202 [ ph ] cloud . explain u ai factory ? whether 's scale operation , amount power need . -- looking term deployment ? many ai factory need next couple year ? give u sense thinking . yes . 's pivot change perhaps cloud past even gpu cloud past . would rent $ 1 per gpu single gpu maybe get node , eight gpu hgx platform , fantastic , 's lot thing put together still today . think ai factory , model -- gen ai need work scale . scale vary . think 's within rack within row across data center ai , scale , training sure , case , inference large model happening scale . different kind compute -- result , engagement enterprise little bit different . instead renting infrastructure , 're training foundational model business . start data . one thing may noticed leader number token train gone quite bit . need make graph . -- used -- everyone feel satisfied maybe hundred million 1 billion , one trillion token . model may growing , number token get quality increasing foundation model , 's also driving sense scale . think cloud unicorn company building using , 're using infrastructure service , ai factory , place access data , able train various scale able process refine transform various amount data turn ai model quickly , course monetize , able take model product provide insight , creates business opportunity , provide additional service capability . turn data compute , ai asset becomes business various way ( inaudible ) product . really want one workload , right ? 's ai model 're going training inference . know , computing training inference linked . fact , train model , first inference model teach got wrong . 's forward past version . -- 're building data center , ai factory one use case , running operating ai model scale . product engagement work ecosystem , course hardware platform . today hgx h100 . 's hgx h200 , hopefully 've seen . increased memory size 144 gigabyte per gpu delivering five terabyte second . number double kind performance ai existing hopper . going grace hopper , take another click term power optimization , course nvlink scale , like aws work . that's top work 're networking ethernet infiniband , enable build ai factory . think 's early . 're seeing first ai factory happen four leading customer . announcement azure around mlperf great example . -- trained gpts mlperf benchmark . training gpt 175b across 10,000 gpus full infiniband cluster give sense -- scale ai factory ( inaudible ) . copy eos supercomputer use research ai factory . dgx cloud business ai factory business , make capability available fortune 500 able access ai factory , rest world continues build capability region cloud . - brett simpson - ian buck - brett simpson - ian buck yes . thinking fast forward 2 , 3 , four year , fortune 500 going ai factory ? mean saw partnership foxconn . sound like 're pretty close deploying something ai factory early next year . yes . mean -- build ai factory , -- nvidia work entire ecosystem taking -- building server system baseboard . technology push limit thing like nvlink water cooling , interconnect work data center ecosystem , includes taiwan system -- taiwan , system , rack scale . , course data center . able -- helping guide -- fit today 's data center retrofit today's data center , provide capability value , forward-looking data center build-out plan provide thing like water cooling , facility water give sense direction data center planning , course major capital expense take -- multiyear road map . 's big construction project . -- see end product partnership customer using ai factory , 've seen probably le , get glimpse nvidia working end-to-end ecosystem help accelerate ability everyone stand ai factory ecosystem , right , support scale 're talking kind data center , kind ai factory people want build . okay . maybe talking dgx cloud . mean talked brieﬂy -- mean mentioned aws agreement , great . many actual customer using dgx cloud today ? think intention going bring customer platform . maybe give u snapshot term activity around dgx cloud ? think also , talked $ 1 billion annualized software revenue division day . much coming dgx cloud ? yes . 'm actually sure 've provided date information provided term number customer others . think simona team provide 's publicly disclosed earnings elsewhere . 'd guide . focus many -- see many lighthouse customer . - brett simpson - ian buckand customer , case , obvious sense data want turn capitalize train build foundational model service capability . servicenow , data . better minefield everybody -- support help ticket back forth engagement provide better experience support even potentially thing like preventive maintenance guide company service need help . dropbox data company n't provide ability train learn data , provide service able ask question data . there's work done genentech engine . vertically applied use case drug discovery using platform called bionemo help mine vast majority drug data discovery data accelerate time drug discovery , anything -- cost develop drug measured trillion . anything applied ai certainty pull schedule reduces cost development , dramatically increase time revenue breakthrough drug . obviously customer want work nvidia . want make sure get latest . also inform u help u drive -- accelerate technology platform . obviously inform infrastructure , inform hardware side , importantly , 's software engagement we're . provide dgx cloud ai factory . 's infrastructure , also software . nemo service talked . nemo framework open-source framework like pytorch ( inaudible ) training large model foundation monitoring scale . 's many use case across industry . nemo , rest service provide way enterprise directly work nvidia work software train model service api format , -- ai ninja . course take dgx cloud , containerize opportunity go deploy wherever choose . often , 's cloud 're hosting dgx cloud , many great partner move on-prem . 's entirely . monetization come engagement dgx cloud , service also nvidia ai enterprise software support run time attached container deploy . continuously maintain engagement best nvidia get support rely nvidia basically providing support software need order continue run business integrate scale . 's driving software revenue guy -- service revenue , yes ? much . - brett simpson - ian buck okay . talk little bit demand , ian , guess 've seen obviously great trend last 12 month . looking beyond , say 2024 , look demand ai , whether 's corporate level government spending even co-pilot lot enterprise software company building ? demand sustainability 're seeing today inﬂection point 're seeing today ? yes , certainly . obviously demand gen ai insatiable , creates -- make job entertaining help -- forecasting make exciting challenge . aware -- going -- growing ? certainly , major cloud provider capability big muscle scale . would use scale perhaps standing map-produced cluster web server [ ph ] back- end operation , know operate hyperscale , n't changed . kind server infrastructure 're standing different . used thin , lightweight , low-cost , i/o , little bit cpu , okay slow storage simple ethernet nic standing ai supercomputer . skill muscle scale obviously leveraged . course ability invest also -- see . also ability build data center build data center . obviously also accelerating . 're going continue see major obviously building ai factory , ai service pick along nvidia 's road map . enterprise choice consume . consume directly cloud . still see on-prem well ability work major cloud , on- prem data center work regional cloud provider , gpu specialty cloud provider , operate potentially hyperscales , smaller , bare metal sometimes little quicker sense -- quickly bring market new technology execute directly , one-on-one customer versus building cloud infrastructure . definitely emerged go-to-market , great . 's another way people consume get access latest nvidia technology . see customer three . -- mentioned sovereign ai . new thrust . 've talking ai nation sovereign ai little bit , definitely seen tick 2023 continue tick moving forward . recently announced supercomputer u.k. , isambard-ai . 's built grace hopper . -- announced actually prime minister opportunity build -- u.k. could resource ai -- - brett simpson - ian bucknation , company , industry put right forefront ai nation . obviously see happening u.s. 're seeing happen across europe part world well . continue grow , dovetail nicely supercomputing business , building ai supercomputer actually build -- gpu 's used train giant model , gpu , capability going supercomputing , pleasure also help camp [ ph ] . 's -- , think new growth angle , new opportunity . think 'll hearing sovereign ai project around world . guess sit back look sort early stage gen ai g2000 market , look opportunity sovereign cloud get built . looking beyond sort like next sort six 12 month , must pretty confident 're pretty good growth trajectory sort medium long term given nascent lot trend nvidia ? yes . think challenge -- mentioned , think call . historically , ai -- 's entirely new computing capability . required different kind software stack different kind -- invented new kind computer science . surprise , started place like google facebook back first . many year , sharing educating happening . next click -- escape search engine news feed become tool enterprise . 's happened . mean see . 're thrilled -- 's happened -- technology matured . world -- world 's smartest people helping mature also build capability making adaptable many different modality . result , opportunity ai expanded since people know tailor , apply different use case way enterprise adopt . technology like rag , using data along query tune improve experience . able ask question proprietary data , data model trained , technology , one -- also talked , really happened year 2023. really crack open door ai allow enterprise ask question proprietary data . vast majority data proprietary . model trained thing -- public internet acquired market . really , value moving forward ability connect proprietary data ( inaudible ) enterprise . 's like 90 % data . 're beginning seen -- internally , hyperscalers use service data , obviously benefited greatly . - brett simpson - ian buckexternally , 've seen 's trained public internet , -- made something rest entire world experiencing become part zeitgeist vernacular everyone , every consumer [ ph ] . moving forward , 're seeing opportunity broadening ai market every business every [ ph ] vertical apply maturity ai term applied , technology software stack , including nemo software , also ability connect proprietary data customer , case dropbox . yes . maybe switching gear little bit . wanted get perspective inference , opportunity see ahead inference . guess n't seen much deployment yet . guess 're also looking -- 're hearing lot folk industry want see big eﬃciencies saving . guess lot company saying , maybe gpu n't best architecture inferencing llm quite expensive . wanted get perspective . think nvidia need look new class accelerator strip double precision -- stuff maybe n't relevant inference try drive cost ? , drive cost per query materially sustain leadership cost ownership advantage versus peer ? yes . first , let address see market . certainly , used explain needed gpu -- cpu 's inferencing limiting . today really ca n't deploy model without form acceleration , without using gpus . see , . used -- clear -- probably product segmentation lightweight t4 gpu a100 training gpu . reality today 's benefit , 's blended entirely . 's going hard , unfortunately , tease apart . value model great , need large -- either large language model . want 100-class gpu , whether 1 , 2 , four eight 9 , even multiple within rack . 's interest grace hopper single grace hopper 32- way ( inaudible ) grace hopper based model . -- let talk little bit , see . let talk second point , nvidia reduce cost inferencing ? reduce cost inferencing applying engineering smart software team study relentlessly way improve throughput gpu inferencing . whether small gpu big gpu , improve throughput , looking -- running model ? massive effort , 's codified software called tensorrt . tensorrt run time optimizing thing inference . recently year actually announced tensorrt-llm , open source , n't get - brett simpsonencouraged . push innovation tensorrt-llm make best possible software platform running inference . inference , thing even extreme could training need make -- training need maintain certain level computational , numerical precision adjust model work derivative . ai turn derivative , still slope difference . inference forecast . instance , announced -- published new performance number , new blog monday past monday h200 . ran falcon , llama2 model infor [ ph ] precision . ran fp eight bit precision , eight -- 0 1 eight bit . hard work figure make four bit precision work infor . 's easy say infor , like , accuracy 95 % 99 % , maintaining accuracy llama2 , 's -- one falcon- 180b , 's 180 billion parameter single gpu . 's possible run whole ( inaudible ) model four bit precision . fact , believe world's largest model ever run single gpu period 180 billion parameter . took cost inferencing cut half . diﬃcult know comprehend fast-moving field . 're engaged entire ecosystem . even nvidia researcher , technique infor used -- called -- used metric called awq , actually invented nvidia researcher . put ( inaudible ) open-sources nvidia ( inaudible ) orbit . encourage everyone watching closely performance blog , tensorrt , performance blog transformer engine framework that's dumb stuff constantly increasing trip performance even buy gpu . 'll continue make gpu throughput reduce cost throughout . last thing 'll say -- asked different kind gpus . today . everyone talk a100 h200 , show big iron infrastructure . also sell lot traditional pcie form factor gpu similar may see gaming pc , fan , fit nicely server , 2- slot version single-slot version , even half-height , half-length l4 . fact , amazon aws google announced bringing l4 market l40 . two pci gpus . different price point . 're great universal gpus excellent value deploying running inference . effectively , minimal change economics , take server turn ai server , adding l4 l4s people put 8. 's -- available u . fact , gambit gpus form factor continues expand . would microsoft copilot inference today ? - ian buck - brett simpson - ian buck ask question 're copilot . worked closely team . see announcement together work 're , see closely nvidia microsoft openai , work together . yes . great . maybe one question open q & . wanted get perspective competitive dynamic 're seeing market ? mean guess 's early inning amd , obviously came last night mi300 . hyperscalers announced ai asics last month . view competition next year 2 , factoring ? much bigger tam 12 month ago competitive offering coming market ? yes . yes . . mean - 've nvidia 20 year , always -- see important accelerated computing become . every computing company ai company , figure path , way contributing , way maybe exploring alternative option , including building silicon certainly , cloud built silicon many , many year , 's logical would also looking explore . see close working partnership together . google announced latest cpu , keynote san francisco , next person walk jensen , talking collaboration 're dgx cloud . h100 instance work 're collaborating building better cloud -- adam [ ph ] reinvent , aws . whole ecosystem advancing ai course advancement done plain sight optimization like 've talked improving providing horizon tag ai . talk contribution want 're looking specialize offer , unique want -- see opportunity . speak nvidia . -- competitive aspect . moving extremely fast . -- often unfortunately requires people sense perspective know 's going thoughtful compare 's claimed making sure information -- latest . 've announced h200 . 've -- blog talked tensorrt nemo transformer engine ( inaudible ) published number 're real end-to-end . open source . - brett simpson - unidentified participant - ian buckwe provide full accuracy throughput latest nvidia software . 's really important innovation happening quickly , always latest nvidia software understand everything . h200 , 're first provider hpe ( inaudible ) memory technology , work closely memory partner , mentioned , whole ecosystem ( inaudible ) market . 's 144-gigabyte gpu five terabyte second memory balance . -- double performance h100 latest innovation monday doubled . 're cooperating like 4x plus clip a100 , a100 month ago . also created growth ai , also given u benefit accelerate investment . think jen-hsun shared investor community , 're pumping new architecture even faster stuff road map next year continue . fast pacing -- fast- paced field . technology advancing . ai definitely advancing . nvidia continuously taking -- purchase data center full gpus , know get increased performance throughput reduce cost year -- data center . n't one-off purchase . obviously capital expenditure last three five year long . investment need -- going -- ai factory producing asset going fuel growth company move forward . diﬃcult predict next -- mean need see , like ai going three year . know nvidia continuously part system optimizing software stack making -- making data center productive ai evolves partner journey . -- obviously go economics decision-making happens deploy scale 're deciding ai infrastructure 're going consume . yes . yes . good . think probably good junction open q & . colleague yanko [ ph ] announce question . , yanko [ ph ] , . guess couple question together everybody interested asking strategy compete amd inference given product -- price point product coming compared h100 claim make superior performance h100 ? like said , encourage community look closely end-to- end workload measured performance delivered . 's participate industry standard benchmark like mlperf . mlperf created - unidentified participant - ian buckgoogle meta others industry end-to-end benchmarking , includes accuracy throughput training inference . see work 've -- contributing basket model , we've company 's actually smoothed every benchmark suite since inception -- four five year ago , continue . make sure understand latest nvidia performance , latest software . change economics 2x software organization 's available tensorrt , change math everywhere . part 're moving quickly . h200 new hbm3e memory , 144-gig 5-terawatt second -- -- shared number blog performance . last comment 'll make ai n't -- inference n't chip benchmark . delivering performance across node 're scale . whether eight gb inside system 32 gb inside system providing end-to-end throughput . 's easy talk ﬂops top number . end , decision 're making happens 's throughput , 's performance end-to-end model . model evolves change scale , 're making sure getting benefit ecosystem software improvement coming nvidia engineering , also partner ( inaudible ) platform . please use ... following -- next question , nvidia plan pursue chiplet architecture road ? keep staying ahead competition monolithic architecture ? ca n't talk future product simona come . -- talk -- let talk compute density second . kind see 's happening ai , go way back pre-ai web scale , math-produce , ai [ ph ] , datasets filled ethernet scale , . n't -- density factor . ai computing , computing generating revenue . use power , budget , space computing number . le watt dollar spent sending bite around compute , move data , want densify . densifying , bringing compute closer together , spend le energy , le joule , le watt moving data optimize cost well going optic copper pcb . extends even inside chip . cheapest way move data within piece silicon . - unidentified participant - ian buckchiplets great regard . provide better form -- entire form communication . course building would -- 's still better obviously able communicate chip able drive sign across chip across multichip model . see benefit multichip model , 's grace hopper . grace hopper call super chip 's basically two chip put right next . regard two model circle together , drive 900 gigabyte second communication , basically taking new nvidia technology building one super chip . think densification trade-off also -- see built , driving thing technology we've cooling building . always think 're going see large chip ability optimize whole architecture compute apply idea densification n't silicon level package level , 's server racking data center level well . right . next question , foresee relative mix customer type data center gpus evolving nvidia future ? hyperscalers largest category today seems like government , traditional enterprise , startup category emerging bigger percentage . -- yes . hyperscalers two kind customer . obviously providing compute capability infrastructure service . amazon talked 're using nvidia gpus improve buyer seller experience . use nvidia gpus help seller write effective , descriptive product description -- model deployed nvidia gpus . obviously use music search . search amazon music right , query literally query go infrastructure . actually processed nvidia gpu nvidia software , 'll call , apply ai model figure really mean . ( inaudible ) song genre get right provide better search experience music . used nvidia gpus nemo framework actually train collection model amazon titan bedrock , example . 're internal customer , obviously . , course 're -- provide gpus market public cloud instance . see azure gcp , aws . continue continue scale . enterprise becoming much bigger portion , obviously consumption ai factory . 're starting see . grow . ratio 2 , perhaps diﬃcult , 're -- 'm trying compare two exponential 's never good idea . -- see enterprise become significant portion term consuming factory . 's early day , definitely trend that's slash . - unidentified participant - ian buckthe new one , think sovereign ai nation . ability every -- every country see learned ai resource domestic industry help nation advance , either solving important problem healthcare climate change weather forecasting . probably science side thing , important research see ai make -- understand thing better make better policy decision inﬂuence policymakers provide -- see ( inaudible ) economy , landscape people . , course resource industry well . every industry afford ai supercomputer , government provide capability . see new -- japan already talked -- providing infrastructure , u.k. hear u.s. well , that's definitely new trend . 's hard prescribe mix , -- kind growth curve . think 's going probably balanced moving continue scale ai next -- rest decade . next question , view industry nvidia pursuing kind walled garden strategy similar apple , seems competitor open source approach software also partner something like ultra ethernet consortium . extent true ? yes . thank question . certainly , like talk technology lot . -- 're engineering , 're true technology company regard , probably pretty unique compared customer others . -- first , open company . look -- look harbor , okay ? make available gpus individual gpus . make available gpus hgx baseboard . make available rack design oems , cloud take reference architecture deploy modify make different . look ngx initiative , 's reference architecture . 's build gpu server -- 're going continue maintain bounding box , thermal electricals . move fast , invest form factor know 're -- n't redesign everything available configure build allow every partner stack innovate way . course want work directly nvidia , gx business allow , small portion activation across entire sustainability system whether cloud sustainability , on- prems supercomputer . software stack standpoint , true . -- nemo framework , framework use train megatron -- megatron model 530 billion parameter . - brett simpson - ian buckthe ai framework amazon used model train use case , open source . 's data . tensorrt llm software stack provide -- reference model latest llama falcon name . open sourced technique train world access need open source likely variance modification need support . technology open source available ecosystem modify change . go way program individual gpu , 's fine . software stack , library -- open sourced . 's couple closed since -- really truly optimized nvidia -- nvidia engineer term internal library . frankly , 's stuff would diﬃcult anyone nvidia engineer actually understand . -- hopper stack , everything 're dgx cloud , sitting top cloud , sitting top ( inaudible ) sitting top ( inaudible ) odm oem platform , gpu open . software innovation include one announced monday technology piece really want consume direct end-to-end solution nvidia pick library interface integrate service . unicorn , example , pick piece apply tell u , give u feedback make even better . guy may see , time make openstack optimized . another way putting 're one ai company work every ai company . everyone platform wherever stack get benefit . tends gravitate toward top get compound value everything 're , 're precluded taking bit piece -- tailoring specializing workload . maybe -- maybe one final question side , ian . china . maybe share thought might play nvidia china ? guess obviously 're going see compliant gpu shipping market fairly soon . think 's going received , maybe firstly ? secondly , think 're going see training move offshore -- china ? lastly , indigenous platform , hearing lot huawei scaling trying scale . think 're going see shift towards indigenous platform market ? yes . unfortunately , ca n't go road map regard . question good question , ca n't answered forum . -- look , opportunity ai one -- hit every nation , every country , every business able meet demand obviously stay within regulation , 'll continue . mean . - brett simpson - unidentified participant - brett simpson - ian buckso -- 're adhering serve ai market . however get guidance provide -- provided u company . continue . forward stuff , think 'll wait thing talked announced . fair point . good . okay . last question , yanko [ ph ] ... question . okay . great . well , think 're time . ian , thanks much coming today sharing thought . really appreciate , always . thanks everyone , dialing . 're going close call . thanks much . 'll chat soon . yes . appreciate . thank . bye .