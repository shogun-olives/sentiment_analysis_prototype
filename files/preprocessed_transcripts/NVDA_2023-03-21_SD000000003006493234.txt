hi , everyone . welcome gtc . simona jankowski , head investor relation nvidia . hope chance view ( inaudible ) morning . also published press release file [ ph ] detailing today 's announcement . next hour opportunity unpack discus today 's event ceo , jensen huang ; cfo , colette kress , open q & session financial analyst . begin let quickly cover safe harbor statement . today's discussion , may make forward-looking statement based current expectation . subject number significant risk uncertainty , actual result may differ materially . jensen huangfor discussion factor could affect future financial result business , please refer recent form 10-k 10-q report may file form 8-k security exchange commission . statement made today based information currently available u . except required law , assume obligation update statement . 'll start brief comment jensen ; followed q & session jensen colette kress . , let turn jensen . hi , everybody . welcome gtc . gtc conference developer inspire world -- possibility accelerated computing celebrate work researcher scientist use . please sure check conference session . cover really amazing topic . gtc keynote highlighted several thing . let -- go slide , 'm going colette cover basically first slide , rest slide provided reference . let make couple comment first . core computing today fundamental dynamic work , course inï¬‚uenced one important technology driver history industry , moore 's law fundamentally come significant slowdown . could argue moore 's law ended . first time history , longer possible using general-purpose computing cpu gain necessary throughput without also corresponding amount increase cost power . lack decreasing power effectively decreasing cost going make really hard world continue sustain increased workload maintaining sustainability computing . one important factor , dynamic computing today sustainability . accelerate workload reclaim power use whatever reclaim invest back growth . first thing waste power -- accelerate everything possibly really focused sustainability . gave several example workload used highlight many case , accelerate application 40 , 50 , 60 , 70 time , 100 time process decreasing powered order magnitude decrease cost factor 20. approach easy . accelerated computing full stack challenge . nvidia accelerated computing full stack . 've talked many -- many session past . start architecture system system software , acceleration library application top . 're data center scale computing architecture . reason refactor application accelerated , algorithm highly paralyzed [ ph ] . , also scale . -- one benefit accelerated computing work , scale , also scale . combination allowed u bring million x acceleration factor many application domain , course one important one artificial intelligence . nvidia 's accelerated computing platform also multidomain . really important data center , computer single-use device . make computer incredible instrument ability process multiple type application . nvidia 's accelerated computing multi-domain , particle physic , ï¬‚uid dynamic , way robotics , artificial intelligence , forth , computer graphic , image processing , video processing . type domain consume enormous amount cpu core [ ph ] today enormous amount power . opportunity accelerate reduce power , reduce cost . , course nvidia 's accelerated computing platform cloud edge . architecture available every cloud . 's available on-prem -- every computer maker world . 's available edge inferencing system autonomous system , robotic self-driving car , forth . lastly , one important characteristic nvidia 's accelerated computing platform although full stack , design architect data center scale . 's available cloud edge . completely open , meaning access literally computing platform computing maker anywhere world . one important characteristic computing platform . openness reach , acceleration capability positive -- virtuous cycle , positive virtual cycle accelerated computing achieved . accelerated computing artificial intelligence arrived . talked three dynamic . one sustainability . mentioned . second generative ai . foundational work done last 10 year , beginning , really big breakthrough computer vision perception led industrial revolution autonomous vehicle , robotics , tip iceberg . generative ai , gone beyond perception generation information , longer understanding world , also make recommendation generate content great value . generative ai triggered inï¬‚ection point artificial intelligence driven step function increase adoption ai world importantly , step function increase amount inference deployed world 's cloud data center . third thing mentioned discussed keynote digitalization . really taking artificial intelligence next phase , next wave ai , ai operating digital information , generating text generating image . ai operating factory physical plant autonomous system robotics . particular case , digitalization real opportunity automate world 's largest industry . spoke digitalization one particular industry . gave example omniverse digital physical operating system industrial digitalization , demonstrated omniverse used beginning product conception , architecture , styling product design , way collaboration design , simulation product , engineering electronics setting virtual plant way digital marketing retail . every aspect physical product company , digitalization opportunity automate help collaborate bring world physical world digital , know exactly happens . get world , digital , ability accelerate workï¬‚ows , ability discover new product idea , ability invent new business model , tremendously increase . spoke digitalization . 5 takeaway spoke keynote . 'll talk today question area , love entertain . first , course generative ai driving accelerating demand nvidia platform . came year full enthusiasm hopper launch . hopper designed transformer engine designed large language model people call foundation model . transformer model -- transformer engine proven incredibly successful . hopper adopted every cloud service provider know available oems . really signaling increase demand hopper versus previous generation accelerating demand really signal inï¬‚ection ai used ai research -- generative ai moving deployment ai world 's industry importantly , significant step function inference ai model . generative ai driving accelerating demand . second thing talked new chip coming marketplace . care deeply accelerating every possible workload . one important workload course artificial intelligence . another important workload accelerate operating system entire data center . imagine giant data center computer , they're ï¬‚eets computer orchestrated operated one giant system . operating system data center , includes containerization , virtualization , networking storage importantly , security , isolation future , confidential computing application operating software-defined layer [ ph ] software layer run across entire data center fabric . software layer consumes lot cpu core . frankly , would n't surprised many , depending type data center operated , would n't surprised 20 % , 30 % data center power dedicated networking , networking fabric virtualization software-defined stack , basically operating system stack . want oï¬ „ oad , accelerate operating system modern software-defined data center . processor called bluefield . announced whole bunch new partner cloud data center adopted bluefield . 'm excited product . really believe going one important contribution make modern data center . company designed , company wo n't resource design something complexity cloud data center everywhere . announced grace hopper , going used one major inference workload , vector database , data processing , recommender system . recommender system , 've spoken past , probably one valuable important application world today lot digital commerce lot digital content made possible sophisticated recommender system . recommender system moving deep learning , important opportunity u . grace hopper designed specifically give u opportunity get 10x speed recommender system large database . spoke grace . grace production . grace also sampling . grace designed rest workload cloud data center possible accelerate . accelerate everything , left software really want strong , single-threaded performance . single-threaded performance grace designed . also designed grace cpu fast computer , cpu , energy-eï¬ƒcient cloud data center . think entire data center one computer , data center computer , way designed cpu context accelerated data center ai-first , cloud-first data center , cpu design radically different . designed grace cpu , excuse ( inaudible ) slightly reach . grace cpu designed . entire computer module . n't cpu , entire computer module great super chip . go passively cool [ ph ] system could rack whole bunch grace computer cloud data center energy eï¬ƒcient yet performing single-threaded operation . 're really excited grace 's sampling . let 's see . spoke lot generative ai 's step function increase amount inference workload 're going see . one thing that's really important inference coming world 's data center really want accelerated one hand . hand , multimodal , meaning many different type workload want inference . sometimes want inference , want bring inference ai video , augment generative ai . sometimes 's image -- producing beautiful image helping co-creator . sometimes 're generating text , long text . prompt could quite long long context could generating long text , writing long program . application , one , video , image , text , , course also vector database , different characteristic . challenge , course cloud data center , one hand , would like specialized accelerator one modality one diverse generative ai workload . hand , would like data center fungible workload moving . 're dynamic . new service coming , new tenant coming . people use different service different time day yet would like entire data center utilized much possible . power architecture one architecture . one architecture 4 different configuration . run software stack mean depending time day one provisioned underutilized , always provision class configuration accelerator workload . fungibility data center give ability -- architecture , one architecture , inference configuration , inference platform give ability accelerate various workload best ability perfectly precisely predict amount workload entire data center ï¬‚exible fungible . one architecture , 4 configuration . one biggest area collaboration collaboration partnership google cloud , gcp . 're working across large area accelerated workload data processing dataproc [ ph ] , spark rapid accelerate dataprocesses , represents -- data processing probably represents 10 % , 20 % , 25 % cloud data center workload . 's probably one intensive cpu core workload . opportunity accelerating , bring 20x speed , bring lot cost reduction customer enjoy . importantly , lot power reduction 's associated . 're also accelerating inference triton server . 're also accelerating generative ai model . google world-class pioneering large language model we're accelerating putting onto inference platform , l4 . course streaming graphic streaming video , opportunity accelerate . two team collaborating take large amount workload could accelerated generative ai accelerated computing workload accelerating l4 platform , gone public gcp . 're really excited collaboration , much tell soon . third thing talked acceleration library . mentioned , accelerated computing full stack challenge . unlike cpu software written 's compiled using compiler general purpose , code run . 's 1 wonderful advantage breakthrough cpu , general purpose . acceleration aspect , want accelerate workload , redesign application , refactor algorithm altogether , codify algorithm acceleration library . acceleration library , way linear algebra fft data processing use ï¬‚uid dynamic particle physic computer graphic forth , quantum chemistry , inverse physic image reconstruction , forth . one domain require acceleration library . every acceleration library requires u understand domain , work ecosystem , create acceleration library , connect application ecosystem power accelerate domain use . every single -- 're constantly improving acceleration library installed base benefit increased optimization investment capital already , infrastructure already . buy nvidia system benefit acceleration year come . 's unusual u platform increase performance anywhere 4x 10x 've installed life . 're delighted continue improve library bring new feature optimization . year , optimized released 100 library 100 model -- 100 library model better performance better capability . also announced several important new library . one new library 'll highlight culitho . computational lithography inverse physic problem calculates -- process -- calculates ( inaudible ) equation go optic interacts photoresist mask . ability basically inverse physic image processing make possible u use wavelength light much , much larger final pattern want create wafer . 's miracle fact , look modern microchip manufacturing . latest generation , 're using 13.5-nanometer light , near x-ray 's extreme ultraviolet yet using 13.5 nanometer light , could pattern nanometer , 3-nanometer , 5-nanometer pattern wafer . mean 's basically like using fuzzy light , fuzzy pen create really fine pattern piece paper . ability requires magical instrument like asmls , magical instrument , computational library synopsys , miracle work tsmc field imaging called computational lithography . 've worked last several year accelerate entire pipeline . single , largest workload eda today computationally intense , million million cpu core running time order make possible u create different mask . step manufacturing process going get lot complicated coming year magic 're going bring future lithography going get increasingly high . machine learning artificial intelligence surely involved . first step u take entire stack accelerate . course last four year , 've accelerated computational lithography 50 time . course reduces cycle time pipeline throughput time chip world manufactured , really quite fantastic $ 40 billion , $ 50 billion investment factory . could reduce cycle time even 10 % , value world really quite extraordinary . thing really fantastic also save enormous amount power . case tsmc work 've done far , opportunity take megawatt , ten megawatt reduce factor 5 10. reduction power course make manufacturing sustainable , 's important initiative u . culitho , 'm excited . lastly , 'll talk single largest expansion business model history . know world becoming heavily cloud-first . cloud give opportunity engage computing platform quickly , instantly web browser . last 10 year , capability cloud continued advance point started cpu running hadoop mapreduce query beginning , high-performance computing , scientific computing system , ai supercomputer cloud . going partner world 's cloud service provider . starting oci , 've also announced cloud partnership azure gcp . 're going partner world 's leading cloud service provider implement -- install host nvidia ai , nvidia omniverse nvidia dgx cloud cloud . incredible capability , one hand , get fully optimized multi-cloud stack nvidia ai nvidia omniverse . opportunity enjoyed world 's cloud optimized configuration . get benefit nvidia software stack optimal form . benefit working directly nvidia computer scientist expert . company large workload would like benefit acceleration , benefit advanced ai direct service engage world 's industry . 's wonderful way u combine best nvidia brings best csps . incredible service security cloud , security , storage , api service offer , well could likely already cloud 've selected . first time , ability combine best world bring nvidia 's best -- combine csps best make capability available world 's industry . one service announced platform service , nvidia ai , nvidia omniverse infrastructure service , nvidia dgx cloud . also offered -- announced new layer . many customer work , many industry partner work build foundational model . customer enterprise , industry would like access foundational model , obvious accessible thing work world-leading service provider like openai microsoft google . example ai model designed highly available , highly ï¬‚exible useful many industry . company want build custom model based specifically data . nvidia capability . customer would like build custom model based proprietary data , trained developed inference specific way whether 's guardrail would like put implement type instruction tuning would like perform type proprietary data set would like retrieved , whatever specific requirement language model , generative image model 2d , 3d video biology , service allows u directly work help create model fine-tune model deploy model nvidia dgx cloud . - simona jankowski q - toshiya hari - jensen huangand mentioned , dgx cloud run world 's major csps . already csp choice , 'm pretty certain 'll able host , okay ? nvidia cloud service going expand business model offer infrastructure service , dgx cloud , platform service , nvidia ai , nvidia omniverse new ai service designed custom , essentially foundry ai model available world 's industry world -- partnership world 's leading csps . 's . announcement made . lot go . thanks joining gtc . , colette answer question . question answer thank , jensen . let welcome financial analyst q & session . we're going taking question zoom . ( operator instruction ) first question toshiya hari goldman sachs . thank much hosting follow-up . jensen , guess 1 question inference opportunity . obviously dominate training space , done many , many year . think inference side , competitive landscape little bit mixed given incumbency around cpu . obviously encouraging see introduced new inference platform . guess criticality recommender system spoke , ( inaudible ) llm work google , seems like market moving direction . think opportunity inference , call , 3 five year versus stand today ? think grace playing role next couple year ? yes , toshi , thank first , 'll work backwards . 3 five year , ai supercomputer building today unquestionably advanced computer world make today . , course gigantic scale . includes computing fabric like nvlink , computing -- large computing , large-scale computing fabric like infiniband sophisticated networking stitch together . software stack , operating system , distributed computing software , 's computer science [ ph ] limit . , 's really going quite exciting ai super computer [ ph ] going go beyond research extending essentially ai factory ai model people develop going fine-tuned improved basically forever . believe every company intelligence manufacturer . - simona jankowskiat core company , produce intelligence . valuable data proprietary . 're inside wall company . capability create -- build ai system help curate data , package data together could used help train proprietary model , custom model , accelerate business . system , ai training system continuous . second , inference . inference largely cpu-oriented workload . reason inference world today fairly lightweight . might recommending something related shopping book query forth . kind recommendation largely done cpu . future , several reason even video processed cpu today . future , likely happen two fundamental dynamic inescapable point . inevitable quite long time . inescapable . one sustainability . ca n't continue take video workload process cpu . ca n't take deep learning model even quality service little bit lesser good using cpu , burn much power . first reason accelerate everything sustainability . accelerate everything moore 's law ended . sensibility permeated every single cloud service provider amount workload requires acceleration increased much . attention acceleration , alertness acceleration increased . secondarily , everybody power limited -- power limit . order grow future , really reclaim power acceleration put back growth . second reason generative ai arrived . 're going see every single industry , benefiting , augmenting co-creators , co-pilot , accelerates everything tech create , chat bot , interact , spreadsheet use , powerpoint photoshop forth , 're going -- 're going augmented , 're going accelerated , inspired co-creator copilot . think net ai training , ai supercomputer become ai factory . every company either on-prem cloud . secondarily , every interaction computer future generative ai connected . therefore , amount inference workload quite large . sense inference balance larger -- larger inference -- larger training . training going right . q - christopher muse - jensen huangour next question come cj muse evercore guess question , 'd like focus grace . past , 've mostly discussed benefit grace hopper combined . today 're also focusing bit grace stand-alone basis kind expecting . speak whether 've changed view expected service cpu [ ph ] share gain outlook ? think potential revenue contribution time , particularly think grace standalone , grace superchip obviously grace hopper combined 'll start punchline work backwards . think grace big business u , -- nowhere near scale accelerated computing . reason genuinely feel every workload accelerated must accelerated . everything data processing , course computer graphic video processing generative ai . every workload accelerated , must accelerated , basically leaf workload ca n't accelerated , meaning converse . another way saying 's single-threaded code . single-threaded code amdahl 's law still prevails . everything left becomes bottleneck . single-threaded code largely related point data processing , fetching lot , moving lot data , design cpu really good two thing . well let say two thing plus design point . two characteristic really , really want cpu one extremely good single-threaded performance . 's many core , 's good single-threaded core . number one . number two , amount data move extraordinary . one module , one module move 1 terabyte per second data . 's extraordinary amount data move want move , want process data extremely low power , reason innovated new way using cellphone dram enhanced data center resilience used server . 's cost effective obviously cell phone volume high . power 1/8 power . moving data going much workload vital u reduce . lastly , designed whole system instead building super fast cpu core -- cpu , design super fast cpu node . , enhance ability data center powered limited able use many cpu possible . - simona jankowski q - joseph moore - jensen huangi think net accelerated computing dominant form computing future moore 's law come end . going remain going heavy data processing , heavy data movement single- threaded code . cpu remain , important . 's design point would different past . next question come joe moore morgan stanley . wanted follow inference question . cost per query becoming major focus generative ai customer . 're talking pretty significant reduction quarter year ahead . talk mean nvidia ? going h-100 workload longer term ? guy work customer get cost ? yes , 's couple dynamic moving time . one hand , model going get larger . reason 're going get larger wanted perform task better better better . 's every evidence capability , quality versatility model correlated size model amount data train model . one hand , want larger larger , versatile . hand , many different type workload . remember , need largest model inference every single workload . 's reason -- 530 billion parameter [ ph ] model . 40 billion parameter model . 20 billion parameter model even 8 billion parameter model . different model created way -- large - - always need large model reason need large model minimum , large model used help improve quality smaller model , okay ? 's kind like need professor improve quality student improve quality student forth . 's many different use case , 're going different size model . optimize across . use right-sized model right-size application . inference platform extends way l4 l40 . one one announced week incredible thing . hopper h100 nvlink , call h10 0nvl . basically 2 hopper connected nvlink . result , 180 gigabyte -- 190 gigabyte , almost 190 gigabyte hbm3 memory . 190 gigabyte memory give ability inference modern , large-sized inference language model way , would like use , small configuration , dual h100 system solution let partition - simona jankowski q - timothy arcuri - jensen huangto 18. 18 ? 16 different , correct 'm wrong later . 16 18 , call multiple instance gpus migs . miniature gpus , fraction gpus could inferencing different language model whole thing could connected 4 could put pci express server , commodity server , used distribute large model across . already reduced performance incredible . already reduced cost language inferencing factor 10 a100 . we're going continue improve every single dimension , making language model better , making small model effective well making inference cost-effective new inference platform like nvl . importantly , software stack . 're constantly improving software stack . course last couple 2 , three year , 've improved much . mean order magnitude couple year . 're expecting continue . next question come tim arcuri ubs . jensen , think thought heard say google 's inferencing large language model system . wanted confirm 's saying . guess mean 're using new l4 platform ? , brand new ? word , using tpu , 're using new l4 platform ? curious detail . partnership gcp , big event . inï¬‚ection point ai , 's also inï¬‚ection point partnership . lot engineer working together bring state-of-the-art model google cloud . l4 versatile inference platform . could use video inferencing , image generation generative model , text generation large language model . mentioned keynote , model 're working together google bring l4 platform . l4 going phenomenal inference platform . energy eï¬ƒcient . 's 75 watt . performance chart , incredibly easy deploy . -- -- l4 one end , 'll show . l4 -- l4 . guy l4 , h100 , okay ? l4 . 2 processor 700 watt . 75 watt . - simona jankowski q - vivek arya - colette kress - jensen huangand power architecture . one software stack run well . depending model size , depending quality service , would like deploy , could infrastructure they're fungible . 'm really excited partnership gcp model 're going bring inference platform gcp basically across board . next question come vivek arya bank america . thank , jansen colette informative event . near-term longer-term question . near term , curious availability hopper , 're term supply ? long-term , jensen , heard range software service innovation . track progress , right ? last number think heard term software sale hundred million . 1 % sale . would consider success next year ? percentage sale think could come software subscription time ? let first start , vivek , statement regarding supply h100 . yes . continue building h100s demand 've seen quarter . keep mind , 're also seeing stronger demand hyperscale customer data center platform focus generative ai . even last month , since 've talked earnings , 're seeing demand . feel confident able serve market continue build supply , feel 're good space time . think software service substantial part business . however know , serve market every layer . 're full-stack company , 're open platform , meaning company would like -- customer would like work u infrastructure level hardware level , 're delighted . would like work u hardware plus library level , 're delighted ; platform level , 're delighted . customer would like work u way service level level , inclusive , 're delighted . opportunity grow three layer . hardware layer , course already large business . colette mentioned , part business , generative ai driving acceleration business . platform layer , two layer stood cloud service . company would like on-prem 're going based subscription . - simona jankowski q - rajvindra gill - jensen huanghowever know today world multi-cloud , really need software cloud well on-prem . ability u multi- cloud , hybrid cloud real advantage real benefit 2 software platform . beginning . lastly , ai foundation service announced beginning . would say model presented last time includes sensibility 're talking today . 've talking laying foundation path towards today . big day u launch probably biggest business model expansion initiative history company . think $ 300 million platform platform software ai software service today pulled . still think 's -- size consistent 've described . next question come raji gill needham . question technological perspective regarding relationship memory compute . mentioned , generative ai model creating huge amount compute . think memory model ? view memory potential bottleneck ? solve memory disaggregation problem ? would helpful understand . yes . well turn computing , everything bottleneck . push limit computing , living , n't build normal computer . know , build extreme computer . build type computer build , processing bottleneck , actual computation bottle neck , memory bandwidth bottleneck , memory capacity bottleneck , networking computing fabric bottleneck , networking bottleneck , utilization bottleneck . everything bottleneck . live world bottleneck . surrounded bottle . thing true , mentioning , amount memory use , memory capacity use increasing tremendously . reason , course generative ai work training model require lot memory , inferencing requires lot memory native -- actual inferencing language model necessarily require lot memory . however -- want connect retrievable model augments language model , augments chatbot proprietary , well curated data - simona jankowski q - stacy rasgon - jensen huangthat custom , proprietary , important , maybe 's healthcare record , maybe 's particular type domain biology , maybe something chip design . maybe 's ai -- 's database domain knowledge nvidia make nvidia click proprietary data embedded inside wall company using large language model , could create data set augment language model . increasingly , need large amount data , need large fast data . large amount data , many idea . course work 's done ssds , work people cxl basically affordable , attached disaggregated memory . fantastic , none fast memory . 's affordable memory . 's large amount accessible hot memory none 's fast memory . need something like grace hopper . need terabyte per second access 0.5 terabyte data . terabyte per second 0.5 terabyte data , wanted petabyte data distributed computing system , imagine much bandwidth 're bringing bear . approach high speed , high capacity data processing exactly grace hopper designed . next question come stacy rasgon bernstein research . wondering could go little bit economics dgx cloud business . like actually pay infrastructure -- cloud vendor pay lease back , 're running ? guess work ? customer pay ? get upside economics customer ? pricing like -- anything give u actually work impact model would super helpful . yes , stacy , thank . first , process go like . presented nvidia dgx cloud partnership csp partner . super excited . reason 's -- onboarding , , important customer large partner would consume storage , security , whole bunch application apis . presented idea would like rent nvidia dgx cloud , would take instance , reserved instance , 's called reserved instance , market engage customer , super , super happy . obviously nvidia deep relationship many - simona jankowski q - aaron rakers - jensen huanglarge vertical ecosystem world . highlighted two slide deck sent guy healthcare drug discovery , deep relationship many company . deep relationship every car company planet . two industry , particularly great deal urgency take advantage latest generation ai -- generative ai omniverse digitalization . first thing present idea , proposal proposes partnership . 're interested , far , 've incredibly enthusiastic . would purchase system -- include people 's gear , also includes gear stand dgx cloud . cloud service provider procure whatever -- whatever infrastructure , power , networking , storage , forth order stand infrastructure host manage , okay ? 's step two . step three take dgx cloud service market , combination value would deliver , would set price engage customer directly engage customer business . next question come aaron rakers well fargo . want go back , think maybe tj 's question earlier , kind breadth grace , maybe grace supership grace cpu strategy . think kind evolution , maybe help u appreciate much data center cloud workload single-threaded performance . context , foresee situation actually -- see server partner deploying grace cpu without necessarily deploying h-100 subsequent version gpus . see actual single cpu deployment market opportunity ? 'll work backwards . appreciate question . answer yes . however grace really targeted niche market , let clear . x86 , use x86 company . use x86 obviously pc , workstation . exciting go-to-markets intel sapphire rapid new workstation line . 're using sapphire rapid dgx . 're using sapphire rapid ovx [ ph ] server single-threaded performance sapphire rapid really quite good . 's excellent fact . mentioned , take application accelerate workload work -- part code accelerate paralyzation , -- really , 's left single-threaded code . - simona jankowski q - matthew ramsayand single thread code either control oftentimes , 's moving memory around giant amount memory , managing memory . amount data 's managing growing quite tremendously . -- mentioned , grace really designed type application data center largely accelerated well moving lot data . said , customer need x86 , obviously represents large part world remains large part world , expect continue . x86 predominant platform , n't make sense move enterprise computing arm -- grace necessarily . think 're focused application mentioned . however csps already going move arm -- would like build cpu bespoke need requirement . grace really great companion . reason design point design grace different design point almost cpu 've known designed . think cloud data center moving direction arm , really wonderful way either accelerate . benefit entire software expertise system ecosystem peripheral ecosystem 've brought grace design point special , 's really designed energy-eï¬ƒcient extreme energy-eï¬ƒcient cloud data center . -- anybody interested particular area , every one world , 's also important segment world . think grace going successful even independent stand- alone cpu . next question matt ramsay cowen . guess two question , jensen , one kind follow-up something 'd ask prior call , 's transition happening , think -- data center business selling accelerator card selling system . 'm really interested mean economics data center business term margin long term . guess -- second question little bit related expands bit dgx cloud opportunity . wanted -- one question 've getting lot last month , 1.5 month since announced , maybe take microsoft acute example . like guy partnering ? -- really partnership ? friction point might want customer relationship . - jensen huangso evolve time ? could kind walk u , mean seems like would want ai customer . guy going go directly rented space cloud . -- evolve time relationship largest csp customer bring business market . really appreciate question . first question . -- ca n't build software , ca n't [ ph ] develop software . ca n't genuinely develop software , 're system company . reason , ca n't build software chip , chip n't sit computer . system company . especially type software develop , 're trying replicate somebody software . 're building bespoke , brand-new software . none software created existed created , even computer graphic rtx full path tracing ai generation use modern computer graphic n't possible created software . order create software , system . system company , make nvidia unique , build entire system data center . literally start data center , chip . start data center , build entire computer . future , data center computer . entire data center computer . 's something i've spoken coming decade . 's one reason combination mellanox strategic , important . think people realize today . work together architect entire data center really quite foundational . way think world , way see world entire data center , frankly , even planetary scale computer . think world starting . includes computing element includes system includes networking storage compute fabric cpu forth , way system software stack importantly , algorithm , library . design data center . way design , design discipline disaggregate , fractionalize . customer would like buy hgx gpu , right . gpu look like today . lot people think gpu look like . course nvidia gpu . nvidia gpu . run software stack . that's kind miracle . one run software , run slower . take long time . ability u design entire data center , disaggregate let customer decide 's best form factor , best configuration . best deployment methodology . people use npi . people use kubernetes . people use vmware people use container bare metal . list go . yet distributed computing stack affected . work industry across layer software disaggregate system , component , system software , disaggregate library , run anywhere like workstation , pc , way cloud supercomputer . disaggregate networking , disaggregate switch . disaggregate literally everything , 're delighted put together . would like u stand supercomputer like 30 day , 's possible 've productized entire thing . disaggregated integrated world 's industry standard wherever could . result , computing platform literally everywhere , 's binary compatible . 's magic . think , 's one reason able , one hand , system company develop software hand computing platform company 's available everywhere . respect go-to-market , lose customer , csp would like direct customer relationship , 're delighted . reason whole bunch nvidia gpus cloud . nvidia computing cloud . software platform cloud anyways . customer would like use way download nvidia ai enterprise . run stack forth . everything work exactly today . however many customer would like need work u refactor entire stack . expertise understand entire stack , take problem otherwise barely possible -- 's barely possible multi-cloud configuration , meaning would like run cloud well . would like run azure oci gcp well on-prem . expertise help . case , need direct access engineer computer scientist . 's also reason 're busy . 're working industry leader would like build something quite special quite proprietary based platform , need computing expertise make possible either deploy scale want , reach multi-cloud want level cost power would like reduce . case contact u . - simona jankowski q - blayne curtis - jensen huangnow notice become direct customer interface , would still invite csp partner n't offer storage , n't offer rest apis . offer security . many industrial safety privacy data management regulation standard complied . world 's leading csps expertise . 's lot collaboration 's going happen . come u , terrific . go csps , fabulous . 're happy either way . next question come blayne curtis barclays . want ask inference 's kind two part 's two half [ ph ] small model large . guess 'm curious , t4 card back ago . n't think anything ampere . l4 kind new version . 'm trying understand , back t4 , think talking inference similar said , kind large market , maybe even big training . think became cpu . 's changed , guess 're feeling like smaller model need move accelerator . 'm trying understand large side , nvl like 700 watt . seems like lot power add every server . customer thinking deploying . 's huge model . need lot horsepower , 's one one every cpu . 's kind two part equation inference guy monetize ? yes . thanks . t4 one successful product history . million t4s cloud . however ten million cpu cloud . 's still lot workload cloud 's done cpu . two reason really need accelerated . one , course sustainability . accelerate every workload . world ca n't continue consume power cpu throughput . 's number one . number two , generative ai inï¬‚ection point . there's question . capability ai , usefulness different industry . generative ai connected . think happened last couple month , generative ai connected popular application planet , oï¬ƒce , team , google doc . popular productivity application history humanity . 's -- generative ai connected . inferenced somewhere . think nvidia platform really ideal platform inference handle video , handle text , handle image , 're going handle 3d , handle video , hand well . - simona jankowski q - william stein - jensen huangwe going handle everything throw u . think , really inï¬‚ection point . respect , reason 750 watt nothing today 's cloud data center . thing really spectacular u , get replace hundred cpu server . 's reason accelerate . reason accelerate spend 70 watt , save 10x . 700 watt 7 kilowatt , 's math . want accelerate everything sudden , reclaim 6.9 kilowatt , reinvest future workload , okay ? 's -- motion , , conservation energy motion world 's csp going , accelerate workload , reclaim power , invested new growth , 1 , 2 , 3 step . way put gpus world csps , easy pc [ ph ] today . time 1 last question , come stein truist . jensen , several year ago , really introduced world accelerated sort oï¬ „ oad parallel processing compute maybe reintroduced something used exist long time ago , certainly modern time , like big computing revolution way . -- product 're talking , particular , grace cpu bluefield dpu . talk vision modern data center envision typical architecture looking like maybe 3 , five year . dpu relevant grace cpu relevant traditional x86 ? see x86 server continuing perpetuate enterprise traditional enterprise software ? see going away ? 'd love sort long-term view . really appreciate . believe data center next 5 10 year , start 10 year work way back even five year work way back , we'll basically look like . ai factory inside . ai factory working 24/7 . ai factory take data input , refine data transform data intelligence . ai factory data center . 's factory . reason 's factory 's 1 job . 1 job either refining , improving enhancing large language model foundation model recommender system . factory job every single day . engineer constantly improving , enhancing , giving new model , new data create new intelligence . every data center number1 ai factory . inference ï¬‚eet . inference ï¬‚eet support diverse set workload . reason know video represents 80 % world 's internet today . video processed . generate text . generate image . generate 3d graphic . image 3d graphic populate virtual world . virtual world -- run diverse type computer . omniverse computer , course simulate physic inside . simulate autonomous agent inside . enable connect different application different tool would able essentially virtual integration plant , digital twin ï¬‚eets computer , self-driving car , forth . 'll type virtual world simulation computer . type inferencing system , whether 's 3d inferencing case omniverse physic inferencing case omniverse different domain generative ai , one configuration optimal domain , fungible , meaning one architecture able receive oï¬ „ oad work something 's provisioned -- oversubscribed pick workload , okay ? second part inference workload . every single one node smartnics , like dpu , data center operating system processing unit . going oï¬ „ oad oï¬ „ oad isolate . 's really important isolate n't want tenant computer basically inside . think world future zero trust . application communication isolated . 're either isolated encoding , 're isolated virtualization . operating system separated -- control plane separated compute plan . control plane , operating system data center run , oï¬ „ oaded , accelerated dpu , bluefield , okay ? 's another characteristic . lastly , whatever left , 's possible accelerate -- code ultimately single-threaded . whatever left , need run cpu energy eï¬ƒcient possibly , cpu level , entire compute node really . reason people n't operate cpu , operate computer . 's nice cpu energy eï¬ƒcient core . rest data processing i/o memory , consumes lot power 's point . entire compute node energy eï¬ƒcient . many cpu -- lot x86 lot arm . think 2 cpu architecture continue grow world 's data center ideally , we've reclaimed power acceleration , give world lot power grow . acceleration , reclaim , grow 3-step process really vital future data center . think represents canonical data center , course different size scale . know -- see -- question kind reveals mental image data center also explains 's vital -- one thing forgot say really vital connected two type network . 's one type network 's computing fabric , nvlink infiniband computing fabric . 're really intended distributed computing , moving lot data around , orchestrating computation different computer . another layer networking ethernet , example , control , multi- tenancy , orchestration , workload management , forth , deployment service user . 's done ethernet . switch , nics , super sophisticated , copper , direct drive , long reach fiber . layer , fabric vitally important . see -- invest . think data center scale start computation , acceleration continue advance point , everything becomes bottleneck . whenever something becomes bottleneck specific viewpoint future , nobody else building way nobody else could build way would tackle endeavor go remove bottleneck computing industry . one important bottleneck , course nvlink another one infiniband , another , dpu , bluefield . talked grace remove bottleneck single-threaded code large data processing code . entire mental model computing think degree implemented , quickly world csps . reason , clear . two fundamental driver computing near future . one sustainability , acceleration vital ; second generative ai , ai computing vital . want thank joining gtc . lot news consume appreciate excellent question . importantly , want thank researcher scientist took risk faith platform building last 2.5 decade continue advance accelerated computing , used technology used computing platform groundbreaking work . 's amazing work really inspired rest world jump accelerated computing . also want thank amazing employee nvidia incredible company 've helped build ecosystem 've built . thank , everybody . great night .