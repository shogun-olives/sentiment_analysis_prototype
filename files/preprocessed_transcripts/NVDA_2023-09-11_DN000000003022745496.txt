thank much , good day , everyone . glad could join u afternoon keynote session . really delighted honored ian buck , general manager vice president nvidia 's accelerated computing business , also importantly , inventor cuda , key operating system underlying every nvidia accelerator . really glad time share perspective . , ian , 'll turn . think one opening remark , would really love get perspective requirement ai hardware changed throughout tenure nvidia . especially , always talk hardware , sometimes forget key part software ecosystem . could give u perspective nvidia 's software capability really helped cement dominance hardware side ai ? yeah . thank , pleasure morning . course , reminder , presentation contains forward-looking statement , investor always advised read report filed sec information related risk uncertainty facing business . yes , 've working accelerated computing quite time . fact , date way back 2006 first introduced cuda . initially , goal address program new kind architecture , new kind processor reached level programmability beyond playing video game making beautiful picture , become computing platform , place accelerate , every workload day , always want make sure best cpu matched gpus right configuration , right ratio , portion competition accelerated . typically either highly data parallel massively parallel compute- intensive . work -- community figure accelerate workload run architecture 's designed high compute throughput need . 's started course high-performance computing , community obviously looking using computer case -- case supercomputer , simulate nature simulate physic simulate problem ca n't necessarily easily identified either wet lab microscope scale -- size scale earth cosmos need computer digital instrument -- instrument science . 've -- -- 2006 first ai moment 2012 , 've made platform available software platform . made cuda available every one gpus including gaming graphic gpus , one everyone workstation , laptop pc . course , lot cloud traction . building software platform engage developer rather strictly hardware platform defines i-s-a , isa . met developer . made easy researcher , ph.d. student , engineer company take care nvidia gpu , download cuda free , library software developed time , figure apply problem , port code whether bc , fortran , today , python , java , others , move compute rich portion . decision upfront make software platform combination hardware platform really important couple reason . first , met developer n't wait others build software exists ramp . frankly , would diﬃcult taken long time given boot strapping problem . second , expanded innovation space . innovate hardware layer , compiler layer , system software layer , library layer , course , everyone else opportunity also contribute . performance delivered time compounding innovation hardware side system driver developer software , course , library top . track progress time , 's quite dramatic . 's benefit accelerated computing . allows compounding value delivered . q - vivek arya - ian buckit also allows nvidia innovate extreme click . constrained interface lower level like instruction set . 're constrained problem think address . requires u change architecture , change instruction set , build entirely totally different kind gpu build gpu talk gpus 're going linked scale across gpus system gps rack across entire data center define interface engage , extremely rapid click allows engineer produce new gpu architecture roughly two year case sooner . allows u think differently cpu gpus connected , also allows u expand entire data center canvas innovation making change inﬂuencing . first decision think basically think different engagement point allowed u really innovate , move quickly , invite everyone else participate ecosystem . 've guess along approaching 20 year nvidia . go . right . part software stack , ian , substitutable ? example , early day , made lot sense , right , couple two , many people also involved ecosystem , whether 's hyper-scalers whether 's r & -- software r & team many hardware competitor . part software ecosystem substitutable ? take application written four nvidia find way port somebody else's hardware example using combination third-party tool open-source software ? yeah . great question get asked lot . certainly , possible think one workload one ai model one specific algorithm get working anyone's hardware platform . make hard make platform continuous optimization evolution , platform run workload run inside data center . today , look software stack , , course , multiple hardware platform ranging pcie card run 70 watt , fit server l4 , larger 300 watt pcie card hgx-based board , multiple gpus talking every link , even shared scale entire rack scale even row scale gpus effectively . -- top , course system software , compiler library get integrated opening ecosystem , include -- hyperscalers , software like pytorch , software like pax ml wonderful part ai 's open innovate together ecosystem . q - vivek aryaso 's certainly possible spike different implementation different model stack . make hard platform 've mentioned n't community , need run different workload operate today across entire data center . n't build data center around one model . 're going run data center run -- large language model , generative ai , well data science use case need . also want accelerate end end , often also see someone inspect particular layer , particular model . deploy ai service , ingestion , data prep , run query , run model , well produce output case , perform multiple stage ai like wanted talk back apply text , 's also done ai . part platform innovation large language model generative ai standing still . year ago , still talking resonance 'm talking coalition around network . 'll talking unit recommender thing . thing still important many people innovating inside oems generative ai , model innovated equipped that's way faster 're actually producing new architecture . order platform , firstly investing data center scale , course huge capital investment take lot time . need platform trust innovation happening generative ai , 're going able run really well . come end-to-end performance optimization 're trying make . certainly , pay model get -- run model innovation platform much challenging ask , one requires connection benchmarking customer giving input order continue make platform improving time . find optimization everywhere . one benefit front part working nvidia get work different ai company . get optimize layer stack matter . n't one part stack need -- simply replaced order port . really get end-to-end workload . , possible , 's challenging sustainable . , let 's talk generative ai . obviously , caught everyone surprise good way , right , demand seems exploding . 'll talk first training generative ai inference . training side , seems like every day , somebody launching yet another large language model , nvidia dominates market training lot model . see point get kind cliff maturation demand training ? think people start look optimizing size model , actually somehow put pressure demand - ian bucktraining hardware ? sustainable demand ai training already producing many large language model ? yeah . large language model different . made 'll make -- large one question could ask . large language model unlike computer vision model past simple -- simpler recommender model . large language model effective 're directly acting human typically , order directly interact human , need understand human knowledge . one reason gpt large 's trained -- 's trying represent interact -- corpus human understanding compare - - take -- download internet teach human know -- started baseline foundational model capture human understanding knowledge , obviously , much larger perhaps would need computer vision model , still important , trained set image eventually , known set image . tend large model also tend great foundation loss specialization . specialize different workload specialize starting foundation model toward perhaps data . 're starting something understands human understands interact human machine , best use take proprietary data able interact ask question data , course , leverage general capability . ask capacity going grow time . interact computer , cloud , data , that's hugely immensely valuable . 's immensely valuable improving customer want interact company , people helping customer want understand assistant sitting right able ask question get prompted information knowledge base thing better experience . allows -- large language allows -- enable recommenders , people want give content -- provide right content -- newsfeed e- commerce get right word right context shared . literally touch every part e-commerce company interaction customer sort answer understanding decade big data we've living . tail ? think becomes continuous space innovation across board . -- 's going one model rule . -- 'll large diversity different model based upon innovation going continue space also specialization across field . way , 're seeing healthcare science drug discovery , q - vivek arya - ian bucklarge language model n't language human . could language biology physic material science well . -- growth vector look like ? becomes many -- 's rate -- many innovator adding defining betting new optimization technique , new kind model . start heroic amazing model coming people like openai , gpt model 're seeing . much research published model published inﬂuence create alternative derivative . scale thinking generative ai large language model . scope n't necessarily size model per se . 're going remain large . since remain large order -- baseline level foundational intelligence . really -- scale grow industry company rest enterprise beyond -- adopts technique interact customers' data apply business . certainly , hyperscalers , 're first jump . obviously talent capital ability basically invent much technology side side nvidia experience . fascinating experience . continue continue push limit figure apply . see starting scale ai across business 's branching rest enterprise -- rest industry . 're seeing whole tier cloud offering . 're seeing specialty regional gpu , data center popping everywhere serve market . operated differently , little agile , perhaps bit smaller focused , large litany middleware solution software company trying help enterprise company deploy technology across board . 's definitely broadening large language model ecosystem , adaptation generative ai language model business really scaling factor experience continue sure . kind similar question applied generative ai inference side . nvidia 's strategy generative ai inference perception training side , company dominates , product expensive ? come really scaling generative ai inference really think way customer monetize right end day , going help monetize ? 's product pipeline look like help gen ai inference ? competitive landscape change move training inference ? -- thank question . think people often get little bit confused , perhaps . certainly , starting point model deploying begin training cluster . stand infrastructure previously , a100 's hgx system . system designed eight gpus getting connected via maximum possible performance , course , infiniband scale across entire data center . today , 's deployed right hopper ( technical diﬃculty ) train natural platform inference . since training inference highly related , model -- order train model , first infer calculate error apply error back model make smarter . first step training inference every event repeatedly . natural customer deploying inference model training cluster hgx . 's place probably see -- place see inference . see inference happening across spectrum way l4 gpu , -- brought one , 's 72-watt gpu . 's half half length . 's candy bar size . small phone fit server . server pcie slot become accelerated server . -- fact , seen cloud adopted oem rest system instruction probably 's great inferencing . 's -- video encode decode capability . 're seeing used smart city application image processing . also run small loms recommenders small task , also see generative ai generation running stable fusion-like model , provides price point 's comparable cpu . , fact , make better -- much better tco -- cpu run model . maybe 'll finally return . need go click , l40 , full-sized pcie card run -- often used larger inferencing fine-tuning task . take existing foundational model fine-tune last mile specialization data workload much lighter task larger training cluster could done l40 l40s pcie-based server , available -- across -- every oem system . provides different price point different capability even way click nvlink-connected system . nvlink connected system , often see people running single node , , need hit , model certain size need execute certain latency say interactive half second latency response q & , example . connecting nvlink , basically build eight gpus term one gpu , actually run model much faster prior real-time latency . inference platform consists many , many choice optimize tco workload deliver performance . case inference , usually 's data center throughput certain latency , 's important . part -- roadmap software . want go back 's easy look benchmark result see bar chart assume speed hardware , often under-reported number say investment nvidia make software q - vivek arya - ian buckstack inference . 's actually even -- apply even optimization within -- training inference kind last mile . optimization model beyond perhaps capable training optimize . hopper , example , 're using -- 've released actually last week , new piece software called tensorrt-llm . tensorrt optimizing compiler inference llm version . optimization made software last month doubled hopper 's performance inference 's came whole bunch optimization optimizing tensor core 's h100 using 8-bit ﬂoating point improving scheduling execution software managing gpus resource increase effective throughput competition eﬃciency . 's really hard ask . 're trying basically optimize using reduced precision , using -- serving different side request quick q & summarization task write along email generate full powerpoint , data center that's going running hopper , data center running inference -- generative ai , going asked thing . getting around eﬃciently able manage workload keep gpus 100 % utilized actually pretty hard , mathematical , statistical , ai system software even hardware-level optimization . continue , last month , 've doubled performance hopper inference 'll continue , 'll see -- continue . think industry right cost structure generative ai inference scale ? see user , go take pick search engine , right , whether 's bard chatgpt , even put inquiry today , take several second get answer , right ? 's different experience used traditional search engine . think industry ? today , seems like everyone training lot thing trying lot thing , think industry actually cost structure take generative ai scale inference side imagine 's take really grow industry sustainable way next several year ? yeah . 's great question . today , live inferencing 're experiencing , course , previous-generation gpus . 's naturally originally developed optimized deployed . many large customer actually bringing hopper variant . 'll see 8x , -- mean , -- term performance , -- term performance , honest , ( technical diﬃculty ) hardware side activate capability another bump software side . q - vivek aryaso expect interaction guy experiencing get better get intelligent . think fixed latency want experience becomes question size capability model fit latency window . process continuous improvement . asked search . every search type take advantage fully optimized take long ? aspect generative ai language model used today may know . type search , 're using word literally index . actually applying language model generate optimized query string search based history thing . seeing aspect . also see thing like transformer large language model technology applied last-mile recommender system . get last 100 document piece information wanted understand adjust produce result . run smaller constrained transformer base model order provide last mile recommender ten hundred thousand whatever afford last mile recommender ? seeing technology deployed today put gpus today . next click , course , 'll richer experience search . expect see hopper . may take click . every generation gpus every invention new software optimization technique every invention community , whether 'd -- next llama ( technical diﬃculty ) gpt , bring cost inference . hopper product a1 brought 8x . tco also order 5x . -- compound continuous software program compile new model algorithm technique , 's order magnitude capability 's going available everyone . best part , 's gpus they've already purchased 's already . fact , performance 're delivering every one new piece software performance 's capable optimized gpus , -- 'm sorry , optimized ai algorithm model . three , continue improvement tco performance experience . 's fascinating time . 's super busy . 're seeing new innovation coming time , 's definitely keeping nvidia community busy optimizing -- continuously optimizing platform . got . wanted get perspective . 're competitive landscape . look demand profile nvidia 's accelerated product , right , ten billion , right , expected increase next year . n't give lot incentive hyperscale customer create custom asic solution ? one customer already tpu product custom solution long time . -- others -- 's lot headline wanting done . - ian buck q - vivek arya - ian buckso first , right positioning product versus internal solution ? use one -- one kind workload one ? become greater competitive threat nvidia going forward ? look -- happen gcp next conference conference , think two week ago . announced new variant processor day keynote , keynote , jensen joined stage talked innovation 're together google , gcp , new instance bring -- announced ga availability a3 instance , also integration gpu vertex ai platform . many research innovation happening gpus inside google elsewhere . give example fact -- well , hyperscalers absolutely mean invest optimize build something may tailored obviously important workload business . continue partner deeply nvidia gpus software team . 's two big company advancing together , helping -- u helping . u partnering together many software platform continue innovate see -- see . see nvidia open platform , course , available every cloud open software ecosystem help advance state-of-the-art ai data science accelerated computing holistically . lift come almost 20 year investing software developer ecosystem . 'll continue see hyperscalers , course , building silicon , mean , optimize specific workload perhaps taking focus business , still remain close connection nvidia see opportunity serve broader ecosystem , also innovate nvidia platform accelerating computing across-the-board . something 've quite comfortable 's good partnership , -- really evident -- . got . see change moving towards generative ai ? know , cost training expensive cost inference also going quite expensive think increase desire bring asic solution done past ? 's choice fit want optimize invest . one thing -- nvidia spending investing billion r & optimize generative ai training inference scale . every generation gpu , every generation interconnect infiniband cx networking technology , every innovation nvlink , thing bring tco increase performance dramatically , also bring cost training . q - vivek arya - ian bucknow , obviously motivated scale possibly order develop something uniquely advanced uniquely new different capitalize , working nvidia , basically opt new leverage billion dollar investment 're core workload training deploying inference large language model generative ai work space . 's question 're going decide optimize take step something may different necessarily take advantage time energy investment nvidia making . 's choice consider make . 're going continue regardless innovate pace mean -- may benefit entire . continue see thing happen . 'm sure would make sense , -- focus n't changed . continues swarm innovate increase performance lower cost also increase capability generative ai large language model . thanks . next topic , ian , wanted broach -- emerging class kind converged cpu , gpu product , example , grace hopper competitor also announcing , right , product . pro con using kind ? n't know whether converge cpu , gpus right n't refer , stack discrete solution 'm using standard x86 cpu one many gpus ? 's pro con moving kind converged architecture ? yeah . 's -- 've optimizing -- community optimizing accelerated computing ai 20 year . 've moved huge amount competition gpu point . many workload , including many ( technical diﬃculty ) 95 % , 99 % computing done course gpus directly communicating nvlink across infiniband , cpu workload either -- either small optimized done parallel overlap gpu competition . combining , appreciate high-performance cpu task , usually around data perhaps scheduling , managing coordinating execution . every time increase gpu performance , course , need make sure cpu performance keep find -- leave -- model like amdahls law . way manage , one , first use best possible cpu , encourage use . also adjust ratio cpu gpus . today , look dgx system , 's two cpu eight gpus , one -- go one four . one eight . course two one , grace hopper , went way one one -- one next . that's one angle . q - vivek aryathe part , though conversion . happens combine cpu gpus something different traditional x86 architecture cpu sitting pcie gpu ? first , bringing two converged together , dramatically improve bandwidth , communication two processor . today , 100 gigabyte second versus 60 gigabyte maybe 100 gigabyte pcie connection . got also much coherent bring two memory system together . memory gpu , today ship 80 gigabyte hpm gpu , we're going announced going 144 gigabyte per gpu . connect grace , connection fast , 600 gigabyte memory around cpu basically becomes combined fast memory platform , allow run even larger model . basically , effectively making 600 gigabyte gpu . activates certain different -- lodged around larger model single platform , single gpu , cpu complex , open new avenue new kind workload acceleration -- especially working large data , application like vector database , application like rationale network see lot finance fraud e-commerce , also used recommenders . large datasets often want either run -- run -- run today across many gpus , could run perhaps optimally different tco point much larger cpu like grace hopper , 600 gigabyte combined one 've tied together . third thing convergence allows u -- 's another vector innovation . add thing cpu could -- optimize workload already know opportunity see future innovate cpu ecosystem -- cpu space addition , gpu addition networking data center scale . -- see work 're dgx gh200 connecting even gpus together . excellent cpu-gpu ratio , nvlink , large memory really give vision future infrastructure generative ai , one basically 256 gpus connected nvlink , fully backed 256 grace cpu . effectively access one extra ﬂop gpu , amazing generative ai platform training extremely large language model inference , might need multiple gpus connected optimally ( technical diﬃculty ) 's really -- mean , three thing , larger -- provide larger memory starting point building block . 's great scale platform inference result . grace hopper fit server . 's complete complex cpu , gpu memory . allows u play ratio explore different ratio different kind workload cpu , gpu 's innovation space . many innovation 've made grace , 're using arm-based core , soc architecture grace core talk quite powerful showing many benchmark provides great companion compute rich workload nvidia focused last two decade . - ian buckgot . know minute left , wanted get take last two question , ian . one , 's role networking stack optimized generative ai cluster ? much advantage nvidia 're able leverage infiniband ? infiniband change ethernet , mean conversely , lose advantage also hyperscaler want move ethernet ? first , role networking part cluster ? anything change move infiniband ethernet ? yeah . great question . basically three interconnects point choice design deploy ai . 's nvlink , previously inside gpus talk directly inside system , going rack room scale . infiniband , certainly developed hpc supercomputing industry lowest possible latency data center scale , really designed . , course , ethernet industry established designed course high manageability capability , come rich ecosystem feature , enterprise , cloud need order manage software-defined infrastructure . see , course , nvlink continue closely tied innovation 'll making inside gpus , -- 's come fast go know bringing gpus , gpus get faster want connect thing quickly possible order -- continue allow operate one . get lowest possible latency inference , giant model , need technique around model parallelism , extremely high inner-communication requirement basically split model way instead way decrease latency . infiniband also continues grow , design point , course , lowest possible shortage latency , result -- well provide excellent bandwidth . see provide significant performance improvement leveraging perhaps rocky , converged ethernet stack , still lot -- deliver comparable performance . fact , support many cluster many deployment cloud scale ethernet , rocky , work great . best possible performance , infiniband get extra click basically come hpc heritage lowest latency high bandwidth . optimization well network competition . 're going math inside switch inside network fabric infiniband . fully expect ethernet working community actually improve ethernet 's performance well , -- great come manageability 's software-defined . two exist -- three exist ecosystem continue get best three layer performance scale , course , requirement reliability q - vivek arya - ian buck q - vivek aryamanageability , security enterprise deployment versus maximum possible performance time . roadmap continue instance go . expected staggered , 'll continue learn absorb technology . got . finally , would love get perspective , term rolling generative ai look application , seem infancy , right ? many application , look rate growth data center business , seems big proportion total spending pie , right ? give pause thinking , already big part spending pie , sustainable growth rate nvidia next several year ? 's fascinating question think . today , think growth 're experiencing right , 's people taking existing data center make optimizing incorporate gpus , om generative ai workload . may coming hyperscalers , enterprise wanting get board using cloud , example , seeing -- gpu regional specialty provider also standing infrastructure . 're largely going data center already exist ca n't build data center overnight . take two year plus save build infrastructure . see world looking cheap hit 're building data center future 're seeing really exciting growth . realize need build capacity , course , able build data center , generic nature , perhaps cpu focus 's majority server going , building everyone hyperscale regional on-prem basically building gpu data center scale . look growth data center build , kind see opportunity oms continuing grow beyond 's able case , quite literally crammed center already establish today versus size opportunity size market data center footprint growth capacity . 've gone corner data center data center designed really exciting , give confidence continued growth business see much company investing , world investing building infrastructure different demand different need . excellent . exciting note , ian , thank much taking time u , sharing perspective . really appreciate . thanks everyone - ian buck q - vivek aryawho joined webcast . got another 45 question chat 'll see work simona help answer question . really , thank much , ian , taking time . 's immensely useful get perspective . thank much . always pleasure , thank much . bye . take care . thank .