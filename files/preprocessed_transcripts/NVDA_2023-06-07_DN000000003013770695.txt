good morning . good afternoon , everybody . thank joining u , nvidia fireside . get started , simona jankowski going read u disclosure 'll jump . simona ? yes . good morning , thank much hosting u . wanted quickly remind audience comment today may contain forward-looking statement , investor advised read report filed sec information relates risk uncertainty facing business . back . simona , thanks . well , 're delighted ian buck . 's couple year . could n't talk last year , conﬂicts . ian veteran . run everything accelerated computing nvidia , interest level day relates ai . includes hardware , software , third-party enablement marketing activity . ian known basically 'll call father cuda led formidable moat around nvidia 's business term compiler technology , acceleration library , framework optimization . could n't better person think talk 's happening world ai , nvidia playing part . ian buck q - han mosesmann - ian buck q - han mosesmannso , ian , welcome . ? pretty busy , imagine . , yeah , 's amazing journey amazing last year since 've talked actually last -- actually last six month , even exciting , definitely riding exponential . 're cooking . yeah . question answer okay , great . -- start . 've nvidia 20 year -- almost 20 year , 's gotten even intense term 's going . 's state ai today ? know 's lot discussion chatgpt , generative ai . brieﬂy , 's state ai term nvidia 's view , guy participating ? 's open-ended question , think started go . yeah . like said , 's 20-year journey far accelerated computing whole , -- started making gpus programmable , launching cuda 2006 , investing ecosystem since . internally within nvidia , building software foundation platform accelerated computing , well working everybody ecosystem -- everybody ecosystem enable gpus computing platform . course broad goal . certain market high-performance computing , simulation others adopted first 's broadening since , course , since 2012 , ai . n't invent ai . [ ph ] ai found u , activation making everywhere , putting gpu , researcher canada able find realize thing working neural network ( inaudible ) others . turn math pretty good fit 're -- cuda . took . 's couple inﬂection point along way ai . first one basically initial work ( inaudible ) canada back 2012 , initially enriching competition . starting point really ai basic image recognition . picture , beach ball stop sign birthday party . image recognition expanded form -- statement would image sentiment , taking view web page tweet understanding positive negative sentiment understanding content . described initial -- concept ai , use ai initial production ramp saw . probably remember -- jeff dean talking finding cat video example . - ian buckall right . obviously , hyperscalers cloud provider social medium internet needed understand content . 's first place really turn data use ai understand -- people posting , review , product , et cetera . shift next shift ai , became capable , along way , ai shifted recognition problem generative one , able understand content , text , speech , video , whatever , able generate meaningful content , create content create product description would want user click link . started small really hit -- fact , set -- actually bert . remember original bert model , first transformer-based model , ability understand textbook , produce simple text statement general text . fact , nvidia one -- bert day , remember . fact , got noticed even market identify idea -- new kind neural network called transformer could understand text . prior , application ai convolution-based . basically looking neighborhood information building understanding localized data . make sense image recognition recognize face . first recognize individual shape certain position certain place . face two circle , nose line mouth , build notion face localized . language different . language sort interaction . 'm speaking right filled pronounce context . 's known part text speech far away 'm saying right understand . transformer based around idea attention , figuring distant relationship incorporating neural network . started bert mentioned google , took offer gpt , gpt transformer accessing idea . nvidia , obviously , swarming convolution , image recognition , cnns , focus -- , still 's still growing used-case important one . transformer taken , including video understand relationship primary use case speech human understanding . speech language hard problem . think computer vision like dog , cat , even bug basic computer vision . brain perspective , 's -- find highly tune reasonable job . really human gift language 's built upon -- deep understanding knowledge . first -- next inﬂection point , transformer , sort understanding knowledge able connect knowledge language . -- mean , understanding little bit generation . q - han mosesmann - ian bucktoday , 're era generative ai . started -- two area . 's two area kick . one obviously image generation . able describe like picture teddy bear swimming ( inaudible ) generates picture . generative ai chatgpt , able conversation . understanding 'm saying repeating back extracting information . database back . 's one large neural network . generative ai , make -- 've moved era recognition , recognition -- important . pick data understand content make decision based informed -- ai informing . ai provide content , provide review , provide mid text , engage customer , generally , help artist , optimize business , build new application build new kind service . 's interesting also though , unlike revelation like pc space mobile space , guy seen new kind application , new kind platform , new kind software , one , generative ai actually making old stuff interesting . look oﬃce 365 , think could -- -- probably n't like saying , stuff listening , excel n't interesting word n't interesting , generative ai , wow , 's way interesting , yeah . revolution , 're seeing generative ai opt create new startup , new kind service , 's also making old stuff , super interesting , fun double exponential . 's 're . think 're really -- cusp begin generative ai . everyone see opportunity . guy . market vc community -- investor community , see amazing startup created . 's -- 's make real super fun right seeing different application new service old getting amplified changed ai . hey , ian , compute , think lot people industry observer talk ai parameter complexity doubling every three , four , five month . ? much longer half ? compute implication -- nvidia industry currently use custom asics even fi ? [ ph ] yeah . 's great question , one get asked lot . first , generative ai , -- ai knowledge , like access knowledge , certainly , access knowledge like database something courier pro found provider important . -- reason gpt big mega -- megatrend $ 530 billion model trained supercomputer large . capture knowledge level and/or starting point generative model . drive model size . thing drive , 's model size . well , first , model size tends limit -- know , bigger model , want extreme , fact . [ ph ] people build model limit 's practical . n't want wait . one training job . build model scale , constantly iterating model , data , tuning parameter give converge level intelligence . lot ai training drop n't complete informe inform next one actually . take many month year build truly intelligent model . final change potential convergence effort . thing drive -- challenge tends training time . nobody want train month two . think 's , go past , productivity -- 's hard innovate 're waiting long . size model tends factor much capacity put place , productivity scale , 's general rule like people deal researcher , one really developing stuff , n't . start building foundation model , n't really want train -- training job take month 're inpatient . make faster gpus , figure connect faster together infiniband build optimized infrastructure thing like grace hopper new dgx gh 200 , productivity increase , train roughly month , model substantially must get bigger get intelligent . say one thing though model size one metric , model size measured parameter . $ 175 billion typical gpt . people -- trained 500. 's trillion perimeter model behind closed door 're starting get little secretive releasing huge model drive u opportunity massive intelligence . thing 's driving model -- design layer , continually tuning intelligence layer , making optimized , clever layer , reduces complexity per layer . noise capturing parameter layer bunch math calculation instead naive connection . human brain way similar . different kind neuron vision processing versus auditory versus memory . specialize layer design . happens ai . one sequence like . n't know guy noticed , one play something like chatgpt , get forget previous conversation drift . 's function sequencing , much information confrontation keep store memory conversation . sequence length increase compute size significantly term -- 's also training inference load captioning billion parameter . 's much memory , much need processed order make informed conversation going forward . diversification going . q - han mosesmann - ian buckhave lot different model palm llama gpt-4 , see different specialization happening . expect moving forward , model naturally want get bigger encapsulate intelligence . believe -- definitely seeing happen . 're seeing integrate deeply intelligence database applying ai database create information better database super interesting . go -- talk forever , tied directly large model , ai working internal system well inform . [ ph ] get -- 's specialization happening . -- 're seeing multiple different model specialization different layer sequence like keep conversation intelligent keep ai working memory adapt also significantly increasing compute requirement . 's chicken egg , saw -- try help . think 's what's driving every time launch new architecture , new interconnect technology , new innovative thing grace hopper , dgx , gh 200. expand scope researcher developer nvidia 's research order move large language model generative ai forward . next chapter probably reasoning . 's interesting 're seeing -- right generative ai , talk reasoning future , 's kind neither 're going 's even harder blue sky problem . well , okay . look like 're going growth category time . listening investor participant , like ask question , click question ask question button right screen come read ian talk . new metric , 's kind interesting . talking contact silicon valley maybe six month ago . price hopper dgx hopper starting come really , really expensive . people saying , way 're going pay kind price kind system much expensive say peer , yet probably hand amount better part year , kind brings mind issue ai model training inference little upfront price . 's le relevant really tco aspect eﬃciency aspect come play , determine come market , architect compute gpus ? yeah . 's -- really appreciate question , 's one get asked lot community entire community world see pricing see sticker shock . way , -- usually n't realize take build hyperscale data center cost go . multi-billion dollar investment new . people building datacenters scale get work hyperscalers . productivity utility compute incredibly important order improve service , improve 're , optimize business increase revenue . compute critical add generation , putting right content front , keeping engagement score high , keeping product want -- provide service . nothing annoying getting use add getting one actually thing want information need lead revenue . 's critical . people see -- maybe see cost gpu , create opportunity invest build service make turn ai opportunity provides based competing data . specifically though talk generation generation , think introducing new gpu , new technology market gtu . -- revolutionizing , compute capability , also tco analysis today existing product next . hopper provides six time compute performance transformer level , implementing transformer layer ampere , six time . end delivering end-to-end training . 's delivering three four time performance , 's complete training job throughput , front , even . infronts -- optimized , course . -- think . think -- look cost individual one , 's throughput entire data center going based today 're going able tomorrow . save ton money . save ton money transitioning one generation next opportunity performance economic tco hugely favor term throughput data center productivity data center , one billion dollar investment , billion take build data center around world . story play enterprise well . moving workload cpu previous generous gpus new gpus , throughput system rack data center data center scale measured x factor often . certainly , model , transformer-based model , including , also look breadth different workload , including model represented image recognition , benchmark see ( inaudible ) example . ( inaudible ) guy n't heard , benchmark 's created google sort provide level-playing field , clean clear benchmark . representative training workload since meta also contributing workload provide honest benchmark change correct level accuracy q - han mosesmann - ian buck q - han mosesmann - ian buckconversions requirement . use measure performance market based previous generation . see hopper done compared ampere . interesting point -- n't stop ship . continuously invest software optimization . suffer massive part . started software engineer manager nvidia cuda hired thousand software engineer others across company . one reason job importance software 's interface rest world people consuming technology , partnering framework like pytorch jax ton ﬂow everything else , rest ecosystem user community . nvidia point software engineer power engineer good march . -- first round benchmarking something like hopper , continuously improve . fact , ampere life got , believe , two half , three time faster . okay . yeah . first time -- go look first time submitted ( inaudible ) benchmark , 's public -- think 've recently stopped submitting shifted hopper . see 2.5 time x [ ph ] improvement model use case . -- mean , 's kind user experience . think 's loyal community user , developer community well biggest customer we're continuously optimizing whole stack platform along improve tco . great . hey , ian , get question . 's interesting one . expand current issue scaling sequence length might solved ? seems push new architecture favorable scaling function . would risk opportunity nvidia 's advantage transformer engine ? yeah , good question . let elaborate little bit . want working customer user community . take relatively small large model , larger input sequence length provides context conversation moving forward , mentioned tuck-ins starting hundred going thousand want push higher . increase compute complexity inference job want tune training . scalability really important , also capacity important . creates larger working memory larger model . ( inaudible ) way address . one scaling . obviously , throughput , 's multiple way optimize . first , transforming , mentioned . hopper revolutionary impactful may need called fpa [ ph ] fiable . fpa , eight bit ﬂoating point presentation , 's basically eight zero one represent ﬂoating point number . 's lot information . 's number character alphanumeric keyboard , example time two . every character type eight per character roughly double , 's balance actually move bit . [ ph ] make training work fpa , 's incredibly fast . obviously , computing eight bit faster computing 16 bit . also memory size half would 16 bit ﬂoating point , everybody really . [ ph ] transformer engine specifically designed . ca n't put expect cut number information eightfold exponential -- eightfold order make training successfully . transformering [ ph ] hopper actually combination hardware software make sure transformer model train convergence 's information corporate community . 's ton work make . actually consumed massive amount supercomputing capability meet work understand , tune figure keep thing within range eight bit . mentioned sequencing , reduce size model , size working set . fit 96 94 80 gig gpu depending ﬂavor available hopper , course , keep response time fast respond question range usability . , scale . scale . need gpu computing order expand , scale amd link . technology called amd link , allows 's -- hopper , 's 900 gigabyte second , lot . 's roughly seven time think get like pcie try use standard pcie connect new device start system . basically combine two gpus together one . 'll split model actually execute model parallel across two gpus . need much bandwidth gpus order keep thing going , keep thing -- make -- allow gpus operate one start model keep latency response time low . need , go two gpus nvl h100 nvl product , actually two pcie carved bridge eight-way . ( inaudible ) system go across eight , beyond , use trick use infiniband , go way ranger dgx gh 200 , 256 gpus connected amd link . announced computex two week ago . thing size model . bigger model , even need latency smaller model longer sequence link , could q - han mosesmann - ian buck q - han mosesmann - ian buckbe served single gpu , single hopper term performance . , grace hopper . grace hopper -- 've announced . 've talking gtc . n't seen gtc conference , check . grace hopper basically 600 gigabyte gpu . combine gpu upwards 96 gigabyte hbm memory cpu gloom together linking , gpu take advantage cpu memory , operates upwards 500 gigabyte , 600 gigabyte second . effectively 600 gigabyte gpu also help - - larger sequence link . lot way -- actually , pitty -- community analysis . 's becoming complex matrix model size , latency requirement sequence length , 're blanketed space . -- 's 're creating -- see u creating many different brand different product hopper pca form factor , hopper amd link dgx form factor , two pcas bridge together grace hopper used deploy inference scale . 's -- 's good answer . yeah . apologize . every day , making sure working hyperscalers startup everyone else dial create new product address . look like 're blanketing market various type product , counter different proprietary new architecture emerged composed . kind like saying ? yeah . others multiple -- 's one click anymore nvidia 's roadmap . think 's kind used . pascal gpu , three year later , 's volta , three-year later , ampere ( inaudible ) nvidia -- 've working diversifying way add value . instead bringing , build cpu , gpus bpus . work ( technical diﬃculty ) infiniband ethernet , make platform ai-capable different path . play optimize connect thing together build different product within -- even within one gpu traditionally generation meet demand wherever want go based guy going . q - han mosesmann - ian buck - simona jankowski q - han mosesmann - ian buckso agility really important ai , thing invented all-time . nvidia -- sort one ai company work every ai company , that's 're seeing product -- meeting different -- see different aspect believe able dial bring-to-market . perhaps parallel partner trying meet demand meet -- optimize workload . thing 'll say 've also accelerated gpu roadmap . used gpu -- 100 class gpus every three year . 're two year , case 18-month cycle . jensen talked hopper-next , timeline addition , time -- would grace next , time quantum next interconnect , 've accelerated . 're able invest chief sure every two year 18 month kind look . 's good know . got bunch question come . n't lot time . tactical one . 'm sure answer . maybe simona come . please talk effort source supply second half year ? nvidia define significant mentioned latest conference call ? yeah . simona comment little bit conference call detail follow . sure , happy . hope guy hear okay . commented earnings call going substantially higher supply second- half relative first half year , essentially back extended demand visibility see stretching quarter year-end well . commented , seen pretty steep increase demand quarter , way leading current time . 're working closely customer ensure supply . also helped underpin strong guidance gave second quarter , even higher baseline second quarter , commented substantially higher level supply second-half versus first-half . n't granular exact linearity q3 q4 , give u bit time get closer back half year , 'll able provide guidance quarter-by-quarter . okay . q - han mosesmann - ian buckno , 's pretty much . n't much add . certainly -- biggest customer course playing u everyone swarming generative ai , part . able -- -- working , course , plan -- planning continue 're thing simona mentioned time . okay . 's another question along line , maybe answer . biggest bottleneck nvidia gpus broadly ? much time think 'll take industry build suﬃcient inventory supply level ? know -- well , 'm going comment specific bottleneck supply standpoint . think challenge bottleneck perhaps adoption , 's really bottleneck . 's going broadening . 're seeing enterprise pick ai , happen , people provider ai including nvidia need meet enterprise , -- ai expertise . either acquisition hired brought in-house working -- closely startup others adopt ai -- inﬂuence improve business . see large language model startup , example , providing . really like work ai , example , know , making easier use older software . ai , click button check box instead existing software , 's degree thing . [ ph ] meeting need enterprise , term perhaps service taking appropriate model fine-tuning something useful , really thing enterprise need provide right kind data take convert appropriate model virtual assistant chat capability . lot activity right helping adopt ai workﬂows , product , work 're email product , example , exactly , 's checking easy-to-use . provide -- hundred maybe thousand two example , text text fine-tune gpt model way 175 larger answer question format context , instead asking generic chatgpt question continue generic answer generic human generic -- unit would answer , -- answer like financial expert support call expert thing , connecting ai information retrieval system . ask question , n't get answer , may may right , certainly , -- ca n't make chatgpt lie write code actually look right 's something made , actually get actual sourcing information . see little bit work bing , broadly , generative ai useful generate answer , tell source explore result . q - han mosesmann - ian buckso democratization -- -- connecting gpus industry big push right , 're starting see early mover space . 's next -- next wave gpu usage also revenue service thing like dgx great effort well partner going -- moving needle . last question got minute . let 's see keep 10 minute . conversation biggest client , hyperscalers enterprise changed last earnings call seems hear real wake-up call many decision-makers make serious investment ai . question , quickly changed since conference call basically two-three week ago ? n't know 's changed conference call . 's certainly changed chatgptmoment -- generative ai moment . converse probably continues amplified activity 're seeing street . opportunity generative ai every one service , every one capability . -- seeing nvidia supplier gpus , supplier infrastructure , partner many level . always partner hyperscale effort . development server , design data center , build something even capable optimized power-eﬃcient at-scale . every one unique challenge capability technology contribute working nvidia make work well . amazon 's efa , elastic adapter , tell lot work make sure work scale . hyperscalers , also use infiniband . 're working scale infiniband 're going platform . [ ph ] 've always partner data center partner . 's amplified side . [ ph ] , 's grace grace hopper cpu land space , exciting . step software side . capability software infrastructure integrating different framework , core capability broadening across service , developer researcher get access infrastructure meet , amplified . always 're partnering ( inaudible ) jax others , certainly continued grown . 're seeing service group seeing nvidia also partner optimize latest platform offer generative ai . seeing opportunity work le , moving workload even still cpu easing little bit ai simplified ai , using much intelligent larger model improve quality service better interaction q - han mosesmann - ian buck q - han mosesmann - simona jankowskiwith device , even 's talking hockey puck kitchen counter , cloud , -- 're also partner optimize model . 2.5x , 3x a100 n't u working back oﬃce . optimization customer -- biggest customer giving u challenge side side working optimize workload , get obviously reﬂected thing like benchmark elsewhere . amplified certainly , engagement service team deploying ai figuring run use hopper , use hopper scale inference better eﬃciently , move workload gpu structure plan growth moving forward big part . definitely gone quite check . well , could imagine . well , ian , thank much . enlightening . look like 're going hiring another thousand software engineer . hopefully , interview , exciting time . simona , thank well . look forward group session later afternoon . great day , thanks . thank , hope see person . got . thank . bye-bye .