okay thanks much hi everyone brett simpson arete pleasure welcome ian buck many know inventor cuda nvidia cuda software headed data center division many year think longer covered nvidia actually ian excited today thanks joining u thank happy help think particularly interesting time connect ian keen get ian perspective ai market going trend next couple year obviously phenomenal year think go back last year spoke ian think annualizing 16 billion revenue data center think pretty much term quarterly sale phenomenal achievement great see start going hand ian maybe talk little bit disclosure statement ian get started yes really quick mean reminder presentation talk may contain forward statement brett simpson brett simpson ian buckinvestors advised always read report filed sec information related risk uncertainty facing business great start call going cover prepared question 45 minute going open investor q operator instruction question answer maybe jumping right ian start maybe thought reviewing year gone obviously phenomenal year commercially looking 2023 setting big priority next year two data center division yes mean obviously oﬃcial chatgpt moment actually happened end really stride impact felt 2023 really created showed world opportunity ai generative ai could interact computer interact cloud like use software think ever least recent history technology introduction unlike maybe pc phone change modality create whole new ecosystem new opportunity new way working clearly happening gen ai new service new kind experience new consumer product new business opportunity also making old stuff way interesting word got sexy always liked powerpoint like actually like working angle seeing commercialization ai happened really make useful tool literally everyone given killer app working ai gosh almost 10 year since first time nvidia gpus used mean found u find back alexnet imagenet day really ramped year couple great advance result nvidia really talked data company data center new unit compute language model generative ai data problem really brought different technology together whether gpus dpus cpu provide infrastructure addition software go nvidia company software engineer hardware engineer opportunity capability deliver certainly llm generative ai ramp nvidia product perspective extremely thrilled brett simpson ian buckabout progress made grace grace hopper seen brought market seen opportunity provide inferencing platform seen supercomputing hpc business often leading indicator going obviously gpus scale 20 year next year slated deploy order 200 exaﬂops ai across supercomputing grace hopper finally course grace hopper going ai year saw first glimpse announcement made invent taking grace hopper putting together nvlink inaudible rack build ai supercomputer aws 16,000 gpus double benefit also dgx cloud partner four dgx cloud partner azure oci gcp aws working together imagine life pretty busy front think naturally carry accelerate next year got enterprise adopting ai building service capability partnering nvidia ask question data whether like servicenow dropbox happening dgx cloud engagement happening ai factory standing software either framework service putting together partnership enterprise yes well let take point laid ian maybe first ai factory strategy maybe lay mean obviously public cloud mainly conventional compute installed base building significant gpus 202 ph cloud explain u ai factory whether scale operation amount power need looking term deployment many ai factory need next couple year give u sense thinking yes pivot change perhaps cloud past even gpu cloud past would rent 1 per gpu single gpu maybe get node eight gpu hgx platform fantastic lot thing put together still today think ai factory model gen ai need work scale scale vary think within rack within row across data center ai scale training sure case inference large model happening scale different kind compute result engagement enterprise little bit different instead renting infrastructure training foundational model business start data one thing may noticed leader number token train gone quite bit need make graph used everyone feel satisfied maybe hundred million 1 billion one trillion token model may growing number token get quality increasing foundation model also driving sense scale think cloud unicorn company building using using infrastructure service ai factory place access data able train various scale able process refine transform various amount data turn ai model quickly course monetize able take model product provide insight creates business opportunity provide additional service capability turn data compute ai asset becomes business various way inaudible product really want one workload right ai model going training inference know computing training inference linked fact train model first inference model teach got wrong forward past version building data center ai factory one use case running operating ai model scale product engagement work ecosystem course hardware platform today hgx h100 hgx h200 hopefully seen increased memory size 144 gigabyte per gpu delivering five terabyte second number double kind performance ai existing hopper going grace hopper take another click term power optimization course nvlink scale like aws work top work networking ethernet infiniband enable build ai factory think early seeing first ai factory happen four leading customer announcement azure around mlperf great example trained gpts mlperf benchmark training gpt 175b across 10,000 gpus full infiniband cluster give sense scale ai factory inaudible copy eos supercomputer use research ai factory dgx cloud business ai factory business make capability available fortune 500 able access ai factory rest world continues build capability region cloud brett simpson ian buck brett simpson ian buck yes thinking fast forward 2 3 four year fortune 500 going ai factory mean saw partnership foxconn sound like pretty close deploying something ai factory early next year yes mean build ai factory nvidia work entire ecosystem taking building server system baseboard technology push limit thing like nvlink water cooling interconnect work data center ecosystem includes taiwan system taiwan system rack scale course data center able helping guide fit today data center retrofit today data center provide capability value data center plan provide thing like water cooling facility water give sense direction data center planning course major capital expense take multiyear road map big construction project see end product partnership customer using ai factory seen probably le get glimpse nvidia working ecosystem help accelerate ability everyone stand ai factory ecosystem right support scale talking kind data center kind ai factory people want build okay maybe talking dgx cloud mean talked brieﬂy mean mentioned aws agreement great many actual customer using dgx cloud today think intention going bring customer platform maybe give u snapshot term activity around dgx cloud think also talked 1 billion annualized software revenue division day much coming dgx cloud yes actually sure provided date information provided term number customer others think simona team provide publicly disclosed earnings elsewhere guide focus many see many lighthouse customer brett simpson ian buckand customer case obvious sense data want turn capitalize train build foundational model service capability servicenow data better minefield everybody support help ticket back forth engagement provide better experience support even potentially thing like preventive maintenance guide company service need help dropbox data company provide ability train learn data provide service able ask question data work done genentech engine vertically applied use case drug discovery using platform called bionemo help mine vast majority drug data discovery data accelerate time drug discovery anything cost develop drug measured trillion anything applied ai certainty pull schedule reduces cost development dramatically increase time revenue breakthrough drug obviously customer want work nvidia want make sure get latest also inform u help u drive accelerate technology platform obviously inform infrastructure inform hardware side importantly software engagement provide dgx cloud ai factory infrastructure also software nemo service talked nemo framework framework like pytorch inaudible training large model foundation monitoring scale many use case across industry nemo rest service provide way enterprise directly work nvidia work software train model service api format ai ninja course take dgx cloud containerize opportunity go deploy wherever choose often cloud hosting dgx cloud many great partner move entirely monetization come engagement dgx cloud service also nvidia ai enterprise software support run time attached container deploy continuously maintain engagement best nvidia get support rely nvidia basically providing support software need order continue run business integrate scale driving software revenue guy service revenue yes much brett simpson ian buck okay talk little bit demand ian guess seen obviously great trend last 12 month looking beyond say 2024 look demand ai whether corporate level government spending even lot enterprise software company building demand sustainability seeing today inﬂection point seeing today yes certainly obviously demand gen ai insatiable creates make job entertaining help forecasting make exciting challenge aware going growing certainly major cloud provider capability big muscle scale would use scale perhaps standing cluster web server ph end operation know operate hyperscale changed kind server infrastructure standing different used thin lightweight i/o little bit cpu okay slow storage simple ethernet nic standing ai supercomputer skill muscle scale obviously leveraged course ability invest also see also ability build data center build data center obviously also accelerating going continue see major obviously building ai factory ai service pick along nvidia road map enterprise choice consume consume directly cloud still see well ability work major cloud prem data center work regional cloud provider gpu specialty cloud provider operate potentially hyperscales smaller bare metal sometimes little quicker sense quickly bring market new technology execute directly customer versus building cloud infrastructure definitely emerged great another way people consume get access latest nvidia technology see customer three mentioned sovereign ai new thrust talking ai nation sovereign ai little bit definitely seen tick 2023 continue tick moving forward recently announced supercomputer u.k. built grace hopper announced actually prime minister opportunity build u.k. could resource ai brett simpson ian bucknation company industry put right forefront ai nation obviously see happening u.s. seeing happen across europe part world well continue grow dovetail nicely supercomputing business building ai supercomputer actually build gpu used train giant model gpu capability going supercomputing pleasure also help camp ph think new growth angle new opportunity think hearing sovereign ai project around world guess sit back look sort early stage gen ai g2000 market look opportunity sovereign cloud get built looking beyond sort like next sort six 12 month must pretty confident pretty good growth trajectory sort medium long term given nascent lot trend nvidia yes think challenge mentioned think call historically ai entirely new computing capability required different kind software stack different kind invented new kind computer science surprise started place like google facebook back first many year sharing educating happening next click escape search engine news feed become tool enterprise happened mean see thrilled happened technology matured world world smartest people helping mature also build capability making adaptable many different modality result opportunity ai expanded since people know tailor apply different use case way enterprise adopt technology like rag using data along query tune improve experience able ask question proprietary data data model trained technology one also talked really happened year 2023 really crack open door ai allow enterprise ask question proprietary data vast majority data proprietary model trained thing public internet acquired market really value moving forward ability connect proprietary data inaudible enterprise like 90 data beginning seen internally hyperscalers use service data obviously benefited greatly brett simpson ian buckexternally seen trained public internet made something rest entire world experiencing become part zeitgeist vernacular everyone every consumer ph moving forward seeing opportunity broadening ai market every business every ph vertical apply maturity ai term applied technology software stack including nemo software also ability connect proprietary data customer case dropbox yes maybe switching gear little bit wanted get perspective inference opportunity see ahead inference guess seen much deployment yet guess also looking hearing lot folk industry want see big eﬃciencies saving guess lot company saying maybe gpu best architecture inferencing llm quite expensive wanted get perspective think nvidia need look new class accelerator strip double precision stuff maybe relevant inference try drive cost drive cost per query materially sustain leadership cost ownership advantage versus peer yes first let address see market certainly used explain needed gpu cpu inferencing limiting today really ca deploy model without form acceleration without using gpus see used clear probably product segmentation lightweight t4 gpu a100 training gpu reality today benefit blended entirely going hard unfortunately tease apart value model great need large either large language model want gpu whether 1 2 four eight 9 even multiple within rack interest grace hopper single grace hopper way inaudible grace hopper based model let talk little bit see let talk second point nvidia reduce cost inferencing reduce cost inferencing applying engineering smart software team study relentlessly way improve throughput gpu inferencing whether small gpu big gpu improve throughput looking running model massive effort codified software called tensorrt tensorrt run time optimizing thing inference recently year actually announced open source get brett simpsonencouraged push innovation make best possible software platform running inference inference thing even extreme could training need make training need maintain certain level computational numerical precision adjust model work derivative ai turn derivative still slope difference inference forecast instance announced published new performance number new blog monday past monday h200 ran falcon llama2 model infor ph precision ran fp eight bit precision eight 0 1 eight bit hard work figure make four bit precision work infor easy say infor like accuracy 95 99 maintaining accuracy llama2 one 180b 180 billion parameter single gpu possible run whole inaudible model four bit precision fact believe world largest model ever run single gpu period 180 billion parameter took cost inferencing cut half diﬃcult know comprehend field engaged entire ecosystem even nvidia researcher technique infor used called used metric called awq actually invented nvidia researcher put inaudible nvidia inaudible orbit encourage everyone watching closely performance blog tensorrt performance blog transformer engine framework dumb stuff constantly increasing trip performance even buy gpu continue make gpu throughput reduce cost throughout last thing say asked different kind gpus today everyone talk a100 h200 show big iron infrastructure also sell lot traditional pcie form factor gpu similar may see gaming pc fan fit nicely server slot version version even l4 fact amazon aws google announced bringing l4 market l40 two pci gpus different price point great universal gpus excellent value deploying running inference effectively minimal change economics take server turn ai server adding l4 l4s people put 8 available u fact gambit gpus form factor continues expand would microsoft copilot inference today ian buck brett simpson ian buck ask question copilot worked closely team see announcement together work see closely nvidia microsoft openai work together yes great maybe one question open q wanted get perspective competitive dynamic seeing market mean guess early inning amd obviously came last night mi300 hyperscalers announced ai asics last month view competition next year 2 factoring much bigger tam 12 month ago competitive offering coming market yes yes mean nvidia 20 year always see important accelerated computing become every computing company ai company figure path way contributing way maybe exploring alternative option including building silicon certainly cloud built silicon many many year logical would also looking explore see close working partnership together google announced latest cpu keynote san francisco next person walk jensen talking collaboration dgx cloud h100 instance work collaborating building better cloud adam ph reinvent aws whole ecosystem advancing ai course advancement done plain sight optimization like talked improving providing horizon tag ai talk contribution want looking specialize offer unique want see opportunity speak nvidia competitive aspect moving extremely fast often unfortunately requires people sense perspective know going thoughtful compare claimed making sure information latest announced h200 blog talked tensorrt nemo transformer engine inaudible published number real open source brett simpson unidentified participant ian buckwe provide full accuracy throughput latest nvidia software really important innovation happening quickly always latest nvidia software understand everything h200 first provider hpe inaudible memory technology work closely memory partner mentioned whole ecosystem inaudible market gpu five terabyte second memory balance double performance h100 latest innovation monday doubled cooperating like 4x plus clip a100 a100 month ago also created growth ai also given u benefit accelerate investment think shared investor community pumping new architecture even faster stuff road map next year continue fast pacing paced field technology advancing ai definitely advancing nvidia continuously taking purchase data center full gpus know get increased performance throughput reduce cost year data center purchase obviously capital expenditure last three five year long investment need going ai factory producing asset going fuel growth company move forward diﬃcult predict next mean need see like ai going three year know nvidia continuously part system optimizing software stack making making data center productive ai evolves partner journey obviously go economics happens deploy scale deciding ai infrastructure going consume yes yes good think probably good junction open q colleague yanko ph announce question yanko ph guess couple question together everybody interested asking strategy compete amd inference given product price point product coming compared h100 claim make superior performance h100 like said encourage community look closely end workload measured performance delivered participate industry standard benchmark like mlperf mlperf created unidentified participant ian buckgoogle meta others industry benchmarking includes accuracy throughput training inference see work contributing basket model company actually smoothed every benchmark suite since inception four five year ago continue make sure understand latest nvidia performance latest software change economics 2x software organization available tensorrt change math everywhere part moving quickly h200 new hbm3e memory second shared number blog performance last comment make ai inference chip benchmark delivering performance across node scale whether eight gb inside system 32 gb inside system providing throughput easy talk ﬂops top number end decision making happens throughput performance model model evolves change scale making sure getting benefit ecosystem software improvement coming nvidia engineering also partner inaudible platform please use ... following next question nvidia plan pursue chiplet architecture road keep staying ahead competition monolithic architecture ca talk future product simona come talk let talk compute density second kind see happening ai go way back web scale ai ph datasets filled ethernet scale density factor ai computing computing generating revenue use power budget space computing number le watt dollar spent sending bite around compute move data want densify densifying bringing compute closer together spend le energy le joule le watt moving data optimize cost well going optic copper pcb extends even inside chip cheapest way move data within piece silicon unidentified participant ian buckchiplets great regard provide better form entire form communication course building would still better obviously able communicate chip able drive sign across chip across multichip model see benefit multichip model grace hopper grace hopper call super chip basically two chip put right next regard two model circle together drive 900 gigabyte second communication basically taking new nvidia technology building one super chip think densification also see built driving thing technology cooling building always think going see large chip ability optimize whole architecture compute apply idea densification silicon level package level server racking data center level well right next question foresee relative mix customer type data center gpus evolving nvidia future hyperscalers largest category today seems like government traditional enterprise startup category emerging bigger percentage yes hyperscalers two kind customer obviously providing compute capability infrastructure service amazon talked using nvidia gpus improve buyer seller experience use nvidia gpus help seller write effective descriptive product description model deployed nvidia gpus obviously use music search search amazon music right query literally query go infrastructure actually processed nvidia gpu nvidia software call apply ai model figure really mean inaudible song genre get right provide better search experience music used nvidia gpus nemo framework actually train collection model amazon titan bedrock example internal customer obviously course provide gpus market public cloud instance see azure gcp aws continue continue scale enterprise becoming much bigger portion obviously consumption ai factory starting see grow ratio 2 perhaps diﬃcult trying compare two exponential never good idea see enterprise become significant portion term consuming factory early day definitely trend slash unidentified participant ian buckthe new one think sovereign ai nation ability every every country see learned ai resource domestic industry help nation advance either solving important problem healthcare climate change weather forecasting probably science side thing important research see ai make understand thing better make better policy decision inﬂuence policymakers provide see inaudible economy landscape people course resource industry well every industry afford ai supercomputer government provide capability see new japan already talked providing infrastructure u.k. hear u.s. well definitely new trend hard prescribe mix kind growth curve think going probably balanced moving continue scale ai next rest decade next question view industry nvidia pursuing kind walled garden strategy similar apple seems competitor open source approach software also partner something like ultra ethernet consortium extent true yes thank question certainly like talk technology lot engineering true technology company regard probably pretty unique compared customer others first open company look look harbor okay make available gpus individual gpus make available gpus hgx baseboard make available rack design oems cloud take reference architecture deploy modify make different look ngx initiative reference architecture build gpu server going continue maintain bounding box thermal electricals move fast invest form factor know redesign everything available configure build allow every partner stack innovate way course want work directly nvidia gx business allow small portion activation across entire sustainability system whether cloud sustainability prems supercomputer software stack standpoint true nemo framework framework use train megatron megatron model 530 billion parameter brett simpson ian buckthe ai framework amazon used model train use case open source data tensorrt llm software stack provide reference model latest llama falcon name open sourced technique train world access need open source likely variance modification need support technology open source available ecosystem modify change go way program individual gpu fine software stack library open sourced couple closed since really truly optimized nvidia nvidia engineer term internal library frankly stuff would diﬃcult anyone nvidia engineer actually understand hopper stack everything dgx cloud sitting top cloud sitting top inaudible sitting top inaudible odm oem platform gpu open software innovation include one announced monday technology piece really want consume direct solution nvidia pick library interface integrate service unicorn example pick piece apply tell u give u feedback make even better guy may see time make openstack optimized another way putting one ai company work every ai company everyone platform wherever stack get benefit tends gravitate toward top get compound value everything precluded taking bit piece tailoring specializing workload maybe maybe one final question side ian china maybe share thought might play nvidia china guess obviously going see compliant gpu shipping market fairly soon think going received maybe firstly secondly think going see training move offshore china lastly indigenous platform hearing lot huawei scaling trying scale think going see shift towards indigenous platform market yes unfortunately ca go road map regard question good question ca answered forum look opportunity ai one hit every nation every country every business able meet demand obviously stay within regulation continue mean brett simpson unidentified participant brett simpson ian buckso adhering serve ai market however get guidance provide provided u company continue forward stuff think wait thing talked announced fair point good okay last question yanko ph ... question okay great well think time ian thanks much coming today sharing thought really appreciate always thanks everyone dialing going close call thanks much chat soon yes appreciate thank bye