hi everyone welcome gtc simona jankowski head investor relation nvidia hope chance view jensen news pack keynote morning also published several press release vlogs detailing today announcement next hour opportunity unpack discus today news ceo jensen huang cfo colette kress open q session financial analyst begin let quickly cover safe harbor statement today discussion may make statement based current expectation subject number significant risk uncertainty actual result may differ materially discussion factor could affect future financial result business please refer recent form report may file form security exchange commission statement made today based information currently available u except required law assume obligation update statement jensen huangwe start today brief comment jensen followed q session jensen colette let turn jensen hi everybody welcome gtc gtc conference developer inspire world deposit possibility accelerated computing celebrate work researcher scientist use please sure conference session cover really amazing topic gtc keynote highlighted several thing let go slide going colette cover basically first slide rest slide provided reference let make couple comment first core computing today fundamental dynamic work course inï¬‚uenced one important technology driver history industry moore law fundamentally come significant slowdown could argue moore law ended first time history longer possible using computing cpu gain necessary throughput without also corresponding amount increase cost power lack decreasing power effectively decreasing cost going make really hard world continue sustain increased workload maintaining sustainability computing one important factor dynamic computing today sustainability accelerate workload reclaim power use whatever reclaim invest back growth first thing waste power accelerate everything possibly really focused sustainability gave several example workload used highlight many case accelerate application 50 time 40 50 60 70 time 100 time process decreasing power order magnitude decrease cost factor 20 approach easy accelerated computing full stack challenge nvidia accelerated computing full stack talked many session past start architecture system system software acceleration library application inaudible data center scale computing architecture reason refactor application accelerated algorithm highly paralyzed also scale one benefit accelerated computing work scale also scale combination allowed u bring million x acceleration factor many application domain course one important one artificial intelligence nvidia accelerated computing platform also really important data center computer device make computer incredible instrument ability process multiple type application nvidia accelerated computing particle physic ï¬‚uid dynamic way robotics artificial intelligence forth computer graphic image processing video processing type domain consume enormous amount cpu core today enormous amount power opportunity accelerate reduce power reduce cost course nvidia accelerated computing platform cloud edge architecture available every cloud available every computer maker world available edge inferencing system autonomous system robotic car forth lastly one important characteristic nvidia accelerated computing platform although full stack design architect data center scale available cloud edge completely open meaning access literally computing platform computing maker anywhere world one important characteristic computing platform openness reach acceleration capability positive virtuous cycle positive virtuous cycle accelerated computing achieved accelerated computing artificial intelligence arrived talked three dynamic one sustainability mentioned second generative ai foundational work done last 10 year beginning really big breakthrough computer vision perception led industrial revolution autonomous vehicle robotics tipping tip iceberg generative ai gone beyond perception generation information longer understanding world also make recommendation generic content great value generative ai triggered inï¬‚ection point artificial intelligence driven increase adoption ai world importantly increase amount inference deployed world cloud data center third thing mentioned discussed keynote digitalization really taking artificial intelligence next phase next wave ai ai operating digital information generating text generating image ai operating factory physical plant autonomous system robotics particular case digitalization real opportunity automate world largest industry spoke digitalization one particular industry gave example omniverse digital physical operating system industrial digitalization demonstrated omniverse used beginning product conception architecture styling product design way collaboration design simulation product engineering electronics setting virtual plant way digital marketing retail every aspect physical product company digitalization opportunity automate help collaborate bring world physical world digital know exactly happens get world digital ability accelerate workï¬‚ows ability discover new product idea ability invent new business model tremendously increase spoke digitalization five takeaway spoke keynote talk today question area love entertain first course generative ai driving accelerating demand nvidia platform came year full enthusiasm hopper launch hopper designed transformer engine designed large language model people call foundation model transformer model transformer engine proven incredibly successful hopper adopted every cloud service provider know available oems really signaling increase demand hopper versus previous generation accelerating demand really signal inï¬‚ection ai used ai research generative ai moving deployment ai world industry importantly significant inference ai model generative ai driving accelerating demand second thing talked new chip coming marketplace care deeply accelerating every possible workload one important workload course artificial intelligence another important workload accelerate operating system entire data center imagine giant data center computer ï¬‚eets computer orchestrated operated one giant system operating system data center includes containerization ph virtualization networking storage importantly security isolation future confidential computing application operating software defined layer software layer run across entire data center fabric software layer consumes lot cpu core frankly would surprised many depending type data center operated would surprised 20 30 data center power dedicated networking fabric virtualization stack basically operating system stack want oï¬ „ oad accelerate operating system modern data center processor called bluefield announced whole bunch new partner cloud data center adopted bluefield excited product really believe going one important contribution make modern data center company design company wo resource design something complexity cloud data center everywhere announced grace hopper going used one major inference workload vector database data processing recommender system recommender system spoken past probably one valuable important application world today lot digital commerce lot digital content made possible sophisticated recommender system moving deep learning important opportunity u grace hopper designed specifically give u opportunity get 10x recommender system large database spoke grace grace production grace also sampling grace designed rest workload cloud data center possible accelerate accelerate everything left software really want strong single threaded performance single threaded performance grace designed also designed grace cpu fast computer cpu cloud data center think entire data center one computer data center computer way designed cpu context accelerated data center ai first cloud first data center cpu design radically different designed grace cpu excuse slightly reach grace cpu designed entire computer module cpu entire computer module grace superchip go possibly called system could rack whole bunch grace computer cloud data center yet performance threaded operation really excited grace sampling let see spoke lot generative ai increase amount inference workload going see one thing really important inference coming world data center really want accelerated one hand hand modal meaning many different type workload want inference sometimes want inference want bring inference ai video augmented generative ai sometimes image producing beautiful image helping sometimes generating text long text prompt could quite long long context could generating long text writing long program application one video image text course also vector database different characteristic another challenge course cloud data center one hand would like specialized accelerator one modality one diverse generative ai workload hand would like data center fungible workload moving dynamic new service coming new tenant coming people use different service different time day yet would like entire data center utilized much possible power architecture one architecture one architecture four different configuration run software stack mean depending time day one could always provision class configuration accelerator workload fungibility data center give ability architecture one architecture inference configuration inference platform give ability accelerate various workload best ability perfectly precisely predict amount workload entire data center ï¬‚exible fungible one architecture four configuration one biggest area collaboration collaboration partnership google cloud gcp working gcp across large area accelerated workload data processing data product spark rapid accelerate data product represents data processing probably represents 10 20 25 cloud data center workload probably one intensive cpu core workload opportunity accelerating bring 20x bring lot customer enjoy importantly lot power reduction associated also accelerating inference triton server also accelerating generative ai model google pioneering large language model accelerating putting onto inference platform l4 course streaming graphic streaming video opportunity accelerate two team collaborating take large amount workload could accelerated generative ai accelerated computing workload accelerating l4 platform gone public gcp really excited collaboration much tell soon third thing talked acceleration library mentioned accelerated computing full stack challenge unlike cpu software written compiled using compiler general purpose code run one wonderful advantage breakthrough cpu acceleration aspect want accelerate workload redesign application refactor algorithm altogether codify algorithm acceleration library acceleration library way linear algebra fft data processing use ï¬‚uid dynamic particle physic computer graphic forth quantum chemistry inverse physic image reconstruction forth one domain require acceleration library every acceleration library requires u understand domain work ecosystem create acceleration library connect application ecosystem power accelerate domain use every single constantly improving acceleration library benefit increased optimization investment capital already infrastructure already buy nvidia system benefit acceleration year come unusual u platform increase performance anywhere four 10 time installed life delighted continue improve library bring new feature optimization year optimized released 100 library 100 model 100 library model better performance better capability also announced several important new library one new library highlight culitho computational lithography inverse physic problem calculates max process calculates maximize equation go optic interacts photoresist mask ability basically inverse physic image processing make possible u use wavelength light much much larger final pattern want create wafer miracle fact look modern microchip manufacturing latest generation using 13.5 nanometer light near extreme ultraviolet yet using 13.5 nanometer light could pattern nanometer three nanometer pattern wafer mean basically like using fuzzy light fuzzy pen create really fine pattern piece paper ability requires magical instrument like asml magical instrument computational library synopsys miracle work tsmc field imaging called computational lithography worked last several year accelerate entire pipeline single largest workload eda today computationally intense million million cpu core running order make possible u create different mask step manufacturing process going get lot complicated coming year magic going bring future lithography going get increasingly high machine learning artificial intelligence surely involved first step u take entire stack accelerate course last four year accelerated computational lithography 50 time course reduces cycle time pipeline throughput time chip world manufactured really quite fantastic 40 billion 50 billion investment factory could reduce cycle time even 10 value world really quite extraordinary thing really fantastic also save enormous amount power case tsmc work done far opportunity take megawatt ten megawatt reduce factor 5 10 reduction power course make manufacturing sustainable important initiative u culitho excited lastly talk single largest expansion business model history know world becoming heavily cloud give opportunity engage computing platform quickly instantly web browser last last 10 year capability cloud continued advance point started cpu running hadoop mapreduce inquiry beginning know high performance computing scientific computing system ai supercomputer cloud going partner world cloud service provider starting oci also announced cloud partnership azure gcp going partner world leading cloud service provider implement install host nvidia ai nvidia omniverse nvidia dgx cloud cloud incredible capability one hand get fully optimized stack nvidia ai nvidia omniverse opportunity enjoyed world cloud optimized configuration get benefit nvidia software stack optimal form benefit working directly nvidia computer scientist expert company large workload would like benefit acceleration benefit advanced ai direct service engage world industry wonderful way u combine best nvidia brings best csps incredible service security cloud security storage api service offer well could likely already cloud selected first time ability combine best world bring nvidia best combine csps best make capability available world industry one service announced platform service nvidia ai nvidia omniverse infrastructure service nvidia dgx cloud also offered announced new layer many customer work many industry partner work build foundational model customer enterprise industry would like access foundation model obvious accessible thing work world leading service provider like open ai microsoft google example ai model designed highly available highly ï¬‚exible useful many industry company want build custom model based specifically data nvidia capability customer would like build custom model based proprietary data trained developed inference specific way whether guard rail would like put implement type instruction tuning would like perform type proprietary datasets would like retrieved whatever specific requirement language model generative image model 2d 3d video biology service allows u directly work help create model model deploy model nvidia dgx cloud mentioned dgx cloud run world major csps already csp choice pretty certain able hosted okay nvidia cloud service going expand business model offer infrastructure service dgx cloud platform service simona jankowski q toshiya hari jensen huang q toshiya hari jensen huangnvidia ai nvidia omniverse new ai service designed custom essentially foundry ai model available world industry world partnership world leading csps announcement made lot go thanks joining gtc colette answer question question answer thank jensen let welcome financial analyst q session going taking question zoom please use raise hand feature zoom would like ask question unmute called upon pause moment review queue take first question first question toshiya hari goldman sachs hi jensen colette hear okay perfectly nice see nice multiple speaker yeah thank much hosting jensen guess one question inference opportunity obviously dominate training space done many many year think inference side competitive landscape little bit mixed given incumbency around cpu obviously encouraging see introduce new inference platform guess criticality recommender system spoke growth llm work google seems like market moving direction think opportunity inference call three five year versus stand today think grace playing role next couple year thank toshiya thank first work backwards three five year ai supercomputer building today unquestionably advanced computer world make today course gigantic scale includes computing fabric like nvlink computing large computing computing fabric like infiniband sophisticated networking stitch altogether software stack operating system distributed computing u software computer science limit really going quite simona jankowskiexciting ai supercomputer going go beyond research extending essentially ai factory ai model people develop going improved basically forever believe every company intelligence manufacturer core company produce intelligence valuable data proprietary inside wall company capability create build ai system help curate data package data together could used help train proprietary model custom model accelerate business system ai training system continuous second inference inference largely cpu oriented workload reason inference world today fairly lightweight might recommending something related shopping inaudible forth kind recommendation largely done cpu future several reason even video processed cpu today future likely happen two fundamental dynamic inescapable point inevitable quite inescapable one sustainability ca continue take video workload process cpu ca take deep learning model even quality service little bit lesser good using cpu burn burn much power first reason accelerate everything sustainability accelerate everything moore law ended sensibility permeated every single cloud service provider amount workload requires acceleration increased much attention acceleration alertness acceleration increased secondarily everybody power limit order grow future really reclaim power acceleration put back growth second reason generative ai arrived going see every single industry benefiting augmenting accelerates everything text text create chatbots interact spreadsheet use powerpoint photoshop forth going going augmented going accelerated inspired think net ai training ai supercomputer become ai factory every company either cloud secondarily every interaction computer future generative ai connected therefore amount inference workload quite large sense inference balance larger inference larger training training going right thank next question come c.j muse evercore q c.j muse jensen huang q c.j muse jensen huanggood morning good afternoon hear yes c.j nice talk perfect well thank today want put question like focus grace past mostly discussed benefit grace hopper combined today also focusing bit grace standalone basis kind expecting speak whether changed view expected service technical diï¬ƒculty think potential revenue contribution time particularly think grace standalone grace superchip obviously grace hopper combined start punch line work backwards think grace big business u nowhere near scale accelerated computing reason genuinely feel every workload accelerated must accelerated everything data processing course computer graphic video processing generative ai every workload accelerated must accelerated basically leaf workload ca accelerated meaning converse another way saying threaded code code amdahl law still prevails everything left becomes bottleneck code largely related point data processing fetching lot moving lot data design cpu really good two thing well mean say let two thing plus design point two characteristic really want cpu one extremely good threaded performance many core good single threated core number two amount data move extraordinary one module move one terabyte per second data extraordinary amount data move want move want process data extremely low power reason innovated new way using cellphone dram enhanced data center resilience used server cost effective obviously cellphone volume high power power moving data going much workload vital u reduce lastly designed whole system instead building cpu core cpu designed superfast cpu node enhance ability data center powered limited able use many cpu possible think net accelerated computing dominant form computing future moore law come end simona jankowski q joseph moore jensen huangbut going remain going heavy data processing heavy data movement code cpu remain important design point would different past next question come joe moore morgan stanley please go ahead great thank much told inference question cost per query becoming major focus generative ai customer talking pretty significant reduction quarter year ahead talk mean nvidia going h100 workload guy work customer get cost yeah couple dynamic moving time hand model going get larger reason going get larger wanted perform task better better better every evidence capability quality versatility model correlated size model amount data train model want larger larger versatile hand many different type work workload remember need largest model inference every single workload reason 530 ph billion parameter model 40 billion parameter model 20 billion parameter model even 8 billion parameter model different model created way largely always need large model reason need large model minimum large model used help improve quality smaller model case kind like need professor improve quality student improve quality student forth many different use case going different size model optimize across use rightsized model rightsized application inference platform extends way l4 l40 one one announced week incredible thing hopper h100 nvlink call h100 nvl basically two hopper connected nvlink result 180 gigabyte 190 gigabyte almost 190 gigabyte hbm3 memory memory give ability inference modern size inference language model way would like use small configuration dual h100 system solution let partition 18 18 16 different correct wrong later 16 18 called multiple instance gpus mix miniature gpus fraction gpus could inferencing different language model whole thing could connected four could put pci express server commodity server used distribute large model across already reduced performance incredible already reduced cost language simona jankowski q timothy arcuri jensen huang simona jankowski q vivek aryainferencing factor 10 a100 going continue improve every single dimension making language model better making small model effective well making inference cost effective new inference platform like nvl importantly software stack constantly improving software stack course last couple two three year improved much mean order magnitude couple year expecting continue next question come tim arcuri ubs please go ahead thanks lot jensen think thought heard say google inferencing large language model system wanted confirm saying guess mean using new l4 platform brand new word using tpu ph using new l4 platform curious detail thanks partnership gcp big event inï¬‚ection point ai also inï¬‚ection point partnership lot engineer working together bring art model google cloud l4 versatile inference platform could use video inferencing image generation generative model text generation large language model mentioned keynote model working together google bring l4 platform l4 going phenomenal inference platform 75 watt performance chart incredibly easy deploy l4 one end show l4 l4 l4 h100 okay l4 l4 two processor 700 watt 75 watt power architecture one software stack run well depending model size depending quality service would like deploy could infrastructure fungible really excited partnership gcp model going bring inference platform gcp basically across board next question come vivek arya bank america please go ahead thanks taking question thank jensen colette informative event question curious availability hopper term supply colette kress jensen huang simona jensen heard range software service innovation track progress right last number think heard term software sale 100 million 1 sale would consider success next year percentage sale think could come software subscription overtime thank let first start vivek statement regarding supply h100 yes continue building h100 demand seen quarter keep mind also seen stronger demand hyperscale customer data center platform focus generative ai even last month since talked earnings seeing demand feel confident able serve market continue build supply feel good space time think software service substantial part business however know serve market every layer full stack company open platform meaning company would like customer would like work u infrastructure level hardware level delighted would like work u hardware plus library level delighted platform level delighted customer would like work u way service level level inclusive delighted opportunity grow three layer hardware layer course already large business colette mentioned part business generative ai driving acceleration business platform layer two layer stood cloud service company like going based subscription however know today world really need software cloud well ability u cloud hybrid cloud real advantage real benefit two software platform beginning lastly ai foundation service analyze beginning would say model presented last time includes sensibility talking today talking laying foundation path towards today big day u launch probably biggest business model expansion initiative history company think 300 million platform platform software ai software service today pulled still think size consistent described next question come raji gill needham q rajvindra gill jensen huang thank jensen thank presentation question technological perspective regarding relationship memory compute mentioned generative ai model creating huge amount compute think memory model view memory potential bottleneck solve memory disaggregation problem help understand thank yeah well turn computing everything bottleneck push limit computing living build normal computer know build extreme computer build type computer build processing bottleneck actual computation bottleneck memory bandwidth bottleneck memory capacity bottleneck networking computing fabric bottleneck networking bottleneck utilization bottleneck everything bottleneck live world bottleneck surrounded bottle thing true mentioning amount memory use memory capacity use increasing tremendously reason course generative ai work training model require lot memory inferencing requires lot memory native actual inferencing language model necessarily require lot memory however want connect retrieval model augments language model augments chatbot proprietary well curated data custom proprietary important maybe healthcare record maybe particular type domain biology maybe something chip design maybe ai database domain knowledge nvidia make nvidia click proprietary data embedded inside wall company using large language model could create datasets augment language model increasingly need large amount data need large fast data large amount data many idea course work done ssds work people cxl basically affordable attached disaggregate memory fantastic none fast memory affordable memory large amount accessible hot memory none fast memory need something like grace hopper need 1 terabyte per second access 0.5 terabyte data 1 terabyte per second 0.5 terabyte data wanted petabyte data distributed computing system imagine much bandwidth bringing bear approach high speed high capacity data processing exactly grace hopper designed simona jankowski q stacy rasgon jensen huang simona jankowski q aaron rakers next question come stacy rasgon bernstein research please go ahead hey guy thanks taking question appreciate wondering could go little bit economics dgx cloud business like actually pay infrastructure cloud vendor pay debt inaudible lease back running guess work customer paid get upside economics customer pricing anything give u actually work impact model would super helpful yes stacy thank first process go like presented nvidia dgx cloud partnership csp partner super excited reason onboarding well important customer large partner would consume storage security whole bunch application apis presented idea would like rent nvidia dgx cloud would take instance reserved instance called reserved instance market engage customer super super happy obviously nvidia deep relationship many large vertical ecosystem world highlighted slide deck sense guy healthcare drug discovery deep relationship many company deep relationship every car company planet two industry particularly great deal urgency take advantage latest generation ai generative ai omniverse digitalization first thing present idea proposal proposal partnership interested far incredibly enthusiastic would purchase system include people gear also includes gear standup dgx cloud cloud service provider procure whatever infrastructure power networking storage forth order standup infrastructure hosted manage okay step two step three take dgx cloud service market combination value would deliver would set price engage customer directly engage customer business next question come aaron rakers well fargo please go ahead jensen huang simona jankowskiyeah thanks taking question want go back think maybe c.j question earlier kind breadth grace maybe grace superchip grace cpu strategy think kind evolution maybe help u appreciate much data center cloud workload single threaded performance context foresee situation actually see server partner deploying grace cpu without necessarily deploying h100 subsequent version gpus see actual single cpu deployment market opportunity look backward appreciate question answer yes however grace really really targeted niche market let clear x86 use x86 company use x86 obviously pc workstation exciting intel sapphire rapid new workstation line using sapphire rapid dgx using sapphire rapid ovx server performance sapphire rapid really quite good excellent fact mentioned take application accelerate workload work part curve accelerate inaudible really left code threaded code either control oftentimes moving memory around giant amount memory managing memory amount data managing growing quite tremendously mentioned grace really designed type application data center largely accelerated well moving lot data said customer need x86 obviously represents large part world remains large part world expect continue x86 predominant platform make sense move enterprise computing arm necessary grace necessarily think focused application mentioned however csps already going move arm ph would like would like build cpu bespoke need requirement grace really great companion reason design point designed grace different design point almost cpu known designed think cloud data center moving direction arm really wonderful way either accelerate benefit entire software expertise system ecosystem peripheral ecosystem brought grace design point special really designed energy eï¬ƒcient extreme energy eï¬ƒcient cloud data center anybody interested particular area everyone world also important segment world think grace going successful even independent standalone cpu q matthew ramsay jensen huangour next question matt ramsay cowen thank simona thanks jensen colette guess two question jensen one kind something asked prior call transition happening think data center business selling accelerate card selling system really interested mean economics data center business term margin guess second question little bit related extends bit dgx cloud opportunity wanted one question getting lot last month month half since announced maybe take microsoft acute example like guy partnering really partnership friction point might want customer relationship evolve time could kind walk u seems like would want ai customer guy going go directly rented space cloud evolved overtime relationship largest csp customer bring business market thanks really appreciate question first question ca build software ca develop software ca generally develop software system company reason ca build software chip chip sit computer system company especially type software develop trying replicate somebody software building bespoke brand new software none software created existed created even computer graphic rtx full path tracing ai generation used modern computer graphic possible created software order create software system system company make nvidia unique build entire system data center literally start data center chip start data center build entire computer future data center computer entire data center computer something spoken coming decade one reason combination mellanox strategic important think people realize today work together architect data center really quite foundational way think world see world entire data center frankly even planetary scale computer think world starting includes computing element includes system includes networking storage compute fabric cpu forth way system software stack importantly algorithm library design data center way design design discipline disaggregate fractionalize customer would like buy aged dgx gpu right gpu look like today lot people think gpu look like course nvidia gpu run software stack kind miracle one run software went slower take ability u design entire data center disaggregate let customer decide best form factor best configuration best deployment methodology people use mpis people use kubernetes people use vmware people use container bare metal list go yet distributed computing stack affected work work industry across layer software disaggregate system component system software aggregate library run anywhere like workstation pc way cloud supercomputer disaggregate networking disaggregate switch disaggregate literally everything delighted put together would like u supercomputer like 30 day possible productized entire thing disaggregated integrated world industry standard wherever could result computing platform literally everywhere binary compatible magic think one reason able system company develop software hand computing platform company available everywhere respect lose customer csp would like direct customer relationship delighted reason whole bunch nvidia gpus cloud nvidia computing cloud software platform cloud anyways customer would like use way download nvidia ai enterprise run stack forth everything work exactly today however many customer like need work u refactor entire stack expertise understand entire stack take problem otherwise barely possible barely possible configuration meaning would like run cloud well would like run azure oci gcp well prem expertise help case need direct access engineer computer scientist also reason busy working industry leader would like build something quite special quite proprietary based platform need computing expertise make possible either deploy simona jankowski q blayne curtis jensen huangit scale want reach want level cost power would like reduce case contact u notice become direct customer interface would still invite csp partner offer storage offer rest apis offer security many industrial safety privacy data management regulation standard complied world leading csps expertise lot collaboration going happen come u terrific go csps fabulous happy either way next question come blayne curtis barclays please go ahead hey thanks let ask question wanted ask inference kind two part two inaudible small model large guess curious t4 card back ago think anything ampere l4 kind new version trying understand back t4 think talking inference similar said kind large market maybe even big training think became cpu changed guess feeling like smaller model need move accelerator trying understand large side nvl like 700 watt seems like lot power add every server customer thinking deploying huge model need lot horsepower one every cpu kind two part equation inference guy monetize yeah thanks t4 one successful product history million t4s cloud however ten million cpu cloud still lot workload cloud done cpu two reason really need accelerated one course sustainability accelerate every workload world ca continue consume power cpu throughput number one number two generative ai inï¬‚ection point question capability ai usefulness different industry generative ai connected think happened last couple month generative ai connected popular application planet oï¬ƒce team google doc popular productivity application history humanity term ai connected inference somewhere simona jankowski q w illiam stein jensen huangand think nvidia platform really ideal platform inference handle video handle text handle image handle 3d handle video handle well inaudible handle everything throw u think really inï¬‚ection point respect reason inaudible nothing today cloud data center thing really spectacular u get replace hundred cpu server reason accelerate reason accelerate spend 700 watt save 10 time 700 watt 7 kilowatt amount want accelerate everything sudden reclaim 6.9 kilowatt reinvest future workload okay motion conservation energy motion world csp going accelerate workload reclaim power invested new growth one two three step way put gpus world csps easy pc ph today thank time one last question come stein truist please go ahead great thanks much squeezing jensen several year ago really introduced world accelerate also oï¬ „ oad parallel processing compute maybe reintroduced something used exist ago certainly modern time like big computing revolution way product talking particular grace cpu bluefield dpu talk vision modern data center envision typical architecture looking like maybe three year five year dpu relevant grace cpu relevant traditional x86 see x86 server continuing perpetuate enterprise traditional enterprise software see going away love sort view thank really appreciate believe data center next five 10 year start 10 year work way back even five year work way back basically look like ai factory insight ai factory working 24x7 ai factory take data input refine data transform data intelligence ai factory data center factory reason factory one job one job either refining improving enhancing large language model foundation model recommender system factory job every single day engineer constantly improving enhancing giving new model new data create new intelligence every data center number one ai factory inference ï¬‚eet inference ï¬‚eet support diverse set workload reason know video represents 80 world internet today video processed generate text generate image generate 3d graphic image 3d graphic populate virtual world virtual world run omniverse type computer omniverse computer course simulate physic insight simulate autonomous agent inside enable connect different application different tool would able essentially virtual integration plant digital twin ï¬‚eets computer car forth type virtual world simulation computer type inferencing system whether 3d inferencing case omniverse physic inferencing case omniverse different domain generative ai one configuration optimal domain fungible meaning one architecture able receive oï¬ „ oad work something oversubscribed excuse oversubscribed take pickup workload okay second part inference workload every single one node smart nics like dpu data center operating system processing unit going oï¬ „ oad oï¬ „ oad isolate really important isolate want tenant computer basically inside think world future zero trust application communication isolated either isolated encoding isolated virtualization operating system separated control plane ph separator compute plane control plane operating system data center run oï¬ „ oaded accelerated dpu bluefield another characteristic lastly whatever left possible accelerate code ultimately single threaded whatever left need run cpu possibly cpu level entire compute node really reason people operate cpu operate computer nice cpu core rest data processing i/o ph memory consumes lot power point entire compute node many cpu lot x86 lot arm think two cpu architecture continue grow world data center ideally reclaimed power acceleration give world lot power grow acceleration reclaim grow process really vital future data center think represents canonical data center course different size scale know see question kind reveals mental image data center also explains vital one thing forgot say really vital connected two type network one type network computing fabric nvlink infiniband computing fabric really intended distributed computing moving lot data around orchestrating computation different computer another layer networking ethernet example control orchestration workload management forth deployment service user done ethernet switch nics super sophisticated copper direct drive fiber layer fabric vitally important see invest think data center scale start computation acceleration continue advance point everything becomes bottleneck whenever something becomes bottleneck specific viewpoint future nobody else building way nobody else could build way would take tackle endeavor go remove bottleneck computing industry one important bottleneck course nvlink another one infiniband another dpu bluefield talked grace remove bottleneck single threaded code large data processing code entire momento ph model computing think degree implemented quickly world csps reason clear two fundamental driver computing near future one sustainability acceleration vital second generative ai ai computing vital want thank joining gtc lot news consume appreciate excellent question importantly want thank researcher scientist took risk faith platform building last 2.5 decade continue advance accelerated computing used use technology used computing platform groundbreaking work amazing work really inspired rest world jump accelerated computing also want thank amazing employee nvidia incredible company felt ecosystem built thank everybody great night