thank much good day everyone glad could join u afternoon keynote session really delighted honored ian buck general manager vice president nvidia accelerated computing business also importantly inventor cuda key operating system underlying every nvidia accelerator really glad time share perspective ian turn think one opening remark would really love get perspective requirement ai hardware changed throughout tenure nvidia especially always talk hardware sometimes forget key part software ecosystem could give u perspective nvidia software capability really helped cement dominance hardware side ai yeah thank pleasure morning course reminder presentation contains statement investor always advised read report filed sec information related risk uncertainty facing business yes working accelerated computing quite time fact date way back 2006 first introduced cuda initially goal address program new kind architecture new kind processor reached level programmability beyond playing video game making beautiful picture become computing platform place accelerate every workload day always want make sure best cpu matched gpus right configuration right ratio portion competition accelerated typically either highly data parallel massively parallel intensive work community figure accelerate workload run architecture designed high compute throughput need started course computing community obviously looking using computer case case supercomputer simulate nature simulate physic simulate problem ca necessarily easily identified either wet lab microscope scale size scale earth cosmos need computer digital instrument instrument science 2006 first ai moment 2012 made platform available software platform made cuda available every one gpus including gaming graphic gpus one everyone workstation laptop pc course lot cloud traction building software platform engage developer rather strictly hardware platform defines isa met developer made easy researcher ph.d. student engineer company take care nvidia gpu download cuda free library software developed time figure apply problem port code whether bc fortran today python java others move compute rich portion decision upfront make software platform combination hardware platform really important couple reason first met developer wait others build software exists ramp frankly would diﬃcult taken long time given boot strapping problem second expanded innovation space innovate hardware layer compiler layer system software layer library layer course everyone else opportunity also contribute performance delivered time compounding innovation hardware side system driver developer software course library top track progress time quite dramatic benefit accelerated computing allows compounding value delivered q vivek arya ian buckit also allows nvidia innovate extreme click constrained interface lower level like instruction set constrained problem think address requires u change architecture change instruction set build entirely totally different kind gpu build gpu talk gpus going linked scale across gpus system gps rack across entire data center define interface engage extremely rapid click allows engineer produce new gpu architecture roughly two year case sooner allows u think differently cpu gpus connected also allows u expand entire data center canvas innovation making change inﬂuencing first decision think basically think different engagement point allowed u really innovate move quickly invite everyone else participate ecosystem guess along approaching 20 year nvidia go right part software stack ian substitutable example early day made lot sense right couple two many people also involved ecosystem whether whether r software r team many hardware competitor part software ecosystem substitutable take application written four nvidia find way port somebody else hardware example using combination tool software yeah great question get asked lot certainly possible think one workload one ai model one specific algorithm get working anyone hardware platform make hard make platform continuous optimization evolution platform run workload run inside data center today look software stack course multiple hardware platform ranging pcie card run 70 watt fit server l4 larger 300 watt pcie card board multiple gpus talking every link even shared scale entire rack scale even row scale gpus effectively top course system software compiler library get integrated opening ecosystem include hyperscalers software like pytorch software like pax ml wonderful part ai open innovate together ecosystem q vivek aryaso certainly possible spike different implementation different model stack make hard platform mentioned community need run different workload operate today across entire data center build data center around one model going run data center run large language model generative ai well data science use case need also want accelerate end end often also see someone inspect particular layer particular model deploy ai service ingestion data prep run query run model well produce output case perform multiple stage ai like wanted talk back apply text also done ai part platform innovation large language model generative ai standing still year ago still talking resonance talking coalition around network talking unit recommender thing thing still important many people innovating inside oems generative ai model innovated equipped way faster actually producing new architecture order platform firstly investing data center scale course huge capital investment take lot time need platform trust innovation happening generative ai going able run really well come performance optimization trying make certainly pay model get run model innovation platform much challenging ask one requires connection benchmarking customer giving input order continue make platform improving time find optimization everywhere one benefit front part working nvidia get work different ai company get optimize layer stack matter one part stack need simply replaced order port really get workload possible challenging sustainable let talk generative ai obviously caught everyone surprise good way right demand seems exploding talk first training generative ai inference training side seems like every day somebody launching yet another large language model nvidia dominates market training lot model see point get kind cliff maturation demand training think people start look optimizing size model actually somehow put pressure demand ian bucktraining hardware sustainable demand ai training already producing many large language model yeah large language model different made make large one question could ask large language model unlike computer vision model past simple simpler recommender model large language model effective directly acting human typically order directly interact human need understand human knowledge one reason gpt large trained trying represent interact corpus human understanding compare take download internet teach human know started baseline foundational model capture human understanding knowledge obviously much larger perhaps would need computer vision model still important trained set image eventually known set image tend large model also tend great foundation loss specialization specialize different workload specialize starting foundation model toward perhaps data starting something understands human understands interact human machine best use take proprietary data able interact ask question data course leverage general capability ask capacity going grow time interact computer cloud data hugely immensely valuable immensely valuable improving customer want interact company people helping customer want understand assistant sitting right able ask question get prompted information knowledge base thing better experience allows large language allows enable recommenders people want give content provide right content newsfeed commerce get right word right context shared literally touch every part company interaction customer sort answer understanding decade big data living tail think becomes continuous space innovation across board going one model rule large diversity different model based upon innovation going continue space also specialization across field way seeing healthcare science drug discovery q vivek arya ian bucklarge language model language human could language biology physic material science well growth vector look like becomes many rate many innovator adding defining betting new optimization technique new kind model start heroic amazing model coming people like openai gpt model seeing much research published model published inﬂuence create alternative derivative scale thinking generative ai large language model scope necessarily size model per se going remain large since remain large order baseline level foundational intelligence really scale grow industry company rest enterprise beyond adopts technique interact customer data apply business certainly hyperscalers first jump obviously talent capital ability basically invent much technology side side nvidia experience fascinating experience continue continue push limit figure apply see starting scale ai across business branching rest enterprise rest industry seeing whole tier cloud offering seeing specialty regional gpu data center popping everywhere serve market operated differently little agile perhaps bit smaller focused large litany middleware solution software company trying help enterprise company deploy technology across board definitely broadening large language model ecosystem adaptation generative ai language model business really scaling factor experience continue sure kind similar question applied generative ai inference side nvidia strategy generative ai inference perception training side company dominates product expensive come really scaling generative ai inference really think way customer monetize right end day going help monetize product pipeline look like help gen ai inference competitive landscape change move training inference thank question think people often get little bit confused perhaps certainly starting point model deploying begin training cluster stand infrastructure previously a100 hgx system system designed eight gpus getting connected via maximum possible performance course infiniband scale across entire data center today deployed right hopper technical diﬃculty train natural platform inference since training inference highly related model order train model first infer calculate error apply error back model make smarter first step training inference every event repeatedly natural customer deploying inference model training cluster hgx place probably see place see inference see inference happening across spectrum way l4 gpu brought one gpu half half length candy bar size small phone fit server server pcie slot become accelerated server fact seen cloud adopted oem rest system instruction probably great inferencing video encode decode capability seeing used smart city application image processing also run small loms recommenders small task also see generative ai generation running stable model provides price point comparable cpu fact make better much better tco cpu run model maybe finally return need go click l40 pcie card run often used larger inferencing task take existing foundational model last mile specialization data workload much lighter task larger training cluster could done l40 l40s server available across every oem system provides different price point different capability even way click system nvlink connected system often see people running single node need hit model certain size need execute certain latency say interactive half second latency response q example connecting nvlink basically build eight gpus term one gpu actually run model much faster prior latency inference platform consists many many choice optimize tco workload deliver performance case inference usually data center throughput certain latency important part roadmap software want go back easy look benchmark result see bar chart assume speed hardware often number say investment nvidia make software q vivek arya ian buckstack inference actually even apply even optimization within training inference kind last mile optimization model beyond perhaps capable training optimize hopper example using released actually last week new piece software called tensorrt optimizing compiler inference llm version optimization made software last month doubled hopper performance inference came whole bunch optimization optimizing tensor core h100 using ﬂoating point improving scheduling execution software managing gpus resource increase effective throughput competition eﬃciency really hard ask trying basically optimize using reduced precision using serving different side request quick q summarization task write along email generate full powerpoint data center going running hopper data center running inference generative ai going asked thing getting around eﬃciently able manage workload keep gpus 100 utilized actually pretty hard mathematical statistical ai system software even optimization continue last month doubled performance hopper inference continue see continue think industry right cost structure generative ai inference scale see user go take pick search engine right whether bard chatgpt even put inquiry today take several second get answer right different experience used traditional search engine think industry today seems like everyone training lot thing trying lot thing think industry actually cost structure take generative ai scale inference side imagine take really grow industry sustainable way next several year yeah great question today live inferencing experiencing course gpus naturally originally developed optimized deployed many large customer actually bringing hopper variant see 8x mean term performance term performance honest technical diﬃculty hardware side activate capability another bump software side q vivek aryaso expect interaction guy experiencing get better get intelligent think fixed latency want experience becomes question size capability model fit latency window process continuous improvement asked search every search type take advantage fully optimized take long aspect generative ai language model used today may know type search using word literally index actually applying language model generate optimized query string search based history thing seeing aspect also see thing like transformer large language model technology applied recommender system get last 100 document piece information wanted understand adjust produce result run smaller constrained transformer base model order provide last mile recommender ten hundred thousand whatever afford last mile recommender seeing technology deployed today put gpus today next click course richer experience search expect see hopper may take click every generation gpus every invention new software optimization technique every invention community whether next llama technical diﬃculty gpt bring cost inference hopper product a1 brought 8x tco also order 5x compound continuous software program compile new model algorithm technique order magnitude capability going available everyone best part gpus already purchased already fact performance delivering every one new piece software performance capable optimized gpus sorry optimized ai algorithm model three continue improvement tco performance experience fascinating time super busy seeing new innovation coming time definitely keeping nvidia community busy optimizing continuously optimizing platform got wanted get perspective competitive landscape look demand profile nvidia accelerated product right ten billion right expected increase next year give lot incentive hyperscale customer create custom asic solution one customer already tpu product custom solution long time others lot headline wanting done ian buck q vivek arya ian buckso first right positioning product versus internal solution use one one kind workload one become greater competitive threat nvidia going forward look happen gcp next conference conference think two week ago announced new variant processor day keynote keynote jensen joined stage talked innovation together google gcp new instance bring announced ga availability a3 instance also integration gpu vertex ai platform many research innovation happening gpus inside google elsewhere give example fact well hyperscalers absolutely mean invest optimize build something may tailored obviously important workload business continue partner deeply nvidia gpus software team two big company advancing together helping u helping u partnering together many software platform continue innovate see see see nvidia open platform course available every cloud open software ecosystem help advance ai data science accelerated computing holistically lift come almost 20 year investing software developer ecosystem continue see hyperscalers course building silicon mean optimize specific workload perhaps taking focus business still remain close connection nvidia see opportunity serve broader ecosystem also innovate nvidia platform accelerating computing something quite comfortable good partnership really evident got see change moving towards generative ai know cost training expensive cost inference also going quite expensive think increase desire bring asic solution done past choice fit want optimize invest one thing nvidia spending investing billion r optimize generative ai training inference scale every generation gpu every generation interconnect infiniband cx networking technology every innovation nvlink thing bring tco increase performance dramatically also bring cost training q vivek arya ian bucknow obviously motivated scale possibly order develop something uniquely advanced uniquely new different capitalize working nvidia basically opt new leverage billion dollar investment core workload training deploying inference large language model generative ai work space question going decide optimize take step something may different necessarily take advantage time energy investment nvidia making choice consider make going continue regardless innovate pace mean may benefit entire continue see thing happen sure would make sense focus changed continues swarm innovate increase performance lower cost also increase capability generative ai large language model thanks next topic ian wanted broach emerging class kind converged cpu gpu product example grace hopper competitor also announcing right product pro con using kind know whether converge cpu gpus right refer stack discrete solution using standard x86 cpu one many gpus pro con moving kind converged architecture yeah optimizing community optimizing accelerated computing ai 20 year moved huge amount competition gpu point many workload including many technical diﬃculty 95 99 computing done course gpus directly communicating nvlink across infiniband cpu workload either either small optimized done parallel overlap gpu competition combining appreciate cpu task usually around data perhaps scheduling managing coordinating execution every time increase gpu performance course need make sure cpu performance keep find leave model like amdahls law way manage one first use best possible cpu encourage use also adjust ratio cpu gpus today look dgx system two cpu eight gpus one go one four one eight course two one grace hopper went way one one one next one angle q vivek aryathe part though conversion happens combine cpu gpus something different traditional x86 architecture cpu sitting pcie gpu first bringing two converged together dramatically improve bandwidth communication two processor today 100 gigabyte second versus 60 gigabyte maybe 100 gigabyte pcie connection got also much coherent bring two memory system together memory gpu today ship 80 gigabyte hpm gpu going announced going 144 gigabyte per gpu connect grace connection fast 600 gigabyte memory around cpu basically becomes combined fast memory platform allow run even larger model basically effectively making 600 gigabyte gpu activates certain different lodged around larger model single platform single gpu cpu complex open new avenue new kind workload acceleration especially working large data application like vector database application like rationale network see lot finance fraud also used recommenders large datasets often want either run run run today across many gpus could run perhaps optimally different tco point much larger cpu like grace hopper 600 gigabyte combined one tied together third thing convergence allows u another vector innovation add thing cpu could optimize workload already know opportunity see future innovate cpu ecosystem cpu space addition gpu addition networking data center scale see work dgx gh200 connecting even gpus together excellent ratio nvlink large memory really give vision future infrastructure generative ai one basically 256 gpus connected nvlink fully backed 256 grace cpu effectively access one extra ﬂop gpu amazing generative ai platform training extremely large language model inference might need multiple gpus connected optimally technical diﬃculty really mean three thing larger provide larger memory starting point building block great scale platform inference result grace hopper fit server complete complex cpu gpu memory allows u play ratio explore different ratio different kind workload cpu gpu innovation space many innovation made grace using core soc architecture grace core talk quite powerful showing many benchmark provides great companion compute rich workload nvidia focused last two decade ian buckgot know minute left wanted get take last two question ian one role networking stack optimized generative ai cluster much advantage nvidia able leverage infiniband infiniband change ethernet mean conversely lose advantage also hyperscaler want move ethernet first role networking part cluster anything change move infiniband ethernet yeah great question basically three interconnects point choice design deploy ai nvlink previously inside gpus talk directly inside system going rack room scale infiniband certainly developed hpc supercomputing industry lowest possible latency data center scale really designed course ethernet industry established designed course high manageability capability come rich ecosystem feature enterprise cloud need order manage infrastructure see course nvlink continue closely tied innovation making inside gpus come fast go know bringing gpus gpus get faster want connect thing quickly possible order continue allow operate one get lowest possible latency inference giant model need technique around model parallelism extremely high requirement basically split model way instead way decrease latency infiniband also continues grow design point course lowest possible shortage latency result well provide excellent bandwidth see provide significant performance improvement leveraging perhaps rocky converged ethernet stack still lot deliver comparable performance fact support many cluster many deployment cloud scale ethernet rocky work great best possible performance infiniband get extra click basically come hpc heritage lowest latency high bandwidth optimization well network competition going math inside switch inside network fabric infiniband fully expect ethernet working community actually improve ethernet performance well great come manageability two exist three exist ecosystem continue get best three layer performance scale course requirement reliability q vivek arya ian buck q vivek aryamanageability security enterprise deployment versus maximum possible performance time roadmap continue instance go expected staggered continue learn absorb technology got finally would love get perspective term rolling generative ai look application seem infancy right many application look rate growth data center business seems big proportion total spending pie right give pause thinking already big part spending pie sustainable growth rate nvidia next several year fascinating question think today think growth experiencing right people taking existing data center make optimizing incorporate gpus om generative ai workload may coming hyperscalers enterprise wanting get board using cloud example seeing gpu regional specialty provider also standing infrastructure largely going data center already exist ca build data center overnight take two year plus save build infrastructure see world looking cheap hit building data center future seeing really exciting growth realize need build capacity course able build data center generic nature perhaps cpu focus majority server going building everyone hyperscale regional basically building gpu data center scale look growth data center build kind see opportunity oms continuing grow beyond able case quite literally crammed center already establish today versus size opportunity size market data center footprint growth capacity gone corner data center data center designed really exciting give confidence continued growth business see much company investing world investing building infrastructure different demand different need excellent exciting note ian thank much taking time u sharing perspective really appreciate thanks everyone ian buck q vivek aryawho joined webcast got another 45 question chat see work simona help answer question really thank much ian taking time immensely useful get perspective thank much always pleasure thank much bye take care thank