good morning good afternoon everyone thank huge thank everybody signed webinar understand realize time extremely important appreciate spending valuable time u today also special warm welcome nvidia team colette cfo lot know already gilad svp networking also special thanks simona stewart made possible kind get networking piece one two question colette based sort speculation percolating medium u.s. china colette maybe light last night press article regarding potential new export control ai chip shipment china tell u potential impact business nvidia thank much thanks question let see provide little bit understanding aware report u.s. department commerce considering control may restrict export a800 ph h800 product china however given strength demand product worldwide anticipate additional restriction adopted would immediate material impact financial result anticipate immediate material impact financial result restriction prohibiting sale data center gpus china implemented result permanent loss opportunity u.s. industry compete lead one world largest market impact future business financial result well clear colette thank could maybe help u one question getting lot morning colette kress harsh kumar colette kress harsh kumar gilad shainercontext around percentage data center revenue driven sale china yeah historically little bit range term seen historically believe contribution sale china range approximately 20 25 data center revenue keep mind includes compute product system also networking okay great one last one getting lot switch guy able pivot quickly a800 matter week people almost believed software change wanted clarify guy made switch a800 software change hardware change software change movement a800 absolutely hardware change made create a800 well create h800 thank colette supposed largely networking session think simona want turnover aaron ph want turn presentation gilad go get networking piece thing thank colette yeah thank much nice gilad svp networking came nvidia acquisition mellanox mellanox almost beginning 20 year plus nvidia mellanox think three year nvidia le started network designer early infiniband device part part design actually start looking entire platform software capability forth excited think move next slide think going slide number word make sure much le word coming slide presentation need say statement presentation may contain statement please refer sec filing risk uncertainty facing business move next slide talk network ai obviously talking ai bit network right design component need look entire system designing network essentially want build need build design full accelerated compute network balance system started nic start switch start gpu level memory io gpus way network essentially started application framework application level look everything great advantage nvidia look nvidia networking bar networking entity actually build design full ai platform use ai platform looking software perspective platform sdks library huge ton amount software part system case connects network gpus example full hardware capability compute server compute gpus switch nics forth able build essentially ability build entire system given opportunity unique opportunity place right data algorithm right place date algorithm want run gpu actually want run network call network computing element traditionally may network probably want gpu going much effective able move able build effective system delivers highest level performance large scale understanding build network actually built entire data center let talk data center dive network know data center computer today past cpu computer server become computer datacenters computer putting data center run workﬂows run application look data center big collection gpus actually way connect gpus define data center way connect gpus define gpus define kind workload run gpus essentially define data center look different example first example open ph cloud traditional cloud traditional cloud data center built support many many user support variety workload small even single node workload lot single node workload traditional cloud connected today traditional ethernet network traditional ethernet network good enough kind platform lot user lot application almost rational actually facing creation new kind cloud new kind cloud new class cloud cloud support gen ai workload cloud support ai workload ai workload different traditional workload run traditional cloud ai workload running single node ai workload need run multiple gpus one across node even important talk ai ai workload actually start talking distributed computing distributed computing completely different disaggregated computing completely different hyperscale something new something new cloud actually requires element support distributed computing started talking latency need day latency effective bandwidth completely different kind requirement traditional internet still fine traﬃc gen ai cloud need use access service control cloud need new class ethernet support new class foreclosing new class cloud exactly first ethernet design ai started interface people enjoy utilize ethernet ecosystem actually combining data center mission actually run massive workﬂows massive application large language large llm large language model complex training deal complex training new kind data different kind data center right talking many many many user variety workload talking much le number user workload going consume entire gpus system single application matter many gpus connect matter many gpus workﬂow consume network completely different thing want support workload going consume ten thousand gpus hundred thousand gpus option option become gold standard combination nvlink infiniband nvlink infiniband think internet completely different kind architecture designed specifically designed distributed computing optimized year network support running workﬂows gpus okay different kind network one network fit way show internet ph infiniband coexist continue coexist every system built need network user access deal network run control south traﬃc always ethernet great ai infrastructure need network need network compute distributed computing nvlink infiniband actually go stronger go next one coming slide going refer couple term want make sure everyone understand term going make life easier particular nccl sharp nccl nccl nccl short nvidia collective communication library software sdk software sdk ai communication ai communication multiple gpus essentially software framework support mainly two arrhythmics communication one reduction reduced operation one communication nccl essentially enables connection gpu side network side support two operation reduction operation operation nccl want measure ai networking performance ai networking performance networking performance ai nccl great great option test performance look performance nccl reduction operation performance nccl operation example demonstrate impact network good way actually test network measure network sharp technology part computing nvidia computing technology implemented infiniband switch asics something run cpu embedded within switch asic enable switch network perform data reduction operation data data transferred within data center previously data reduction operation part nccl done host running host big toll host going part nvidia advantage ability move algorithm one side another side run right place moving data reduction operation run switch network reduces amount data need send network half huge impact mean 400 gig infiniband network sharp better 800 gigabit per second network without sharp amazing capability infiniband one element enables infiniband making infiniband go standard ai factory look nccl impact nccl sharp see right gaining 1.7x sharp running nccl running reduction switch network compare best network actually beat would compare best theoretical performance ethernet 1.7x one key thing actually make infiniband gold standard ai ai factory go back talk different network start cloud go cloud cloud two kind ethernet network essentially two wall ethernet network connectivity controlled access user access cloud service cloud service user access loosely coupled application typically use tcp traﬃc jitter fine user access jitter jitter okay latency actually critical predictive constant performance bandwidth important well important deal heterogeneous traﬃc need deal multiple loosely coupled process process enable run network traditional ethernet used right ethernet designed kind cloud network know cloud second network compute network call traditional cloud much traﬃc workload user running single node therefore traditional cloud take network use west network fine okay work want host ai workload want cloud generative ai application network need completely something else network need deal disaggregated computing distributed computing sensitive latency even sensitive tail latency distributed computing run application across multiple gpus many many gpus sense one gpu communication going late one let say running 500 gpus one gpu communication going late 400 one entire workﬂows delayed entire workﬂow delayed tail latency critical element ai performance completely relevant traﬃc critical effective bandwidth important want provide constant performance change performance level need deal burstiness ph requirement distributed computing completely different would say opposite need use traditional ethernet need something else need different class network support new need ai application cloud reason reason designed needed new class ethernet kind infrastructure next slide please let let look left side see start 51.2 tera number port essentially forth right side see snapshot software developed x ton software sdks doca case run gpu bluefield provide network utilization ph isolation application infrastructure application infrastructure spectrum key switching magnum io sdk includes nccl framework mentioned operating system run spectrum switch sonic cumulus aspect like ton software would essentially design ai build new capability actually designed new capability ethernet capability including first lossless ethernet interesting essentially combination element going go first lossless ethernet want drop packet dropping packet mean creating jitter creating jitter reducing ai performance want drop packet top lossless ethernet want support adaptive routing ﬂow adaptive routing see ﬂow late adaptive routing ethernet switch traditional ethernet ﬂow ﬂow mean need run stream data change path stream data stream end good ai ai want fine grain adaptive routing want adaptive routing element enabled actually lossless even even want adaptive routing lossless network shallow buffer buffer buffer ethernet option example sometimes referred fabric ethernet sometimes run actually ethernet depend buffer big buffer switch shock observer congestion kind hold data stuff like buffer mean latency latency something nice ai workload want buffer idea combining lossless ethernet find good ph adaptive routing shallow buffer combination combination exist traditional ethernet completely exist one part advantage second part congestion control need eliminate hotspot designed congestion control based first telemetry information also unique capability network order identify latency change react hotspot impact performance application important key provide traﬃc resolution key eliminate noise make sure noise impact ai performance cloud run many many workload want make sure workload especially workload impact scale workload running network want make sure isolate noise small workload impact ai workload exactly congestion control based congestion control capability different latency change identify hotspot actually negative impact give u give u 1.6x higher ai fabric performance traditional internet talking 95 effective bandwidth skill load keeping performance constant predictive performance keeping performance constant even lot workload running environment cloud think security virtualized network everything part actually bring speed feed need ai ethernet interface people leverage ethernet ecosystem service built ethernet cloud service thing sort actually ethernet designed ai next slide please look support ai workload go infiniband infiniband starting see left side latest generation one thing need understand infiniband designed based different kind architecture versus ethernet ethernet built network time within data center algorithm designed ethernet algorithm designed ethernet pfc example bgp moderate designed ethernet ethernet complicated protocol complicated protocol build ethernet network need actually choose feature performance need choose feature performance ethernet one switch fit see variety switch coming different kind entity reason one switch fit switch shallow buffer port much good performance distributed computing supporting kind cloud interface switch buffer order support sometimes doca service application come issue day latency reduced number port forth need choose feature performance stuff actually designed right element inaudible actually created thing exist traditional ethernet infiniband look infiniband different kind architecture using architecture infiniband designed beginning support distributed computing reason infiniband protocol simple lightweight simple simple meaning infiniband leaf spine kind freaking term ethernet leaf spine ethernet try build network switch go beyond switch stuff like exist infiniband thing infiniband infiniband use many switch want even system using three level switch infiniband system even use four want use five use five performance penalty issue around build size system want like designing formula race car designing formula race car many seat going put car care different kind design look switch infiniband system use today go way 65,000 gpus go four level several four level multiple four level already go two million gpus infiniband network want go five go five limit many gpus connect together even see limit many gpus use single workﬂow important point limit infiniband gold standard ai infiniband pioneered rdma obviously lot element rdma infiniband pioneered rdma within ph full computing saw impact sharp sharp give 1.7x nccl compare best ethernet network build pure pure network designed sdn people knew sdn mean sdn mean control entire routing single place optimize routing workﬂows build different kind network topology treat change network quickly reconfigure network course ph port configure quickly huge amount benefit pure network give u look total performance small 2x gracious many gpus want building network loss latency large scale load short latency extremely short latency know impact sharp nccl operation nearly 100 effective bandwidth scale amazing network really amazing network developed 20 year right every generation bring new capability upcoming thing planning amazing completely amazing take infiniband completely next level compared anything else infiniband also tonne software right sdk element magnum io nccl two obviously management network able simulate everything ton software well important design next slide please look impact network network essentially small part data center small part data center expense huge impact huge impact ai performance essentially network pay network pay infiniband offer highest scalability build size system want 3 q harsh kumarlevels 4 level 5 level unlimited number gpus connect together looking performance took nccl nccl good indication network performance ai first see completely different design ethernet enabled ethernet ecosystem right want joint ecosystem need performance ai want build system going go scale want get highest level performance also bring infiniband cloud reason look infiniband kind amazing top look impact total ai performance network essentially free completely pay even someone going offer traditional ethernet free completely free going good enough right actually paying good infiniband going get much better much better right essentially building ai infrastructure network essentially free next slide think last one yeah look looking networking revenue nvidia networking revenue revenue doubled since mellanox acquisition within see breakdown infiniband ethernet infiniband tripled growing growing fast continue grow continue grow new class ethernet new class ethernet needed new class cloud therefore x boost cloud ai network market increase market increase ethernet revenue moving forward overall believe see essentially believe every data center become accelerated data center future data center accelerated used situation got 2x performance every two year nothing work anymore work anymore know want able increase capability accelerated computing therefore every data center become accelerated datacenter every server gpu processing unit every datacenter element talking 60 billion market opportunity nvidia networking side first thank listening took time happy answer question question answer yeah hey gilad thank much extremely informative actually answered whole bunch question one one get investor concern around fact already come nvidia compute come nvidia networking based merit example talked get lot question basically tied nvidia lot think people know semiconductor gilad shainer q harsh kumar gilad shainerbusiness always want option could maybe talk work around one make infiniband farmed place yeah well infiniband infiniband standard technology proprietary technology standard like ethernet ethernet also standard sense company definitely create infiniband device actually company build infiniband device different kind application company building device connectivity infiniband element fpga thing forth course infiniband open everyone use always guarantee networking right want use infiniband use ethernet want use ethernet use infiniband always choose question question understanding essentially especially look ai look ai ai requires datacenter skill look actually want right element inside said used get 2x performance every two year case therefore going see specialized technology actually us accelerated computing use technology enable achieve goal achieve goal optimizing ai workload performance performed discrete compute networking device level want look full stack approach important essentially would say time market time solution customer considers total cost ownership performance availability ph time build deploy scale architecture nvidia delivers deliver full ph platform huge amount optimization customer take whole customer want take piece whether wanted take piece thing happen market exist market great gilad one guy sort gold standard company accelerated datacenters come infiniband network adoption noticed big difference metric training versus inferencing example infiniband network either term port term metric think talk investor generally feel like inferencing common going huge opportunity wanted address yeah definitely good question training training requires cluster right tightly coupled optimized massive massive data q harsh kumar gilad shainer q harsh kumarcompute inferencing typically required much smaller smaller scale cluster happening generative ai becoming mainstream therefore number separate job running inferencing dramatically increase therefore inferencing require larger number accelerated server ﬂexibility essentially probably going see people going deploy system want use system training inferencing training inferencing therefore case obviously infiniband great option someone going inferencing need go large one course use going see probably system going use make sense build system used infiniband good option great one gilad perception investment community even people know generative ai well infiniband work nvidia gpus accurate listening talk seems like case wanted ask since expert topic yeah infiniband open used accelerated accelerated compute platform nvidia develop full stack platform customer choose take whole want take design copy design whole actually take piece take gpus use network take network use compute element free use platform definitely tied ph obviously lot benefit right invested lot effort lot effort investing customer much faster time computing much faster time solution much faster time build system build supercomputer ai supercomputer want spend nine month build nine month lifetime expensive system build system week month take full performance people choose take component use network within compute element forth wonderful know road want mindful time got two minute ask one final question typical setup let say guy go deploy accelerated ai datacenter typically find entire datacenter either infiniband ethernet possibility offering depending line supposed gilad shainer q harsh kumar gilad shainer yeah first obviously entire ethernet system know system full ethernet system different kind ethernet created order bring right class ethernet ai compute fabric definitely ethernet system want build cloud system want leverage ethernet ecosystem element need develop software cloud good answer give speed ph needed ai give ecosystem friendliness ethernet system definitely going exist side took need system essentially combining infiniband ethernet one versus completely completely going coexist large ai factory large system run large language model training ethernet south access infiniband built user access meant purpose interface ethernet compute fabric want connect large amount gpus thousand ten thousand hundred thousand gpu single workﬂow infiniband actually give combination nvlink infiniband give connectivity connectivity look system design system example recommend look coping enable leverage everything designed system includes infiniband ethernet completely coexit one replaces think network exist one purpose come end presentation gilad thank enough time particularly know road colette thank time appreciate comment thought early simona stewart thank help pulling together next time thank thank much