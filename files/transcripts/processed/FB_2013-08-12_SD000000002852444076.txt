hi everyone thanks coming evan wilson senior analyst following internet game pacific crest excited facebook presenting u today jason taylor director infrastructure foundation facebook lead group manage hardware design supply chain technical program management server budget allocation infrastructure plan prior joining facebook 2009 jason worked number startup including bluemountain.com also hold phd mit ultrafast laser quantum computing everybody jason taylor hello okay name jason taylor lead infrastructure group responsible server facebook pretty exciting last year term scaling facebook want take opportunity share picture infrastructure view think coming next talk place technology go could create eﬃciency opportunity industrywide potentially disrupt existing market next two five year focus mostly technology potential technology medium long term definitely next year begin reviewing facebook infrastructure talk eﬃciency far facebook area pursuing talk something idea working right disaggregated rack go detail talk new component think could designed brought market next couple year could highly impactful data center space broadly term scale 1.15 billion user 84 monthly active user outside united state data center five region term stats lot user 700 million people use facebook daily 350 million photo added day 240 billion photo aggregated far 4.5 billion like post comment per day active large infrastructure term cost infrastructure spend 2012 1.24 billion capital expenditure related purchase server networking equipment storage infrastructure construction data center scale eﬃciency work really top priority facebook several year going talk little bit facebook architecture give idea think think infrastructure scale think computing couple thing talk one cluster say cluster mean network cluster could cluster cluster essentially group server lot networking capacity organized group use really three different way first cluster cluster really workhorse facebook time anything facebook post message look something phone anything direction request sent one web server web server work rest server facebook infrastructure deliver content within web server service really scale directly busy site scale web server based many request get per second composition request ad multifeed small service tend scale put unit scaling server lot cluster term ad rack rack server talk rack talking think unit capacity server rack 8 foot tall 20 inch wide 26 inch deep contains anywhere 20 40 server multifeed something get quite bit essentially newsfeed facebook think facebook page think center column recent activity service dedicated serving piece talk software little bit also service cluster essentially computer dedicated serve particular purpose search set computer photo message several others cluster database database everything able run database back keep reliable first push cost optimization hardware really genesis open compute 2010 built call server really standard server except removed component typically found server use also data center computer together talk little bit done data center idea much solve engineering problem eﬃciently put server online data center built computer large variety customer would want use open compute focused web large infrastructure scale getting design service newsfeed rack common design u called leaf aggregator happens within rack 40 server server cooperate answer single question happens query come hit aggregator aggregator piece software running server also another piece software called leaf running server well leaf take ram aggregator computation reason aggregator asked okay jason viewing site web server say list jason friend tell one rack index activity every user facebook last couple day aggregator receive request ask server rack ask leaf data user set user set user set user subset sent aggregator data reduced sent web say rack unit capacity one server could answer question properly alone work together really scale needed put terabyte ram online make accessible order serve needed adopt something cooperative way talking little bit facebook serf client drawn time going request start hit web server first thing web server check login authentication hit lot caching server memcached server later might hit feed aggregator like multifeed newsfeed described list story data index might hit cache high cache hit rate maybe 96 one memcached server probably 1,200 server per cluster really lot lot cache server might need look something database might need add data memcache pull ad really send data thing little bit complicated essentially hit life facebook see really right pull lot data order serve page facebook look think different piece data really lot information term built facebook one big win one thing really helped u lot last year infrastructure five server type facebook engineer server want long one type variance change done designed sku particular major service mind web ton ton web server web server putting lot computation online cheaply database providing enough iop order service request coming web server hadoop kind big data service need lot compute lot storage photo gig least number dollar storing lot lot photo lot lot data petabyte data feed one type vi feed server rack popular leaf aggregator talked lot service consume major advantage constrained number server standardization get volume pricing right negotiating five different type really focus negotiation really work make sure getting best deal facebook advantage obvious repurposing repurposing service launch launching 1 billion user lot user right even within service launch product could le successful le popular another one make sure enough server online catch rather essentially buying different type server service buying maximum amount could possibly need run actually say little bit like mutual fund might five 10 service launching might something new service coming time kind make good bet say well guy guy guy going popular pretty sure least two going quite big projection need slosh around identical server across three service repurpose really big win u also make easier operation essentially every server new easy build maintain get good time reduced number server also allocate server hour rather month always five type hand unexpected need major drawback 40 major service also 200 minor one everyone fit five type perfectly also service need change time disaggregated rack one idea one thing working towards think address problem really u think would useful industrywide talk little bit minute getting onto eﬃciency short section want summarize focus eﬃciency last year get couple idea think going meaningful future term data center data center traditional data center like walmarts right ton power got air conditioner tend really cold server consume lot power put way much air conditioning capacity essentially data center really energetically ineﬃcient way measure industry pue pue total power consumed data center street divided amount power consumed server level beautifully perfectly eﬃcient data center power going server would pue 1 typical data center majority data center pue 1.9 mean electricity bill paying 90 every watt electricity consumed server paying 90 air condition room ineﬃcient system start little bit clever hot aisle containment cold aisle containment lot thing get pue 1.5 real trick get rid air conditioner completely lot work electrical system ups system talked proud pue data center 1.07 mean really pumping cold air outside running across server taking hot air pumping outside building ca really get eﬃcient running big wall fan happy data center team figure way generate power number going go lower server focused building server exactly needed really much lot supply chain optimization building server get much better deal commodity thing like software spent lot work building eﬃcient web server eﬃcient php web server stack hhvm effort something read interesting cache database tier work lot web service optimization eﬃciency facebook shared across major product group best guy able optimize code really men woman fantastic understand exactly need work closely eﬃciency getting next opportunity really talking disaggregated rack talk disaggregated rack talking able extend useful life component server made extending useful life compute ram drive ﬂash sort thing get detail also think essentially component work today result really 20 year industry focus providing solution desktop computer actually look large scale look large infrastructure scale would probably come different type component start talk component might look like disaggregated rack ignore fact 40 server think actually spending money like resource online resource compute 80 processor worth compute 6 terabyte ram 80 tb storage case 30 tb ﬂash raw resource need connect within rack think server think rack really want get resource line want able architecture fit think really thinking sled look like server providing one resource right focus disaggregated rack build something simple like web server really simple compute mowing lot computation think database got multiple bottleneck manage always buying weakest link database storage buying database database functioning well buying ram essentially lot waste hitting resource allocation perfectly compute really think standard server pretty close need really bringing many core possible cheap possible ram think basically idea ram sled server responds pair server nothing fill leaf thing data yes nothing low power probably modest processor think storage like knox sled really bringing compute think build small server slam back knox sled basically put four 15 drive directly onto network flash flash really sled appliance really putting lot ﬂash high network interconnect building block really three win disaggregated rack first server/service fit across service make rack us maybe four commodity four basic sled fit lot service much better service/server fit time talk little bit really key longer useful life think multifeed type 6 server essentially amount resource server need ram compute box much server provisioned service need square overall box server look another service one le intense essentially wasted cpu resource cpu expensive deploying service use cpu effectively buying ram best thing buy spend lot money time product change goal service change database evolve get data per compute sort thing time might find enough ram disaggregated rack slam another ram sled rack ram much serviceable scalable way building computer key useful life typically server data center three year sometimes little bit longer generally lot come three year become really awkward code building service really building want building future resource going server three year old diﬃcult think rack level imagine keeping compute maybe three six year ram sled five year disc easily four five year ﬂash could six year ﬂash wear bring back graph search think well got much compute ram storage ﬂash year graph search team need index data pretty likely considering facebook data growth pretty significant would much better u slam ﬂash sled buy 50 rack essentially one win happen maintained volume pricing serviceability essentially provide custom configuration service got data center full kind server everybody get slightly different mix rack hardware evolve service time also allows extend lifetime helpful get smarter technology refreshes better useful life better depreciation speed innovation nonobvious one think important essentially structure like try newer technology much easily essentially compete one compute resource another physical change required interface overhead look approximate win estimate number think fry electronics looking really commodity pricing pay point conservative assumption disagg rack could give 12 20 opex saving opex talking inclusive depreciation really tco win aggressive assumption promise 14 30 opex saving different infrastructure forecasting exactly win going facebook size win feel appropriate good investment part also industrywide interested move onto new component look last 20 year really 20 year design computer use data center almost identical 386 desktop 20 year ago fact think one 3.5 inch hard drive basically computer pci ebus look innovation last 20 year significant really amount math put two processor every server nice win multicore processor allowed chip scale significant gpus great vector math nice innovation anything us graphic image processing ﬂash memory course transformative lot database workload think fundamental component compute ram disk nic always asked never asked anything different going talk way pose engineering problem one fundamental component manufacturer bring market could different scale much better data center infrastructure scale market today server cpu really version desktop server see really mobile phone driving development processor one neat trick applying processor lot peripheral need support processor putting onto silicon case package really nice cost win really nice power win real estate win think server market essentially server processor adopting approach next year powerful idea think interesting think ram today ddr standard ram component built coevolved ram ram fit really well think lot alternative memory technology really could hit ddr standard term cost per performance basis way computer designed today entry point market place could actually start nothing connect think processor architecture software architecture really focused near ram far ram essentially got really fast ddr stuff got perhaps maybe even alternative technology faster ddr got much slower thing really filling space ram performance nanosecond scale ﬂash performance microsecond scale somewhere really lot interesting technology essentially ready part way design software think actually interesting opportunity next year figuring way allow alternative ram come online term storage photo data storage us drive almost exclusively workhorse photo data fantastic technology great everything work also designed somewhat desktop also designed workload lot update see facebook think going see industry course big data course massive massive amount medium photo storage going go away projection put u going 3 zetabytes data 1 billion terabyte 1 billion hard drive year ago amazing 40 zetabytes data 2020 going see lot lot data need stored think insight would like share lot data really cold word nobody ever look photo several year ago people often view essentially data want keep around long time sure value need keep might valuable future key thing want understand change edit photo later change something photo written stay forever data collected want data decay change time actually think ﬂash ﬂash alternative potentially lot technology could deliver write entrance whole ﬂash market really geared toward pushing towards higher higher iop higher higher performance higher higher endurance really pushing edge performance actually saying go really lowest common denominator build something kind barely work build something write try write ruin whole ton storage ton workload really want data keep little keep really long time never touch think big opportunity actually feel like could interesting course hard drive absolutely scale maybe optical drive thing like got two minute left okay term ﬂash maybe talk little bit flash used database focused higher higher write endurance performance think idea worm ﬂash remember back worm drive long time ago essentially write memory thing write ca change think actually really interesting opportunity ﬂash think perhaps ﬂash perhaps medias essentially different engineering problem present lot fantastic engineer major company one sanity check think rack ﬂash versus rack drive rack drive 2 petabyte data rack ﬂash 4 petabyte data consume power weigh amount get density online using laptop drive ridiculous form factor build differently focus silicon build far far denser silicon module could really create really interesting rack storage think pretty compelling last word nic 10 gig really common server today couple year feel like essentially last year lot really interesting research lot really interesting work done optical nics right forecast predict 40 gig 2018 100 gig 2020 threshold becomes somewhat commoditized essentially cheap think actually probably pulled year think next two three year probably see maybe even nics server level actually exciting thing disaggregated rack ﬂash number complication really think exciting time technology think lot interesting option going developed next year thing go well think change provide eﬃciency win facebook really web scale infrastructure whole thank time