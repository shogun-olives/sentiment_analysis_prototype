right think going go ahead get started stephen ju credit suisse internet equity research team joined jason taylor head infrastructure development effort facebook without ado take away great name jason taylor run group called infrastructure foundation facebook responsible server design server supply chain overall capacity management capacity engineering performance review thing like also infrastructure plan today going walk little bit infrastructure talk eﬃciency program excited look toward future eﬃciency computing facebook large 82 monthly active user outside united state global deployment international data center one lulea several united state 1.35 billion connect u monthly 1.2 billion mobile stunning 930 million photo uploaded site every day lot medium lot content distributed facebook 6 billion like 12 billion message per day active site dynamic built infrastructure accommodate last five year really eﬃciency top priority company initially would say really necessity facing huge uptick adoption facebook usage eﬃciency always core able scale reached became necessary financial viability also ability build platform scale well cost perspective really eﬃciency break three area data center heat management really one important thing term core eﬃciency poorly designed facility facility concentrate heat much could easily pay 50 90 additional electricity bill every watt deliver server facebook designed server server data center heat tax 7 mean using cold air outside chilling air passing across server mixing hot aisle evacuating side building term raw thermal eﬃciency data center second none server pride design really focus supply chain optimization 2011 released first data center first set server also started open compute project sure many familiar give away design server open design approach think eﬃciency server level main eﬃciency win really come software horizontal win like hhvm hphp win cash database web absolutely critical continuing deliver really eﬃcient infrastructure peak time one cluster really pretty piping hot run 10 hour day 90 93 server utilization really work tremendous amount making sure individual server software optimized also whole data center optimized provide content like blue server blue led see front server enclosed space hot aisle containment cold air come ceiling sucked server inside hot aisle containment temperature reach 100 degree hot air evacuated building potentially mixed winter time thermally eﬃcient system thing notice server look really work hard homogeneous footprint get good win term serviceability maintenance driver everything else also open eﬃciency win talked publicly released data center design released server design also released core software power facebook hhvm core php web server ballpark five six time eﬃcient traditional apache stack flashcache service use database trade ﬂash caching access slower hard drive presto one data processing/data warehouse piece software rock stevie ph proxy thrift folly general library use case really try able support open sourcing project try keep reason really believe entire industry benefit eﬃciency work benefit industry feeding back contributing new idea design fundamentally company going win lose based product cost infrastructure cost eﬃciency win infrastructure something would like entire industry benefit term architecture keep pretty simple cluster cluster synonymous network cluster large 12,000 server per cluster stamp capacity push order serve hot request user service cluster contain many dedicated service search photo message others cluster built optimized database storage think lot power redundancy cluster take one service useful think little bit one large service work facebook viewing main feed desktop center mobile main experience reason activity friend facebook data kept index called news feed rack reason activity last day people using facebook kept one rack design leaf aggregator leaf contain data storage recent activity ram aggregator thing ranking algorithm consolidates information respond request web hit come say need story go news feed rack pick one aggregator say give story aggregator blast query parallel 40 server rack gather subset data rank based interest since displayed engineer facebook server like long one five server allow variation one main team facebook infra capacity engineering wear black say front good saying kind engineering request fundamentally software far ﬂexible hardware pay hardware hardware becomes ineﬃcient lot variation homogeneity keep infrastructure easier optimize better supply chain better eﬃciencies stack term operating understanding server facebook year five type server web main compute workhorse database year evolved purely disk thing entirely ﬂash hadoop main data warehouse data processing lot compute lot disk photo lowest dollar gig certainly store lot medium facebook optimizing dollar per gig really important last feed rack news feed service talked engineer like lot memory lot compute rack give advantage five server type really constraining pretty classic get volume pricing putting order large order really work deeply supplier work way beneficial supply chain predictable pas saving important also important repurposing five service large projection term well something could launch something could launch using type server ﬂexible reallocating server one service another mean server infrastructure lay fallow waiting product launch well guy mutual fund like mutual fund dollar really manage well key advantage easier operation typical data center facility might data center tech server ratio one 400 450 facility somewhere 1 15,000 1 20,000 server optimized serviceability work hard make easy also translates operation software benefit eﬃciency consumer device drawback drawback intrinsic hardware soon allocate hardware hardware land life three four year however software need change time point time hardware software fit well facebook rack computer really thinking piece software fit individual server allocate rack question software live whole rack want talk point really idea mentioned want talk fun result disaggregated rack shooting better component service fit time also looking extend useful life server think rack news feed server ignore fact bunch server think well really got bunch compute got bunch ram got ﬂash exactly computer ram matter within rack got nice healthy network break disaggregated rack idea major component got processor compute server got ram kind ram server might storage server might flash rather put one server time going hit weakest link going enough computer enough ram let break put backplane switch resource service need time wasting service wasting resource server service fit better across service time also accommodate longer hardware refresh type 6 server news feed server cpu left right left ram news feed server fit cpu ram well designed server news feed however go another service maybe search one index service might need ram cpu mean terminally using cpu resource thing happen beginning service life maybe year one perfect fit along year two need ram need ﬂash able allocate time allocate hardware along need service provides huge benefit otherwise buying server really needed ram ca open case 10,000 server upgrade ram work able add sled ram huge benefit third benefit really keep hardware long physically last many time computer scale deprecating based critical resource longer good enough mean throwing away resource perfectly fine compute really get old ram device pure ram device operate forever disk wear time ﬂash depending write volume might wear time actually live think disaggregated rack say graph search rather computer three resource let compute server thing compute ram mainly focused compute would type 1 server u ﬂash sled would anywhere 30 256 terabyte ﬂash single sled ram sled might 256 512 gig ram storage year one ratio pick might perfect year two might discover might eﬃciency win index might grow time best thing would give service ﬂash rather allocate entire separate rack thereby doubling cost allocate one resource slam another flash sled kind ﬂexibility really lead pretty nice eﬃciency win still maintain core strength volume pricing custom configuration sort thing really allows u much smarter technology refreshes hardware resource thinking rack level evolve hardware service last year talked approximate tco win three period looking 12 20 opex saving conservative side aggressive 14 30 using different approach keep mind nothing fundamentally changed computer allocating allocating different way helping software team little bit ﬂexible bring resource work pretty much anybody scale time talked last year working project landed one service got 20 30 service maybe 40 major one one started actually able realize 40 saving total cost operating equipment nothing bringing resource necessary online right time customizing rack scalable ﬂexible way maintains supply chain win able realize 40 reduction cost one service something approach technique think pretty much anyone use think computer resource think last 20 year one idea disaggregated rack able adopt different type resource look last 20 year type thing server scale pretty much server amir michael holding one first server server architecture perspective almost identical 386 tower case 20 year ago new technology math coprocessors two processor server multicore good game changer last 20 year really gpus great vector math ﬂash memory phenomenal win last four five year looking forward really major advancement network gig nics 400 gig switch perfectly reasonable given state technology flash going flash provider really pushing higher higher iop ﬂash actually feel environment want look direction going lower iop really need many also much careful use ﬂash mean get much denser ﬂash sled realize nice benefit think ﬂash always going nice market performance ﬂash think data center world lot ﬂash interest going shift towards lower lower ﬂash last year talk asked industry please make worst ﬂash possible really work q stephen ju q unidentified participantvery necessarily ﬂash ﬂash tlc even beyond term also number ram alternative think coming interesting memory think going work going pretty good resistive memory par term technology also cold ﬂash worm storage storage need spinning medium perfectly reasonable immutable data put state device look eye chart technology next year eye chart lot detail simplify bit give one second take photo sorry pretty basic evolution last say 2009 say 2020 think computer data center going much focused one processor processor whole eﬃciency ecosystem developing really strong yes still ram think memory resistive memory also strong player worm storage technology might evolve next say three five year permanent immutable storage completely reasonable think bit density get point superior tco sense thin hard drive compared hard drive optical data storage great future term ability densify medium time seen number announcement term archival disk taking look like disc taking 100 gig 500 gig even terabyte fiber server think couple year away technology interested technology excited like open question question answer yes probably time couple question poll question audience lot talk mesos implication computing software infrastructure stack perspective mesos get adopted implication change jason taylor q unidentified participant jason taylor q unidentified participant q stephen ju jason taylor mesos virtualization thing yes infrastructure based bare metal scaling virtualization cloud excellent managing heavy idle workload 20 idle server need compact two idle server obviously win buy 18 idle server building scale building throughput best eﬃciency win come one really balancing utilization gear lot disag work also really looking deeply software hardware work together look big performance win first would say 3x hphp win really came going interpreted language compiled language solid win win since really come optimization code work also work specific hardware run soon start virtualizing soon start putting layer abstraction decouple engineer fantastic essentially allow make kind optimization anymore facebook large workload really focus using piece hardware much extent eﬃciencies discussed morning going allow facebook growth company continues grow revenue next three five year think fundamentally eﬃciency top priority u long time think important aspect eﬃciency u coherence brings software engineer term unifying thinking software hardware work together want talk actual capital spend would talk deborah term eﬃciency core way work talked lot ph fiber talk inaudible activity expansion fiber within footprint sure say talked fiber fiber within data center happened telecommunication industry delivered fantastic technology last bunch q stephen ju jason taylorof year talked dark fiber major facility two major facility running hundred mile well look core technology ready adopted data center space kind bandwidth within data center possible number great technology developing right term silicon photonics several company working essentially fully integrated chip optic package deliver performance really low cost largest thing happened biggest thing happened two three year ago ﬂash data center thing happening right amount network buy reasonable price climbing dramatically joined facebook nic everywhere 1 gig standard 2011 shifted 10 gig pretty soon next couple year 25 gig server say within three year 100 gig service around period going 100x amount bandwidth available transformative data center operates also write service network think work improvement networking going largest driver towards change internet company work think actually time thank much thank